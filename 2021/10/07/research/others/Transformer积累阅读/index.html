<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>Transformer积累阅读 | Curious;的个人划水博客</title>
  <meta name="description" content="" />
  <meta name="keywords" content="" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="shortcut icon" href="/">
  <link rel="alternate" href="/atom.xml" title="Curious;的个人划水博客" type="application/atom+xml">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Transformer是Google的研究者在2017年《Attention Is All You Need》论文中提出的用于seq2seq任务的模型，它没有RNN的循环结构或CNN的卷积结构，在机器翻译等任务中取得了一定的提升。 主要的motivation在于RNN、LSTM、GRU类的序列结构中的固有顺序属性阻碍了训练样本之间的并行化，对于长序列，内存限制将阻碍对训练样本的批量处理。 Tran">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer积累阅读">
<meta property="og:url" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/index.html">
<meta property="og:site_name" content="Curious;的个人划水博客">
<meta property="og:description" content="Transformer是Google的研究者在2017年《Attention Is All You Need》论文中提出的用于seq2seq任务的模型，它没有RNN的循环结构或CNN的卷积结构，在机器翻译等任务中取得了一定的提升。 主要的motivation在于RNN、LSTM、GRU类的序列结构中的固有顺序属性阻碍了训练样本之间的并行化，对于长序列，内存限制将阻碍对训练样本的批量处理。 Tran">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-00-23-19.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-00-25-51.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-01-21-44.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-14-45-35.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-14-57-11.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-15-01-00.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-11-15-30-06.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-11-15-39-22.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-11-15-48-30.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-12-12-12-03.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-12-13-22-21.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-12-14-07-46.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-12-14-13-01.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-09-54-56.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-10-01-25.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-10-10-57.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-10-15-18.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-10-21-43.png">
<meta property="og:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-10-24-34.png">
<meta property="article:published_time" content="2021-10-07T15:57:00.000Z">
<meta property="article:modified_time" content="2022-11-20T01:50:08.929Z">
<meta property="article:author" content="Curious;">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-00-23-19.png">
    
  <link href="https://fonts.googleapis.com/css?family=Inconsolata|Titillium+Web" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">
  <link href='//cdn.bootcss.com/node-waves/0.7.5/waves.min.css' rel='stylesheet'>
  
<link rel="stylesheet" href="/style.css">

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>


  <script>setLoadingBarProgress(20)</script> 
  <header class="l_header">
	<div class='wrapper'>
		<div class="nav-main container container--flex">
			<a class="logo flat-box" href='/' >
				Curious;的个人划水博客
			</a>
			<div class='menu'>
				<ul class='h-list'>
					
						<li>
							<a class='flat-box nav-home' href='/'>
								Home
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-archives' href='/archives'>
								Archives
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-gallery' target="_blank" rel="noopener" href='https://photos.google.com/album/AF1QipNoqKYgspQo5O1YhlFXGCQ7p575KBH3Yxf8WHL4?hl=zh-CN'>
								Gallery
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-about' href='/about'>
								About
							</a>
						</li>
					
				</ul>
				<div class='underline'></div>
			</div>
			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="Search" />
						<span class="icon icon-search"></span>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a href='javascript:void(0)'><span class="icon icon-search flat-box"></span></a></li>
				
				<li class='s-menu'><a href='javascript:void(0)'><span class="icon icon-menu flat-box"></span></a></li>
			</ul>
		</div>
		
		<div class='nav-sub container container--flex'>
			<a class="logo" class="flat-box" href='javascript:void(0)'>
				Word of Forks
			</a>

			<ul class='switcher h-list'>
				<li class='s-comment'><a href='javascript:void(0)'><span class="icon icon-chat_bubble_outline flat-box"></span></a></li>
				<li class='s-top'><a href='javascript:void(0)'><span class="icon icon-arrow_upward flat-box"></span></a></li>
				<li class='s-toc'><a href='javascript:void(0)'><span class="icon icon-format_list_numbered flat-box"></span></a></li>
			</ul>
		</div>
	</div>
</header>
<aside class="menu-phone">
	<nav>
		
			<a href="/" class="nav-home nav">
				Home
			</a>
		
			<a href="/archives" class="nav-archives nav">
				Archives
			</a>
		
			<a target="_blank" rel="noopener" href="https://photos.google.com/album/AF1QipNoqKYgspQo5O1YhlFXGCQ7p575KBH3Yxf8WHL4?hl=zh-CN" class="nav-gallery nav">
				Gallery
			</a>
		
			<a href="/about" class="nav-about nav">
				About
			</a>
		
	</nav>
</aside>

    <script>setLoadingBarProgress(40);</script>
  <div class="l_body">
    <div class='container clearfix'>
      <div class='l_main'>
        <article id="post-research/others/Transformer积累阅读"
  class="post white-box article-type-post"
  itemscope itemprop="blogPost">
	<section class='meta'>
	<h2 class="title">
  	<a href="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/">
    	Transformer积累阅读
    </a>
  </h2>
	<time>
	  10月 7, 2021
	</time>
	
    
    <div class='cats'>
        <a href="/categories/NLP/">NLP</a>
    </div>

	</section>
	
		<section class="toc-wrapper"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#A-High-Level-Look"><span class="toc-number">1.</span> <span class="toc-text">A High-Level Look</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Bringing-The-Tensors-Into-The-Picture"><span class="toc-number">2.</span> <span class="toc-text">Bringing The Tensors Into The Picture</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Now-We%E2%80%99re-Encoding"><span class="toc-number">3.</span> <span class="toc-text">Now We’re Encoding!</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Self-Attention-at-a-High-Level"><span class="toc-number">4.</span> <span class="toc-text">Self Attention at a High Level</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Self-Attention-in-Detail"><span class="toc-number">5.</span> <span class="toc-text">Self-Attention in Detail</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Matrix-Calculation-of-Self-Attention"><span class="toc-number">6.</span> <span class="toc-text">Matrix Calculation of Self-Attention</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#The-Beast-With-Many-Heads"><span class="toc-number">7.</span> <span class="toc-text">The Beast With Many Heads</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BD%99%E7%9A%84Transformer%E9%83%A8%E5%88%86%E5%92%8C%E6%89%80%E8%A6%81%E4%BA%86%E8%A7%A3%E7%9A%84%E4%BB%A3%E7%A0%81%E6%9A%82%E6%97%B6%E6%97%A0%E5%85%B3%EF%BC%8C%E6%9C%AA%E6%9D%A5%E5%86%8D%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%AD%A6%E4%B9%A0%E4%BA%86"><span class="toc-number">8.</span> <span class="toc-text">其余的Transformer部分和所要了解的代码暂时无关，未来再进一步学习了</span></a></li></ol></section>
	
	<section class="article typo">
  	<div class="article-entry" itemprop="articleBody">
    	<p>Transformer是Google的研究者在2017年《Attention Is All You Need》论文中提出的用于seq2seq任务的模型，它没有RNN的循环结构或CNN的卷积结构，在机器翻译等任务中取得了一定的提升。</p>
<p>主要的motivation在于RNN、LSTM、GRU类的序列结构中的固有顺序属性阻碍了训练样本之间的并行化，对于长序列，内存限制将阻碍对训练样本的批量处理。</p>
<p>Transformer中完全依赖于注意力机制对输入输出的全局依赖关系进行建模。因为对依赖的建模完全依赖于注意力机制，Transformer使用的注意力机制被称为自注意力（self-attention）</p>
<span id="more"></span>

<p>References:</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/85864250">https://zhuanlan.zhihu.com/p/85864250</a><br><a target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a></p>
</blockquote>
<h1 id="A-High-Level-Look"><a href="#A-High-Level-Look" class="headerlink" title="A High-Level Look"></a>A High-Level Look</h1><p>把整个Transformer当做一个黑盒，在机器翻译任务中，一种语言作为Transformer的输入，另外一种经过翻译后的语言作为Transformer的输出。<br><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-00-23-19.png"></p>
<p>略微细化下，我们看到Transformer架构由Encoding组件，Decoding组件，还有Encoding Decoding两个组件之间的连接组成。<br><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-00-25-51.png"></p>
<p>其中的Encoding组件是一个由encoders组成的栈。Decoding组件是一个由和encoders相同数目的decoders组成的栈。<br><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-01-21-44.png"></p>
<p>编码器在结构上都是相同的（但是互相之间不共享权重）。每一层分为两个子层，分别是Self-Attention机制和Feed Forward Neural Network</p>
<p>编码器的输入首先要通过一个self-attention层，一个帮助编码器在编码特定单词时查看输入句子中其他单词的层。将在之后进一步分析。</p>
<p>self-attention层的输出反馈给前馈神经网络（feed forward neural network），完全相同的前馈网络独立应用于每个位置。<br><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-14-45-35.png"></p>
<p>解码器具有这两个层，但在这两个层之间有一个Encoder-Decoder注意力层，帮助解码器关注输入句子的相关部分（类似于seq2seq模型中注意力的作用）。<br><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-14-57-11.png"></p>
<h1 id="Bringing-The-Tensors-Into-The-Picture"><a href="#Bringing-The-Tensors-Into-The-Picture" class="headerlink" title="Bringing The Tensors Into The Picture"></a>Bringing The Tensors Into The Picture</h1><p>首先使用embedding算法将每个输入字词转化为向量。使用这种小型的vectors来进行表示。（在SUMBT中，这一步对应了哪一步？为什么这里每个输入字词会变为vectors?）<br><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-08-15-01-00.png"></p>
<p>embedding只发生在最底层的编码器中。所有编码器都有一个共同的抽象概念，即它们接收一个大小为512的向量列表——在底部编码器中是word embedding，在其他编码器中则是直接位于下方的编码器的输出。这个列表的大小是我们可以设置的超参数，基本上是训练数据集中最长句子的长度。</p>
<p>在输入序列中word embedding后，每个单词都会流经编码器的两层。<br><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-11-15-30-06.png"></p>
<p>在这里，我们看到Transformer的一个关键属性，<strong>即每个位置的字在编码器中通过自己的路径流动。在self-attention层中，这些路径之间存在依赖关系。</strong>然而前馈层（feed forward layer）没有这些依赖关系，因此在流经前馈层时，可以并行执行各种路径。</p>
<p>接下来，把示例切换到一个较短的句子，并查看编码器的每个子层中放生了什么。</p>
<h1 id="Now-We’re-Encoding"><a href="#Now-We’re-Encoding" class="headerlink" title="Now We’re Encoding!"></a>Now We’re Encoding!</h1><p>正如我们已经提到的，编码器接收向量列表作为输入（在SUMBT中就是bert embedding后的那些内容）。它通过将这些向量传递到self-attention层，然后传入前馈神经网络，然后将输出向上发送到下一个编码器来处理该列表。<br><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-11-15-39-22.png"><br>每个位置的单词都经过一个自我注意过程。然后，它们各自通过一个前馈神经网络——一个完全相同的网络，每个向量分别通过它。</p>
<h1 id="Self-Attention-at-a-High-Level"><a href="#Self-Attention-at-a-High-Level" class="headerlink" title="Self Attention at a High Level"></a>Self Attention at a High Level</h1><p>不要被我胡说“self-attention”这个词所愚弄，因为这是每个人都应该熟悉的概念。在阅读《Attention Is All You Need》这篇论文之前，博客作者从未想到过这个概念。让我们总结一下它的工作原理。</p>
<p>假设以下句子是我们要翻译的输入句子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The animal didn&#x27;t cross the street because it was too tired</span><br></pre></td></tr></table></figure>

<p>在这一句子中，“it”指代的是什么？是指代的street还是animal？对于人类这是简单的，但是对于算法这是复杂的</p>
<p>当我们的模型在处理“it”这个单词的时候，self-attention允许把“it”和“animal”联系起来</p>
<p>当模型在处理每个单词的时候（输入语句的每个位置处），self-attention允许其关注输入语句的其他位置来寻求线索，并得出一种对于这个word更好的encoding</p>
<p>如果对RNN足够熟悉，请考虑如何维护hidden state，使得RNN能够将其已处理的先前单词/向量的表示形式与其正在处理的当前单词/向量结合起来。self-attention是Transformer用来将其他相关单词的“理解”bake到我们当前正在处理的单词中的方法。<br><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-11-15-48-30.png"></p>
<p>当我们在编码器#5（堆栈中的顶部编码器）中对单词“it”进行编码时，部分注意力机制将注意力集中在“动物”上，并将其表示的一部分烘焙到“it”的编码中。</p>
<font color="red">
这里需要更加明确self-attention机制的输入输出分别是什么，从个人理解来说是3个输入，1个输出？
</font>

<h1 id="Self-Attention-in-Detail"><a href="#Self-Attention-in-Detail" class="headerlink" title="Self-Attention in Detail"></a>Self-Attention in Detail</h1><p>让我们先看看如何使用向量计算自我注意，然后继续看看它是如何实际实现的——使用矩阵</p>
<p>计算self-attention的第一步是从编码器的每个输入向量中创建三个向量（在本例中，是每个单词的embedding），所以对于每个单词，我们创建Query vector，一个Key vector， 和一个Value vector。<strong>这些向量是通过将embedding乘以我们在训练过程中训练的三个矩阵来创建的</strong></p>
<font color="red">
突然注意到768 / 64 = 12
</font>

<p>请注意，这些新向量的维数小于嵌入向量，它们的维数为64，而且如和编码器输入/输出向量的维数为512。它们不必更小，这是一种架构选择，可以使用MultiHeadAttention（多头注意力机制）来使得计算保持不变（这里是指维度？）</p>
<p>将x1乘以WQ权重矩阵生成q1，即与该单词相关联的“查询”向量。我们最终创建了输入句子中每个单词的“查询”、“键”和“值”投影。</p>
<p>（差一张手画的图插进来）</p>
<p>什么是query key value向量？</p>
<font color="red">
根据师兄的解释，query感觉可以理解成一个自己的内容信息，而key理解成一个别人的信息，这样在和别人进行比较的时候就是上是用自己的q和别人所有的k进行比较
</font>

<p>它们是用于计算和思考注意力机制的抽象概念，继续阅读下面的注意力是如何计算的，就会知道关于每个向量所扮演角色的几乎所有内容。</p>
<p>计算self-attention的<strong>第二步</strong>是计算分数，假设我们正在计算本例中第一个单词“Thinking”的自我关注度。我们需要给输入句子中的每个单词打分。分数决定了当我们在某个位置对一个单词进行编码时，要把多少注意力放在输入句子的其他部分上。</p>
<p>分数是通过将query向量的点积与我们正在评分的各个单词的key向量相结合来计算的。因此，如果我们处理位置#1的单词的自我注意，第一个分数将是q1和k1的点积。第二个分数是q1和k2的点积。<strong>（点积将会得到一个分数）</strong></p>
<p><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-12-12-12-03.png"></p>
<p><strong>第三步和第四步</strong>是将分数除以8（论文中使用的关键向量维数的平方根–64。这会导致更稳定的梯度。这里可能有其他可能的值，但这是默认值），然后通过softmax操作传递结果。Softmax将分数标准化，使其全部为正值，加起来等于1。</p>
<p>此softmax分数确定每个单词在此位置的表达量。很明显，这个位置上的单词将具有最高的softmax分数，但有时关注与当前单词相关的另一个单词会很有用。</p>
<p><strong>第五步</strong>是将每个值向量乘以softmax分数（准备将他们相加）。这里的直觉是保持我们想要关注的单词的完整值，并忽略不相关的单词（例如，将它们乘以0.001这样的小数字）</p>
<p><strong>第六步</strong>是对加权值向量求和。这将在该位置（对于第一个单词）生成自我注意层的输出。</p>
<p><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-12-13-22-21.png"></p>
<p>自我注意力计算到此结束。结果向量是我们可以发送到前馈神经网络的向量。然而，在实际实现中，这种计算是以矩阵形式进行的，以加快处理速度。现在我们来看一下，我们已经看到了单词级计算的直觉。</p>
<h1 id="Matrix-Calculation-of-Self-Attention"><a href="#Matrix-Calculation-of-Self-Attention" class="headerlink" title="Matrix Calculation of Self-Attention"></a>Matrix Calculation of Self-Attention</h1><p>第一步是计算查询、键和值矩阵。我们通过将嵌入项打包到矩阵X中，并将其乘以我们训练的权重矩阵（WQ，WK，WV）来实现这一点。</p>
<p><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-12-14-07-46.png"></p>
<p>X矩阵中的每一行对应于输入句子中的一个单词。我们再次看到嵌入向量（512，或图中的4个框）和q/k/v向量（64，或图中的3个框）的大小差异</p>
<p>最后，由于我们处理的是矩阵，我们可以将第二步到第六步浓缩成一个公式来计算自我注意层的输出。</p>
<p><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-12-14-13-01.png"></p>
<h1 id="The-Beast-With-Many-Heads"><a href="#The-Beast-With-Many-Heads" class="headerlink" title="The Beast With Many Heads"></a>The Beast With Many Heads</h1><p>本文通过添加了一种称为“Multi-Head”注意力的机制，进一步细化了self-attention层。从这两个方面提高了注意层的性能：</p>
<ol>
<li><p>它扩展了模型关注不同位置的能力。是的，在上面的例子中，z1包含了一些其他单词所产生的编码，但是它可能被更加实际的单词本身所支配（这里是指权重比较高？）。如果我们翻译一句话，比如“动物没有过马路是因为它太累了”，我们会想知道“它”指的是哪个词。</p>
</li>
<li><p>它为注意力层提供了多个“表示子空间”（有一种增大参数量的感觉？）。正如我们接下来将要看到的，对于multihead-attention，我们不仅有一组，而且有多组query/key/value权重矩阵（Transformer使用8个attentionhead，因此每个编码器/解码器有8组）。这些集合中的每一个都是随机初始化的。然后，在训练之后，使用每个集合将输入embedding（或来自较低编码器/解码器的向量）投影到不同的表示子空间。</p>
</li>
</ol>
<p><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-09-54-56.png"></p>
<p>通过multihead-attention，我们为每个head维护单独的Q/K/V权重矩阵，从而产生不同的Q/K/V矩阵。和前面一样，我们用X乘以W^Q/W^K/W^V矩阵，得到Q/K/V矩阵。</p>
<p>如果我们做上边所述的同样的self-attention计算，只需使用不同的权重矩阵进行八次不同的计算，我们最终得到八个不同的Z矩阵。</p>
<p><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-10-01-25.png"></p>
<p>这给我们留下了一点挑战，前馈层不需要八个矩阵，它需要一个矩阵（每个单词对应一个向量（自：这里说的是z？））。所以我们需要一种方法把这八个元素压缩成一个矩阵。</p>
<p>我们怎么做？我们将矩阵合并（concatenate），然后将它们乘以一个额外的权重矩阵W^O。</p>
<font color="red">
这里感觉是concate表示了的不行，还要把这些融合到一起才行？或者说要保证输入的X和输出的Z是shape相同的？
multi-head如果是这样的操作的话就是一种增加参数量的作用？ 其亮点还主要在于“平权”的对待各个位置处，起到一种self-attention的作用。
</font>

<p><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-10-10-57.png"></p>
<p>这几乎就是multi-head attention的全部内容。这是相当多的矩阵，如果把他们放在一个图表示就可以更直观的看到这个过程。</p>
<p><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-10-15-18.png"></p>
<p>既然我们已经谈到了multihead attention，那么让我们回顾一下之前示例，看看在我们的示例语句中对单词“it”进行编码时，不同的注意力头集中在哪里。</p>
<p><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-10-21-43.png"></p>
<p>当我们对“它”这个词进行编码时，一个注意力集中在“动物”身上，而另一个注意力集中在“疲劳”身上——从某种意义上说，模型对“它”这个词的表达同时包含了一些“动物”和“疲劳”的表达。</p>
<p>然而，如果我们把所有的注意力都放在画面上，事情就更难解释了：</p>
<p><img src="/2021/10/07/research/others/Transformer%E7%A7%AF%E7%B4%AF%E9%98%85%E8%AF%BB/2021-10-13-10-24-34.png"></p>
<h1 id="其余的Transformer部分和所要了解的代码暂时无关，未来再进一步学习了"><a href="#其余的Transformer部分和所要了解的代码暂时无关，未来再进一步学习了" class="headerlink" title="其余的Transformer部分和所要了解的代码暂时无关，未来再进一步学习了"></a>其余的Transformer部分和所要了解的代码暂时无关，未来再进一步学习了</h1>
  	</div>
	  
	  <div class="article-tags tags">
      
        <a href="/tags/NLP/">NLP</a>
      
        <a href="/tags/Transformer/">Transformer</a>
      
	  </div>
    
		
	
		<div class="art-item-footer">
				
					<span class="art-item-left"><i class="icon icon-chevron-thin-left"></i>prev：<a href="/2021/10/11/macOS%E5%88%A0%E9%99%A4%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B%E7%9A%84-DS-Store/" rel="prev"  title="macOS删除一个文件夹下的.DS_Store">
						macOS删除一个文件夹下的.DS_Store 
					</a></span>
				
				
					<span class="art-item-right">next：<a href="/2021/09/27/research/papers/DST%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-SUMBT/" rel="next"  title="DST论文阅读-SUMBT">
						DST论文阅读-SUMBT
					</a><i class="icon icon-chevron-thin-right"></i></span>
				
		</div>
	
	</section>
	
</article>
<script>
	window.subData = {
		title: 'Transformer积累阅读',
		tools: true
	}
</script>

      </div>
      <aside class='l_side'>
        
  <section class='m_widget about'>

<div class='header'>Curious;</div>
<div class='content'>
<div class='desc'>BUPT, Computer Science and Technology, 2021-2024; BJUT, Information Security, 2017-2021</div>
</div>
</section>

  <section class='m_widget links'>
<div class='header'>Links</div>
<div class='content'>
    <ul class="entry">
    
        <li><a class="flat-box" target="_blank" href="https://github.com/yixuan004">
            <div class='name'>yixuan004</div>
        </a></li>
    
    </ul>
</div>
</section>

  <section class='m_widget categories'>
<div class='header'>Categories</div>
<div class='content'>
    
    <ul class="entry">
    
        <li><a class="flat-box" href="/categories/Crsenal/"><div class='name'>Crsenal</div><div class='badget'>16</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/"><div class='name'>LeetCode-python</div><div class='badget'>72</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/LeetCode-%E7%83%AD%E9%A2%98-HOT-100/"><div class='name'>LeetCode-热题 HOT 100</div><div class='badget'>29</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/LeetCode%E5%91%A8%E8%B5%9B/"><div class='name'>LeetCode周赛</div><div class='badget'>26</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/LeetCode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"><div class='name'>LeetCode每日一题</div><div class='badget'>4</div></a></li>
    
        <li><a class="flat-box" href="/categories/NLP/"><div class='name'>NLP</div><div class='badget'>6</div></a></li>
    
        <li><a class="flat-box" href="/categories/docker/"><div class='name'>docker</div><div class='badget'>4</div></a></li>
    
        <li><a class="flat-box" href="/categories/%E7%AC%94%E8%AF%95%E7%BB%83%E4%B9%A0-python/"><div class='name'>笔试练习-python</div><div class='badget'>1</div></a></li>
    
    </ul>
    
</div>
</section>

  
<div class="m_widget tagcloud">
    <div class="header">Tags</div>
    <div class='content'>
        <a href="/tags/Dataset/" style="font-size: 14px; color: #808080">Dataset</a> <a href="/tags/Dialogue/" style="font-size: 14px; color: #808080">Dialogue</a> <a href="/tags/Dialogue-State-Tracking/" style="font-size: 14.75px; color: #707070">Dialogue State Tracking</a> <a href="/tags/EASY/" style="font-size: 19.63px; color: #080808">EASY</a> <a href="/tags/HARD/" style="font-size: 16.63px; color: #484848">HARD</a> <a href="/tags/LeetCode-python/" style="font-size: 14px; color: #808080">LeetCode-python</a> <a href="/tags/MEDIUM/" style="font-size: 20px; color: #000">MEDIUM</a> <a href="/tags/NLP/" style="font-size: 15.88px; color: #585858">NLP</a> <a href="/tags/TODO%E4%BC%98%E5%8C%96/" style="font-size: 14.38px; color: #787878">TODO优化</a> <a href="/tags/Transformer/" style="font-size: 14px; color: #808080">Transformer</a> <a href="/tags/git/" style="font-size: 14.75px; color: #707070">git</a> <a href="/tags/hexo/" style="font-size: 14.75px; color: #707070">hexo</a> <a href="/tags/macOS%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/" style="font-size: 16.63px; color: #484848">macOS基础操作</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size: 14.38px; color: #787878">二分查找</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/" style="font-size: 14.75px; color: #707070">二叉搜索树</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 16.63px; color: #484848">二叉树</a> <a href="/tags/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/" style="font-size: 15.13px; color: #686868">优先队列</a> <a href="/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/" style="font-size: 16.25px; color: #505050">位运算</a> <a href="/tags/%E5%87%A0%E4%BD%95/" style="font-size: 14px; color: #808080">几何</a> <a href="/tags/%E5%88%86%E6%B2%BB/" style="font-size: 14px; color: #808080">分治</a> <a href="/tags/%E5%89%8D%E7%BC%80%E5%92%8C/" style="font-size: 15.13px; color: #686868">前缀和</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 18.13px; color: #282828">动态规划</a> <a href="/tags/%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/" style="font-size: 14px; color: #808080">双向链表</a> <a href="/tags/%E5%8F%8C%E6%8C%87%E9%92%88/" style="font-size: 15.88px; color: #585858">双指针</a> <a href="/tags/%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/" style="font-size: 14px; color: #808080">哈希函数</a> <a href="/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/" style="font-size: 18.5px; color: #202020">哈希表</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size: 15.5px; color: #606060">回溯</a> <a href="/tags/%E5%A0%86/" style="font-size: 15.13px; color: #686868">堆</a> <a href="/tags/%E5%AD%97%E5%85%B8%E6%A0%91/" style="font-size: 14.75px; color: #707070">字典树</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size: 18.88px; color: #181818">字符串</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D/" style="font-size: 14px; color: #808080">字符串匹配</a> <a href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" style="font-size: 14px; color: #808080">并查集</a> <a href="/tags/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/" style="font-size: 15.88px; color: #585858">广度优先搜索</a> <a href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" style="font-size: 14px; color: #808080">归并排序</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 17.75px; color: #303030">排序</a> <a href="/tags/%E6%95%B0%E5%AD%A6/" style="font-size: 18.13px; color: #282828">数学</a> <a href="/tags/%E6%95%B0%E7%BB%84/" style="font-size: 19.25px; color: #101010">数组</a> <a href="/tags/%E6%95%B0%E8%AE%BA/" style="font-size: 14px; color: #808080">数论</a> <a href="/tags/%E6%9E%9A%E4%B8%BE/" style="font-size: 15.13px; color: #686868">枚举</a> <a href="/tags/%E6%A0%88/" style="font-size: 15.13px; color: #686868">栈</a> <a href="/tags/%E6%A0%91/" style="font-size: 17.75px; color: #303030">树</a> <a href="/tags/%E6%A8%A1%E6%8B%9F/" style="font-size: 17px; color: #404040">模拟</a> <a href="/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" style="font-size: 14px; color: #808080">正则表达式</a> <a href="/tags/%E6%B0%B4%E5%A1%98%E6%8A%BD%E6%A0%B7/" style="font-size: 14px; color: #808080">水塘抽样</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/" style="font-size: 17.38px; color: #383838">深度优先搜索</a> <a href="/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/" style="font-size: 15.5px; color: #606060">滑动窗口</a> <a href="/tags/%E6%BB%9A%E5%8A%A8%E5%93%88%E5%B8%8C/" style="font-size: 14px; color: #808080">滚动哈希</a> <a href="/tags/%E7%8A%B6%E6%80%81%E5%8E%8B%E7%BC%A9/" style="font-size: 14px; color: #808080">状态压缩</a> <a href="/tags/%E7%9F%A9%E9%98%B5/" style="font-size: 16.25px; color: #505050">矩阵</a> <a href="/tags/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/" style="font-size: 14px; color: #808080">组合数学</a> <a href="/tags/%E8%AE%A1%E6%95%B0/" style="font-size: 16.25px; color: #505050">计数</a> <a href="/tags/%E8%AE%B0%E5%BF%86%E5%8C%96%E6%90%9C%E7%B4%A2/" style="font-size: 14px; color: #808080">记忆化搜索</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size: 14px; color: #808080">论文笔记</a> <a href="/tags/%E8%AE%BE%E8%AE%A1/" style="font-size: 14.75px; color: #707070">设计</a> <a href="/tags/%E8%B4%AA%E5%BF%83/" style="font-size: 17.75px; color: #303030">贪心</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size: 14px; color: #808080">递归</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15.5px; color: #606060">链表</a> <a href="/tags/%E9%9A%8F%E6%9C%BA%E5%8C%96/" style="font-size: 14px; color: #808080">随机化</a>
    </div>
</div>



      </aside>
      <script>setLoadingBarProgress(60);</script>
    </div>
  </div>
  <footer id="footer" class="clearfix">

	<div class="social-wrapper">
  	
      
        <a href="https://github.com/yixuan004" class="social github"
          target="_blank" rel="external">
          <span class="icon icon-github"></span>
        </a>
      
        <a href="https://twitter.com/kevinsfork" class="social twitter"
          target="_blank" rel="external">
          <span class="icon icon-twitter"></span>
        </a>
      
        <a href="/atom.xml" class="social rss"
          target="_blank" rel="external">
          <span class="icon icon-rss"></span>
        </a>
      
    
  </div>
  
  <div>Theme <a target="_blank" rel="noopener" href='https://github.com/stkevintan/hexo-theme-material-flow' class="codename">MaterialFlow</a> designed by <a href="http://keyin.me/" target="_blank">Kevin Tan</a>.</div>
  
</footer>


  <script>setLoadingBarProgress(80);</script>
  

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src='//cdn.bootcss.com/node-waves/0.7.5/waves.min.js'></script>
<script src="//cdn.bootcss.com/scrollReveal.js/3.3.2/scrollreveal.min.js"></script>

<script src="/js/jquery.fitvids.js"></script>

<script>
	var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
	var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
	var ALGOLIA_API_KEY = "";
	var ALGOLIA_APP_ID = "";
	var ALGOLIA_INDEX_NAME = "";
  var AZURE_SERVICE_NAME = "";
  var AZURE_INDEX_NAME = "";
  var AZURE_QUERY_KEY = "";
  var BAIDU_API_ID = "";
  var SEARCH_SERVICE = "hexo";
  var ROOT = "/"||"/";
  if(!ROOT.endsWith('/'))ROOT += '/';
</script>

<script src="/js/search.js"></script>


<script src="/js/app.js"></script>



  <script>setLoadingBarProgress(100);</script>
</body>
</html>
