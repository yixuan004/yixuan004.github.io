<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>NLP代码学习笔记 | Curious;的个人划水博客</title>
  <meta name="description" content="" />
  <meta name="keywords" content="" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="shortcut icon" href="/">
  <link rel="alternate" href="/atom.xml" title="Curious;的个人划水博客" type="application/atom+xml">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="记录一些代码中常用的操作说明及bug解决记录">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP代码学习笔记">
<meta property="og:url" content="http://example.com/2021/10/30/research/code/NLP%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Curious;的个人划水博客">
<meta property="og:description" content="记录一些代码中常用的操作说明及bug解决记录">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-10-30T07:36:19.000Z">
<meta property="article:modified_time" content="2022-11-20T01:50:08.892Z">
<meta property="article:author" content="Curious;">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
    
  <link href="https://fonts.googleapis.com/css?family=Inconsolata|Titillium+Web" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">
  <link href='//cdn.bootcss.com/node-waves/0.7.5/waves.min.css' rel='stylesheet'>
  
<link rel="stylesheet" href="/style.css">

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>


  <script>setLoadingBarProgress(20)</script> 
  <header class="l_header">
	<div class='wrapper'>
		<div class="nav-main container container--flex">
			<a class="logo flat-box" href='/' >
				Curious;的个人划水博客
			</a>
			<div class='menu'>
				<ul class='h-list'>
					
						<li>
							<a class='flat-box nav-home' href='/'>
								Home
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-archives' href='/archives'>
								Archives
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-gallery' target="_blank" rel="noopener" href='https://photos.google.com/album/AF1QipNoqKYgspQo5O1YhlFXGCQ7p575KBH3Yxf8WHL4?hl=zh-CN'>
								Gallery
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-about' href='/about'>
								About
							</a>
						</li>
					
				</ul>
				<div class='underline'></div>
			</div>
			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="Search" />
						<span class="icon icon-search"></span>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a href='javascript:void(0)'><span class="icon icon-search flat-box"></span></a></li>
				
				<li class='s-menu'><a href='javascript:void(0)'><span class="icon icon-menu flat-box"></span></a></li>
			</ul>
		</div>
		
		<div class='nav-sub container container--flex'>
			<a class="logo" class="flat-box" href='javascript:void(0)'>
				Word of Forks
			</a>

			<ul class='switcher h-list'>
				<li class='s-comment'><a href='javascript:void(0)'><span class="icon icon-chat_bubble_outline flat-box"></span></a></li>
				<li class='s-top'><a href='javascript:void(0)'><span class="icon icon-arrow_upward flat-box"></span></a></li>
				<li class='s-toc'><a href='javascript:void(0)'><span class="icon icon-format_list_numbered flat-box"></span></a></li>
			</ul>
		</div>
	</div>
</header>
<aside class="menu-phone">
	<nav>
		
			<a href="/" class="nav-home nav">
				Home
			</a>
		
			<a href="/archives" class="nav-archives nav">
				Archives
			</a>
		
			<a target="_blank" rel="noopener" href="https://photos.google.com/album/AF1QipNoqKYgspQo5O1YhlFXGCQ7p575KBH3Yxf8WHL4?hl=zh-CN" class="nav-gallery nav">
				Gallery
			</a>
		
			<a href="/about" class="nav-about nav">
				About
			</a>
		
	</nav>
</aside>

    <script>setLoadingBarProgress(40);</script>
  <div class="l_body">
    <div class='container clearfix'>
      <div class='l_main'>
        <article id="post-research/code/NLP代码学习笔记"
  class="post white-box article-type-post"
  itemscope itemprop="blogPost">
	<section class='meta'>
	<h2 class="title">
  	<a href="/2021/10/30/research/code/NLP%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
    	NLP代码学习笔记
    </a>
  </h2>
	<time>
	  10月 30, 2021
	</time>
	
    
    <div class='cats'>
        <a href="/categories/NLP/">NLP</a>
    </div>

	</section>
	
		<section class="toc-wrapper"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#nn-Embedding-from-pretrained"><span class="toc-number">1.</span> <span class="toc-text">nn.Embedding.from_pretrained</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8A%A5%E5%9C%A8%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8cpu%E5%92%8Cgpu%E7%9A%84%E9%94%99%E8%AF%AF"><span class="toc-number">2.</span> <span class="toc-text">报在同时使用cpu和gpu的错误</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E4%B8%AD%E5%88%A4%E6%96%AD%E4%B8%A4%E4%B8%AAtensor%E6%98%AF%E5%90%A6%E7%9B%B8%E7%AD%89"><span class="toc-number">3.</span> <span class="toc-text">pytorch中判断两个tensor是否相等</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E8%BE%93%E5%87%BA%E6%95%B4%E4%B8%AAtensor%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">pytorch输出整个tensor的方法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#tensor-tensor-0-0"><span class="toc-number">5.</span> <span class="toc-text">tensor &#x3D; tensor[0, :, 0]</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#contigous-view"><span class="toc-number">6.</span> <span class="toc-text">.contigous().view()</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E5%B8%B8%E7%94%A8%E7%9A%84%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C%E5%8F%8A%E5%BD%92%E4%B8%80%E5%8C%96%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.</span> <span class="toc-text">pytorch常用的张量操作及归一化算法实现</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#squeeze-%E5%92%8C-unsqueeze"><span class="toc-number">8.</span> <span class="toc-text">.squeeze() 和 .unsqueeze()</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#nn-GRU-%E4%B8%8E-nn-LSTM"><span class="toc-number">9.</span> <span class="toc-text">nn.GRU 与 nn.LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%96%87%E6%A1%A3%E8%AF%B4%E6%98%8E"><span class="toc-number">9.1.</span> <span class="toc-text">基本文档说明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">9.2.</span> <span class="toc-text">关于循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%9A%84%E4%B8%89%E4%B8%AA%E7%BB%B4%E5%BA%A6"><span class="toc-number">9.2.1.</span> <span class="toc-text">关于输入输出的三个维度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E4%BA%8Ebatch-first"><span class="toc-number">9.2.2.</span> <span class="toc-text">关于batch first</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DST%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84slot-accuracy%E5%92%8Cjoint-accuracy"><span class="toc-number">10.</span> <span class="toc-text">DST任务中的slot_accuracy和joint_accuracy</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#tqdm%E4%B8%AD%E7%9A%84desc%E5%8F%82%E6%95%B0"><span class="toc-number">11.</span> <span class="toc-text">tqdm中的desc参数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#tensorboard%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">12.</span> <span class="toc-text">tensorboard的使用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#TensorDataset%EF%BC%8CSequentialSampler%EF%BC%8CDataloader%E7%9B%B8%E5%85%B3"><span class="toc-number">13.</span> <span class="toc-text">TensorDataset，SequentialSampler，Dataloader相关</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CLASS-torch-utils-data-SequentialSampler-data-source"><span class="toc-number">13.1.</span> <span class="toc-text">CLASS torch.utils.data.SequentialSampler(data_source)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CLASS-torch-utils-data-RandomSampler-data-source-replacement-False-num-samples-None-generator-None"><span class="toc-number">13.2.</span> <span class="toc-text">CLASS torch.utils.data.RandomSampler(data_source, replacement&#x3D;False, num_samples&#x3D;None, generator&#x3D;None)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#np-prod"><span class="toc-number">14.</span> <span class="toc-text">np.prod()</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#fuzz-token-sort-ratio"><span class="toc-number">15.</span> <span class="toc-text">fuzz.token_sort_ratio()</span></a></li></ol></section>
	
	<section class="article typo">
  	<div class="article-entry" itemprop="articleBody">
    	<p>记录一些代码中常用的操作说明及bug解决记录</p>
<span id="more"></span>

<h1 id="nn-Embedding-from-pretrained"><a href="#nn-Embedding-from-pretrained" class="headerlink" title="nn.Embedding.from_pretrained"></a>nn.Embedding.from_pretrained</h1><p>整个代码的上下文是在做label_ids和slot_ids的embedding</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_slot_value_lookup</span>(<span class="params">self, label_ids, slot_ids</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    self.sv_encoder = BertForUtteranceEncoding.from_pretrained(</span></span><br><span class="line"><span class="string">            os.path.join(args.bert_dir, &#x27;bert-base-uncased&#x27;)</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string">    # 作者把调用sv_encoder的部分fix住</span></span><br><span class="line"><span class="string">    for p in self.sv_encoder.bert.parameters():</span></span><br><span class="line"><span class="string">        p.requires_grad = False</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    self.sv_encoder.<span class="built_in">eval</span>() <span class="comment"># 切换到evaluate模式</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Slot encoding，这个是fix住的部分</span></span><br><span class="line">    slot_type_ids = torch.zeros(slot_ids.size(), dtype=torch.long).to(self.device) <span class="comment"># 初始化一个空的slot_type_ids</span></span><br><span class="line">    slot_mask = slot_ids &gt; <span class="number">0</span></span><br><span class="line">    hid_slot, _ = self.sv_encoder(slot_ids.view(-<span class="number">1</span>, self.max_label_length),</span><br><span class="line">                                    slot_type_ids.view(-<span class="number">1</span>, self.max_label_length),</span><br><span class="line">                                    slot_mask.view(-<span class="number">1</span>, self.max_label_length),</span><br><span class="line">                                    output_all_encoded_layers=<span class="literal">False</span>) <span class="comment"># 获取CLS的token，hidden的</span></span><br><span class="line">    hid_slot = hid_slot[:, <span class="number">0</span>, :] <span class="comment"># 博客中指出，这样的操作是在获取CLStoken，而CLStoken是用来进行分类的，也一般被认为是整句话的embedding</span></span><br><span class="line">    hid_slot = hid_slot.detach()</span><br><span class="line">    self.slot_lookup = nn.Embedding.from_pretrained(hid_slot, freeze=<span class="literal">True</span>) <span class="comment"># slot的embedding结果，是不可以训练的</span></span><br></pre></td></tr></table></figure>

<h1 id="报在同时使用cpu和gpu的错误"><a href="#报在同时使用cpu和gpu的错误" class="headerlink" title="报在同时使用cpu和gpu的错误"></a>报在同时使用cpu和gpu的错误</h1><p>报在同时使用cpu和gpu的错误除了tensor要tensor.to(device)以外，model也要.to(device)，否则可能会报在同时使用cpu和gpu的错误</p>
<h1 id="pytorch中判断两个tensor是否相等"><a href="#pytorch中判断两个tensor是否相等" class="headerlink" title="pytorch中判断两个tensor是否相等"></a>pytorch中判断两个tensor是否相等</h1><ol>
<li><p>tensor.equal()方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 该方法用于比较两个tensor是否一样，一样则返回True否则为False</span></span><br><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">b = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(a.equal(b))    <span class="comment"># 返回True</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>tensor.eq()方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 该方法用于主元素比较是否相等，相等则在对应位置返回True，否则为False</span></span><br><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(a.eq(b))  <span class="comment"># 返回tensor([False,True,False,True]),与a==b返回的结果一样</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="pytorch输出整个tensor的方法"><a href="#pytorch输出整个tensor的方法" class="headerlink" title="pytorch输出整个tensor的方法"></a>pytorch输出整个tensor的方法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.set_printoptions(profile=<span class="string">&quot;full&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># prints the whole tensor</span></span><br><span class="line">torch.set_printoptions(profile=<span class="string">&quot;default&quot;</span>) <span class="comment"># reset</span></span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># prints the truncated tensor</span></span><br></pre></td></tr></table></figure>
<p>在这样的输出下，之后就可以写到文件里了</p>
<h1 id="tensor-tensor-0-0"><a href="#tensor-tensor-0-0" class="headerlink" title="tensor = tensor[0, :, 0]"></a>tensor = tensor[0, :, 0]</h1><p>这种操作可能代表着仅需要获取bert的cls token的embedding结果，也被认为是整句话的embedding</p>
<h1 id="contigous-view"><a href="#contigous-view" class="headerlink" title=".contigous().view()"></a>.contigous().view()</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.contigous().view()</span><br></pre></td></tr></table></figure>
<p>有些tensor并不是占用一整块内存，而是由不同的数据块组成，而tensor的view()操作依赖于内存是整块的，这时只需要执行contigous()这个函数，把tensor变成在内存中连续分布的形式，再使用view。</p>
<p>Pytorch0.4中，增加了一个reshape函数，就相当于contigous().view()的功能了！</p>
<h1 id="pytorch常用的张量操作及归一化算法实现"><a href="#pytorch常用的张量操作及归一化算法实现" class="headerlink" title="pytorch常用的张量操作及归一化算法实现"></a>pytorch常用的张量操作及归一化算法实现</h1><blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/76255917">https://zhuanlan.zhihu.com/p/76255917</a></p>
</blockquote>
<h1 id="squeeze-和-unsqueeze"><a href="#squeeze-和-unsqueeze" class="headerlink" title=".squeeze() 和 .unsqueeze()"></a>.squeeze() 和 .unsqueeze()</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.squeeze()</span><br><span class="line">.unsqueeze()</span><br></pre></td></tr></table></figure>
<p>squeeze()为压缩的意思，即去掉维度数为1的dim，默认是去掉所有为1的，但是也可以自己指定，但如果指定的维度不为1则不会发生任何改变。</p>
<p>unsqueeze(dim)则与squeeze(dim)正好相反，为添加一个维度的作用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print(&quot;hidden.shape: &quot;, hidden.shape) # torch.Size([96, 1, 768]</span></span><br><span class="line">hidden = hidden.squeeze() <span class="comment"># .squeeze()舍弃维度</span></span><br><span class="line"><span class="comment"># print(&quot;after .squeeze(), hidden.shape: &quot;, hidden.shape) # torch.Size([96, 768])</span></span><br></pre></td></tr></table></figure>

<h1 id="nn-GRU-与-nn-LSTM"><a href="#nn-GRU-与-nn-LSTM" class="headerlink" title="nn.GRU 与 nn.LSTM"></a>nn.GRU 与 nn.LSTM</h1><p>循环神经网络是一种能够自适应的变长网络，能够对带有上下文的连续序列很好地进行编码</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html">https://pytorch.org/docs/stable/torch.html</a></p>
</blockquote>
<h2 id="基本文档说明"><a href="#基本文档说明" class="headerlink" title="基本文档说明"></a>基本文档说明</h2><p><strong>参数设置</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">input_size: 单个LSTM神经元的输入维度</span><br><span class="line">hidden_size: 单个LSTM神经元的隐含层输出维度</span><br><span class="line">num_layers: LSTM的层数，这里指的是叠起来的层数，而不是展开的层数，展开是自适应的。</span><br><span class="line">bias: 计算过程中是否需要偏置</span><br><span class="line">batch_first: batch是否位于第一个维度，很多时候容易混淆，将在之后进一步解释</span><br><span class="line">dropout: 其中每一层输出的dropout概率，默认为<span class="number">0</span>即不进行dropout，需要注意的一点是最后一层的输出是不会加上dropout概率的。也就是说，当只用到一层LSTM的时候，这个参数是不起作用的。</span><br><span class="line">bidirectional: 是否双向，当设置为<span class="literal">True</span>的时候，输出会为将双向LSTM的输出进行拼接，输出的feature size会增加一倍</span><br><span class="line">proj_size: 很多博客中都没有解释，用到的时候可能需要参考 <span class="comment"># https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM</span></span><br><span class="line"></span><br><span class="line">self.nbt = nn.LSTM(input_size=self.bert_output_dim,</span><br><span class="line">                              hidden_size=self.hidden_dim,</span><br><span class="line">                              num_layers=self.rnn_num_layers,</span><br><span class="line">                              dropout=self.hidden_dropout_prob,</span><br><span class="line">                              batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">self.nbt = nn.GRU(input_size=self.bert_output_dim,</span><br><span class="line">                              hidden_size=self.hidden_dim, <span class="comment"># args.hidden_dim</span></span><br><span class="line">                              num_layers=self.rnn_num_layers,</span><br><span class="line">                              dropout=self.hidden_dropout_prob,</span><br><span class="line">                              batch_first=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><strong>Inputs</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span>: <span class="built_in">input</span>, (h_0, c_0)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span>: 当batch_first = <span class="literal">False</span>的时候(L, N, H_in)，当batch_first=<span class="literal">True</span>的时候(N, L, H_in)</span><br><span class="line">h_0: (D*num_layers, N, H_out)，containing the initial hidden state <span class="keyword">for</span> each element <span class="keyword">in</span> the batch. Defaults to zeros <span class="keyword">if</span> (h_0, c_0) <span class="keyword">is</span> <span class="keyword">not</span> provided.</span><br><span class="line">c_0: (D*num_layers, N, H_cell)，containing the initial cell state <span class="keyword">for</span> each element <span class="keyword">in</span> the batch. Defaults to zeros <span class="keyword">if</span> (h_0, c_0) <span class="keyword">is</span> <span class="keyword">not</span> provided.</span><br><span class="line"></span><br><span class="line">其中：</span><br><span class="line">    N是batch_size</span><br><span class="line">    L是每句话的长度</span><br><span class="line">    如果使用双向LSTM则D是<span class="number">2</span>，否则是<span class="number">1</span></span><br><span class="line">    H_in是输入的hiddendim（例如是bert的输出<span class="number">768</span>）</span><br><span class="line">    H_cell是LSTM内部的hidden_size</span><br><span class="line">    H_out和输入参数中的proj_size相关，但基本可以理解为就是hidden_size，</span><br></pre></td></tr></table></figure>

<p><strong>Outputs</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Outputs: output, (h_n, c_n)</span><br><span class="line"></span><br><span class="line">output: 当batch_first=<span class="literal">False</span>的时候是(L, N, D*H_out)，当batch_first=<span class="literal">True</span>的时候是(N, L, D*H_out)，其中包括了LSTM最后一层的输出h_t，对于每个t时刻。在PackedSequence相关上还有其他的操作，不过暂时就先不管了</span><br><span class="line">h_n: (D*num_layers, N, H_out)包含了每个batch中最后的一个hidden state的element</span><br><span class="line">c_n: (D*num_layersm N, H_cell)包含了最后一个cell的state，对于每个batch的最后一个element？</span><br></pre></td></tr></table></figure>

<h2 id="关于循环神经网络"><a href="#关于循环神经网络" class="headerlink" title="关于循环神经网络"></a>关于循环神经网络</h2><h3 id="关于输入输出的三个维度"><a href="#关于输入输出的三个维度" class="headerlink" title="关于输入输出的三个维度"></a>关于输入输出的三个维度</h3><p>自：维度在tensor的变化中始终是最关键的部分，怎么理解维度背后的含义？</p>
<p>对于输入输出，我们首先需要注意是传给的网络输出必须是三维的<br>其中每个维度代表的意思，我们习惯的方式是[batch_size, sequence_length, feature_size]<br>具体来说，假如输入的是句子的话，每个维度的含义就是：</p>
<p>[一次投入到网络中的句子的条数，句子的长度，句子中每个单词对应的向量维度]</p>
<p>自：在SUMBT代码中，这里的输入该怎么一步步的理解</p>
<h3 id="关于batch-first"><a href="#关于batch-first" class="headerlink" title="关于batch first"></a>关于batch first</h3><p>这个是一个非常有趣的参数，他能够将输入的形式变为我们习惯的[batch_size, seq_len, feature_size]</p>
<p>也就是说原本输入参数的形式是[seq_len, batch_size, feature_size]可以视作原本为一列一句话，现在给我们改成了更习惯的一行一句话</p>
<p>更通俗的来说，就是原本一行为一个句子，变成每一列为一个句子，其实设置了batch_first，也不过是在内部也是使用了第1维度和第2维度的转置操作来变成初始形式</p>
<p>在SUMBT中怎么理解这个事情？……</p>
<h1 id="DST任务中的slot-accuracy和joint-accuracy"><a href="#DST任务中的slot-accuracy和joint-accuracy" class="headerlink" title="DST任务中的slot_accuracy和joint_accuracy"></a>DST任务中的slot_accuracy和joint_accuracy</h1><p>slot_accuracy:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">acc_slot = torch.<span class="built_in">sum</span>(accuracy, <span class="number">0</span>).<span class="built_in">float</span>() / torch.<span class="built_in">sum</span>(labels.view(-<span class="number">1</span>, slot_dim) &gt; -<span class="number">1</span>, <span class="number">0</span>).<span class="built_in">float</span>()</span><br></pre></td></tr></table></figure>
<p>个人总结：按照三个槽分别算，对的除以总的（需要去除padding）就是slot_accuracy</p>
<p>joint_accuracy:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">acc = <span class="built_in">sum</span>(torch.<span class="built_in">sum</span>(accuracy, <span class="number">1</span>) / slot_dim).<span class="built_in">float</span>() / torch.<span class="built_in">sum</span>(labels[:, :, <span class="number">0</span>].view(-<span class="number">1</span>) &gt; -<span class="number">1</span>, <span class="number">0</span>).<span class="built_in">float</span>() <span class="comment"># joint accuracy</span></span><br></pre></td></tr></table></figure>
<p>个人总结：每轮对话的算成一个，例如在每轮对话中有3个槽，对了2个，该轮对话就是0.66，之后把所有轮对话的加在一起，除以对话的有效轮数就是joint_accuracy</p>
<h1 id="tqdm中的desc参数"><a href="#tqdm中的desc参数" class="headerlink" title="tqdm中的desc参数"></a>tqdm中的desc参数</h1><p>这里desc参数是进度条的前缀名称</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tqdm(dev_dataloader, desc=<span class="string">&quot;Validation&quot;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="tensorboard的使用"><a href="#tensorboard的使用" class="headerlink" title="tensorboard的使用"></a>tensorboard的使用</h1><p>看起来tensorboard和tensorboardX不是一个东西？ 所以需要使用pip install进行安装(venv环境下)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">pip install tensorboard</span></span><br><span class="line">...</span><br><span class="line">(venvsumbt) lyx@h1:/hdd1/lyx$ tensorboard</span><br><span class="line">TensorFlow installation not found - running with reduced feature set.</span><br><span class="line">Error: A logdir or db must be specified. For example `tensorboard --logdir mylogdir` or `tensorboard --db sqlite:~/.tensorboard.db`. Run `tensorboard --helpfull` for details and examples.</span><br></pre></td></tr></table></figure>

<p>使用方法如下（SUMBT-lyx为例）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(venvsumbt) lyx@h1:/hdd1/lyx/SUMBT-lyx$ tensorboard --logdir=&#x27;SUMBT-lyx/tensorboard/output&#x27;</span><br><span class="line">TensorFlow installation not found - running with reduced feature set.</span><br><span class="line"></span><br><span class="line">NOTE: Using experimental fast data loading logic. To disable, pass</span><br><span class="line">    &quot;--load_fast=false&quot; and report issues on GitHub. More details:</span><br><span class="line">    https://github.com/tensorflow/tensorboard/issues/4784</span><br><span class="line"></span><br><span class="line">Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all</span><br><span class="line">TensorBoard 2.7.0 at http://localhost:6007/ (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure>

<p>此时还需要配合一条端口转发命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh lyx@xxx.xxx.xxx.xxx -L 6007:localhost:6007</span><br></pre></td></tr></table></figure>

<p>注意要在训练前另开一个bash执行如下，然后再开启训练，否则可能会出现tensorboard没有显示的情况<br>使用绝对路径！</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=/hdd1/lyx/SUMBT-lyx/tensorboard/ckpt-output</span><br><span class="line"></span><br><span class="line">tensorboard --logdir=/hdd1/lyx/SUMBT-lyx/tensorboard/20211020-1152-lyx测试</span><br></pre></td></tr></table></figure>

<h1 id="TensorDataset，SequentialSampler，Dataloader相关"><a href="#TensorDataset，SequentialSampler，Dataloader相关" class="headerlink" title="TensorDataset，SequentialSampler，Dataloader相关"></a>TensorDataset，SequentialSampler，Dataloader相关</h1><p>Reference:</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/data.html?highlight=sequentialsampler#torch.utils.data.SequentialSampler">https://pytorch.org/docs/stable/data.html?highlight=sequentialsampler#torch.utils.data.SequentialSampler</a></p>
</blockquote>
<p>在代码中看到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xxx_sampler = SequentialSampler(xxx_data)  <span class="keyword">or</span> RandomSampler(xxx_data)</span><br><span class="line">xxx_dataloader = DataLoader(xxx_data, sampler=xxx_sampler, batch_size=...)</span><br></pre></td></tr></table></figure>

<p>自：一般来说在训练过程中使用RandomSampler，dev和test过程中使用SequentialSampler，</p>
<p>在DST任务中因为和上下文一些状态有关，所以是不是只能顺序采样</p>
<h2 id="CLASS-torch-utils-data-SequentialSampler-data-source"><a href="#CLASS-torch-utils-data-SequentialSampler-data-source" class="headerlink" title="CLASS torch.utils.data.SequentialSampler(data_source)"></a>CLASS torch.utils.data.SequentialSampler(data_source)</h2><p>按顺序采样元素，始终按相同顺序采样（构建一个迭代器）</p>
<p>源代码是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SequentialSampler</span>(Sampler[<span class="built_in">int</span>]):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Samples elements sequentially, always in the same order.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data_source (Dataset): dataset to sample from</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data_source: Sized</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_source: Sized</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.data_source = data_source</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>) -&gt; Iterator[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(self.data_source)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data_source)</span><br></pre></td></tr></table></figure>

<h2 id="CLASS-torch-utils-data-RandomSampler-data-source-replacement-False-num-samples-None-generator-None"><a href="#CLASS-torch-utils-data-RandomSampler-data-source-replacement-False-num-samples-None-generator-None" class="headerlink" title="CLASS torch.utils.data.RandomSampler(data_source, replacement=False, num_samples=None, generator=None)"></a>CLASS torch.utils.data.RandomSampler(data_source, replacement=False, num_samples=None, generator=None)</h2><p>随机抽取元素样本。如果没有替换，则从无序数据集中采样。如果使用替换，则用户可以指定要绘制的样本数</p>
<p>源代码见：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/_modules/torch/utils/data/sampler.html#RandomSampler">https://pytorch.org/docs/stable/_modules/torch/utils/data/sampler.html#RandomSampler</a></p>
</blockquote>
<h1 id="np-prod"><a href="#np-prod" class="headerlink" title="np.prod()"></a>np.prod()</h1><p>Reference: </p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40522801/article/details/106578775">https://blog.csdn.net/weixin_40522801/article/details/106578775</a></p>
</blockquote>
<p>np.prod()用来计算所有元素的乘积，pro应该是product的简写，开始的时候不是很明白为什么在计算acc的时候会使用np.prod这个函数，后来发现这个是在计算jointacc上的很好用的函数，因为对于jointacc来说一轮中只要有一个错就算错了</p>
<p>下边这个代码展示了一个JointGA的计算方式，注意在fuzz模式下，可能出现不是1的单轮jointacc值，但是还会有一种越乘越小的感觉</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Joint goal accuracy.</span></span><br><span class="line">goal_acc[JOINT_GOAL_ACCURACY] = np.prod(list_acc) <span class="keyword">if</span> list_acc <span class="keyword">else</span> NAN_VAL</span><br></pre></td></tr></table></figure>

<h1 id="fuzz-token-sort-ratio"><a href="#fuzz-token-sort-ratio" class="headerlink" title="fuzz.token_sort_ratio()"></a>fuzz.token_sort_ratio()</h1><p>在对于DST任务non-categorical槽进行评价的时候，很多方法中会使用fuzz这个模式匹配，代码如下，其中str_ref（erence）是真值字符串，str_hyp（othesis）是预测的那个字符串</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">match_score = fuzz.token_sort_ratio(str_ref, str_hyp) / <span class="number">100.0</span></span><br></pre></td></tr></table></figure>

<p>解读下fuzz.token_sort_ratio这个函数，在源代码中调用顺序如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">token_sort_ratio</span>(<span class="params">s1, s2, force_ascii=<span class="literal">True</span>, full_process=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a measure of the sequences&#x27; similarity between 0 and 100</span></span><br><span class="line"><span class="string">    but sorting the token before comparing.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> _token_sort(s1, s2, partial=<span class="literal">False</span>, force_ascii=force_ascii, full_process=full_process)</span><br><span class="line"></span><br><span class="line"><span class="meta">@utils.check_for_none</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_token_sort</span>(<span class="params">s1, s2, partial=<span class="literal">True</span>, force_ascii=<span class="literal">True</span>, full_process=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    自己注释：按照token进行排序</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    sorted1 = _process_and_sort(s1, force_ascii, full_process=full_process)</span><br><span class="line">    sorted2 = _process_and_sort(s2, force_ascii, full_process=full_process)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> partial:</span><br><span class="line">        <span class="keyword">return</span> partial_ratio(sorted1, sorted2)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> ratio(sorted1, sorted2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_process_and_sort</span>(<span class="params">s, force_ascii, full_process=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return a cleaned string with token sorted</span></span><br><span class="line"><span class="string">    返回一个按照token排序的干净的string，这里这个干净就是调用full_process</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># pull tokens</span></span><br><span class="line">    ts = utils.full_process(s, force_ascii=force_ascii) <span class="keyword">if</span> full_process <span class="keyword">else</span> s</span><br><span class="line">    tokens = ts.split()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort tokens and join</span></span><br><span class="line">    sorted_string = <span class="string">u&quot; &quot;</span>.join(<span class="built_in">sorted</span>(tokens))</span><br><span class="line">    <span class="keyword">return</span> sorted_string.strip()</span><br><span class="line"></span><br><span class="line"><span class="comment"># utils.full_process</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">full_process</span>(<span class="params">s, force_ascii=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Process string by</span></span><br><span class="line"><span class="string">        -- removing all but letters and numbers</span></span><br><span class="line"><span class="string">        -- trim whitespace</span></span><br><span class="line"><span class="string">        -- force to lower case</span></span><br><span class="line"><span class="string">        if force_ascii == True, force convert to ascii</span></span><br><span class="line"><span class="string">    这里是几种字符过滤方式，    </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> force_ascii:</span><br><span class="line">        s = asciidammit(s)</span><br><span class="line">    <span class="comment"># Keep only Letters and Numbers (see Unicode docs).</span></span><br><span class="line">    string_out = StringProcessor.replace_non_letters_non_numbers_with_whitespace(s) <span class="comment"># 用空格替代所有不是字母和数字的</span></span><br><span class="line">    <span class="comment"># Force into lowercase.</span></span><br><span class="line">    string_out = StringProcessor.to_lower_case(string_out)</span><br><span class="line">    <span class="comment"># Remove leading and trailing whitespaces.</span></span><br><span class="line">    string_out = StringProcessor.strip(string_out)</span><br><span class="line">    <span class="keyword">return</span> string_out</span><br></pre></td></tr></table></figure>

<p>首先把一个字符串不是字母、数字的字符都用空格替换并转化成小写，然后按照空格切分后进行排序，排序后按照字符级别计算编辑距离比。</p>
<p>编辑距离比的计算方式是：(len(str1)+len(str2)-编辑距离) / (len(str1)+len(str2))</p>
<p>例如：<br>“Curious San Francisco”（字符含空格长度为21） 和 “San Francisco”（字符不含空格长度为13），编辑距离为8<br>(21 + 13 - 8) / (21 + 13) = 0.7647</p>
<p>“CuriousAAA San Francisco”（字符含空格长度为24） 和 “San Francisco”（字符不含空格长度为13），编辑距离为11<br>(24 + 13 - 11) / (24 + 13) = 0.7027</p>

  	</div>
	  
	  <div class="article-tags tags">
      
        <a href="/tags/NLP/">NLP</a>
      
	  </div>
    
		
	
		<div class="art-item-footer">
				
					<span class="art-item-left"><i class="icon icon-chevron-thin-left"></i>prev：<a href="/2021/11/10/%E8%AE%B0%E5%BD%95%E6%9C%8D%E5%8A%A1%E5%99%A8miniconda%E9%85%8D%E7%BD%AE%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/" rel="prev"  title="记录服务器miniconda配置虚拟环境">
						记录服务器miniconda配置虚拟环境 
					</a></span>
				
				
					<span class="art-item-right">next：<a href="/2021/10/28/macOS%E5%8F%8ALinux-%E7%BB%9F%E8%AE%A1%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B%E7%9A%84%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E4%B8%AA%E6%95%B0/" rel="next"  title="macOS及Linux-统计文件夹下的文件目录个数">
						macOS及Linux-统计文件夹下的文件目录个数
					</a><i class="icon icon-chevron-thin-right"></i></span>
				
		</div>
	
	</section>
	
</article>
<script>
	window.subData = {
		title: 'NLP代码学习笔记',
		tools: true
	}
</script>

      </div>
      <aside class='l_side'>
        
  <section class='m_widget about'>

<div class='header'>Curious;</div>
<div class='content'>
<div class='desc'>BUPT, Computer Science and Technology, 2021-2024; BJUT, Information Security, 2017-2021</div>
</div>
</section>

  <section class='m_widget links'>
<div class='header'>Links</div>
<div class='content'>
    <ul class="entry">
    
        <li><a class="flat-box" target="_blank" href="https://github.com/yixuan004">
            <div class='name'>yixuan004</div>
        </a></li>
    
    </ul>
</div>
</section>

  <section class='m_widget categories'>
<div class='header'>Categories</div>
<div class='content'>
    
    <ul class="entry">
    
        <li><a class="flat-box" href="/categories/Crsenal/"><div class='name'>Crsenal</div><div class='badget'>16</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/"><div class='name'>LeetCode-python</div><div class='badget'>72</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/LeetCode-%E7%83%AD%E9%A2%98-HOT-100/"><div class='name'>LeetCode-热题 HOT 100</div><div class='badget'>29</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/LeetCode%E5%91%A8%E8%B5%9B/"><div class='name'>LeetCode周赛</div><div class='badget'>26</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/LeetCode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"><div class='name'>LeetCode每日一题</div><div class='badget'>4</div></a></li>
    
        <li><a class="flat-box" href="/categories/NLP/"><div class='name'>NLP</div><div class='badget'>6</div></a></li>
    
        <li><a class="flat-box" href="/categories/docker/"><div class='name'>docker</div><div class='badget'>4</div></a></li>
    
        <li><a class="flat-box" href="/categories/%E7%AC%94%E8%AF%95%E7%BB%83%E4%B9%A0-python/"><div class='name'>笔试练习-python</div><div class='badget'>1</div></a></li>
    
    </ul>
    
</div>
</section>

  
<div class="m_widget tagcloud">
    <div class="header">Tags</div>
    <div class='content'>
        <a href="/tags/Dataset/" style="font-size: 14px; color: #808080">Dataset</a> <a href="/tags/Dialogue/" style="font-size: 14px; color: #808080">Dialogue</a> <a href="/tags/Dialogue-State-Tracking/" style="font-size: 14.75px; color: #707070">Dialogue State Tracking</a> <a href="/tags/EASY/" style="font-size: 19.63px; color: #080808">EASY</a> <a href="/tags/HARD/" style="font-size: 16.63px; color: #484848">HARD</a> <a href="/tags/LeetCode-python/" style="font-size: 14px; color: #808080">LeetCode-python</a> <a href="/tags/MEDIUM/" style="font-size: 20px; color: #000">MEDIUM</a> <a href="/tags/NLP/" style="font-size: 15.88px; color: #585858">NLP</a> <a href="/tags/TODO%E4%BC%98%E5%8C%96/" style="font-size: 14.38px; color: #787878">TODO优化</a> <a href="/tags/Transformer/" style="font-size: 14px; color: #808080">Transformer</a> <a href="/tags/git/" style="font-size: 14.75px; color: #707070">git</a> <a href="/tags/hexo/" style="font-size: 14.75px; color: #707070">hexo</a> <a href="/tags/macOS%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/" style="font-size: 16.63px; color: #484848">macOS基础操作</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size: 14.38px; color: #787878">二分查找</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/" style="font-size: 14.75px; color: #707070">二叉搜索树</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 16.63px; color: #484848">二叉树</a> <a href="/tags/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/" style="font-size: 15.13px; color: #686868">优先队列</a> <a href="/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/" style="font-size: 16.25px; color: #505050">位运算</a> <a href="/tags/%E5%87%A0%E4%BD%95/" style="font-size: 14px; color: #808080">几何</a> <a href="/tags/%E5%88%86%E6%B2%BB/" style="font-size: 14px; color: #808080">分治</a> <a href="/tags/%E5%89%8D%E7%BC%80%E5%92%8C/" style="font-size: 15.13px; color: #686868">前缀和</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 18.13px; color: #282828">动态规划</a> <a href="/tags/%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/" style="font-size: 14px; color: #808080">双向链表</a> <a href="/tags/%E5%8F%8C%E6%8C%87%E9%92%88/" style="font-size: 15.88px; color: #585858">双指针</a> <a href="/tags/%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/" style="font-size: 14px; color: #808080">哈希函数</a> <a href="/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/" style="font-size: 18.5px; color: #202020">哈希表</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size: 15.5px; color: #606060">回溯</a> <a href="/tags/%E5%A0%86/" style="font-size: 15.13px; color: #686868">堆</a> <a href="/tags/%E5%AD%97%E5%85%B8%E6%A0%91/" style="font-size: 14.75px; color: #707070">字典树</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size: 18.88px; color: #181818">字符串</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D/" style="font-size: 14px; color: #808080">字符串匹配</a> <a href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" style="font-size: 14px; color: #808080">并查集</a> <a href="/tags/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/" style="font-size: 15.88px; color: #585858">广度优先搜索</a> <a href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" style="font-size: 14px; color: #808080">归并排序</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 17.75px; color: #303030">排序</a> <a href="/tags/%E6%95%B0%E5%AD%A6/" style="font-size: 18.13px; color: #282828">数学</a> <a href="/tags/%E6%95%B0%E7%BB%84/" style="font-size: 19.25px; color: #101010">数组</a> <a href="/tags/%E6%95%B0%E8%AE%BA/" style="font-size: 14px; color: #808080">数论</a> <a href="/tags/%E6%9E%9A%E4%B8%BE/" style="font-size: 15.13px; color: #686868">枚举</a> <a href="/tags/%E6%A0%88/" style="font-size: 15.13px; color: #686868">栈</a> <a href="/tags/%E6%A0%91/" style="font-size: 17.75px; color: #303030">树</a> <a href="/tags/%E6%A8%A1%E6%8B%9F/" style="font-size: 17px; color: #404040">模拟</a> <a href="/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" style="font-size: 14px; color: #808080">正则表达式</a> <a href="/tags/%E6%B0%B4%E5%A1%98%E6%8A%BD%E6%A0%B7/" style="font-size: 14px; color: #808080">水塘抽样</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/" style="font-size: 17.38px; color: #383838">深度优先搜索</a> <a href="/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/" style="font-size: 15.5px; color: #606060">滑动窗口</a> <a href="/tags/%E6%BB%9A%E5%8A%A8%E5%93%88%E5%B8%8C/" style="font-size: 14px; color: #808080">滚动哈希</a> <a href="/tags/%E7%8A%B6%E6%80%81%E5%8E%8B%E7%BC%A9/" style="font-size: 14px; color: #808080">状态压缩</a> <a href="/tags/%E7%9F%A9%E9%98%B5/" style="font-size: 16.25px; color: #505050">矩阵</a> <a href="/tags/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/" style="font-size: 14px; color: #808080">组合数学</a> <a href="/tags/%E8%AE%A1%E6%95%B0/" style="font-size: 16.25px; color: #505050">计数</a> <a href="/tags/%E8%AE%B0%E5%BF%86%E5%8C%96%E6%90%9C%E7%B4%A2/" style="font-size: 14px; color: #808080">记忆化搜索</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size: 14px; color: #808080">论文笔记</a> <a href="/tags/%E8%AE%BE%E8%AE%A1/" style="font-size: 14.75px; color: #707070">设计</a> <a href="/tags/%E8%B4%AA%E5%BF%83/" style="font-size: 17.75px; color: #303030">贪心</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size: 14px; color: #808080">递归</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15.5px; color: #606060">链表</a> <a href="/tags/%E9%9A%8F%E6%9C%BA%E5%8C%96/" style="font-size: 14px; color: #808080">随机化</a>
    </div>
</div>



      </aside>
      <script>setLoadingBarProgress(60);</script>
    </div>
  </div>
  <footer id="footer" class="clearfix">

	<div class="social-wrapper">
  	
      
        <a href="https://github.com/yixuan004" class="social github"
          target="_blank" rel="external">
          <span class="icon icon-github"></span>
        </a>
      
        <a href="https://twitter.com/kevinsfork" class="social twitter"
          target="_blank" rel="external">
          <span class="icon icon-twitter"></span>
        </a>
      
        <a href="/atom.xml" class="social rss"
          target="_blank" rel="external">
          <span class="icon icon-rss"></span>
        </a>
      
    
  </div>
  
  <div>Theme <a target="_blank" rel="noopener" href='https://github.com/stkevintan/hexo-theme-material-flow' class="codename">MaterialFlow</a> designed by <a href="http://keyin.me/" target="_blank">Kevin Tan</a>.</div>
  
</footer>


  <script>setLoadingBarProgress(80);</script>
  

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src='//cdn.bootcss.com/node-waves/0.7.5/waves.min.js'></script>
<script src="//cdn.bootcss.com/scrollReveal.js/3.3.2/scrollreveal.min.js"></script>

<script src="/js/jquery.fitvids.js"></script>

<script>
	var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
	var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
	var ALGOLIA_API_KEY = "";
	var ALGOLIA_APP_ID = "";
	var ALGOLIA_INDEX_NAME = "";
  var AZURE_SERVICE_NAME = "";
  var AZURE_INDEX_NAME = "";
  var AZURE_QUERY_KEY = "";
  var BAIDU_API_ID = "";
  var SEARCH_SERVICE = "hexo";
  var ROOT = "/"||"/";
  if(!ROOT.endsWith('/'))ROOT += '/';
</script>

<script src="/js/search.js"></script>


<script src="/js/app.js"></script>



  <script>setLoadingBarProgress(100);</script>
</body>
</html>
