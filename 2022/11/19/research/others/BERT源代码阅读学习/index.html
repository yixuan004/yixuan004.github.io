

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Curious;">
  <meta name="keywords" content="">
  
    <meta name="description" content="BERT源代码阅读学习，主要是Transformer架构中的Encoder部分，各层的源代码理解与阅读学习">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT源代码阅读学习">
<meta property="og:url" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Curious;的个人划水博客">
<meta property="og:description" content="BERT源代码阅读学习，主要是Transformer架构中的Encoder部分，各层的源代码理解与阅读学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711105616573.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711105642016.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711110109758.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712195033334.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711135726421.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220713092751251.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711110109758.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220713165208863.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220713170145685.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712110214629.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712093703201.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711141124709.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711172228802.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711173534108.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711232228890.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711224750910.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711233904281.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712001457966.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712001307673.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711141742912.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711142519040.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712122136461.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712123154538.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712214108979.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712214133925.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712214114211.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712214155672.png">
<meta property="article:published_time" content="2022-11-19T03:30:41.000Z">
<meta property="article:modified_time" content="2022-11-20T07:29:25.712Z">
<meta property="article:author" content="Curious;">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711105616573.png">
  
  
  
  <title>BERT源代码阅读学习 - Curious;的个人划水博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Curious;的个人划水博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="BERT源代码阅读学习"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-11-19 11:30" pubdate>
          2022年11月19日 中午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          48k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          398 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">BERT源代码阅读学习</h1>
            
            
              <div class="markdown-body">
                
                <p>BERT源代码阅读学习，主要是Transformer架构中的Encoder部分，各层的源代码理解与阅读学习</p>
<span id="more"></span>

<h1 id="BERT源代码阅读学习"><a href="#BERT源代码阅读学习" class="headerlink" title="BERT源代码阅读学习"></a>BERT源代码阅读学习</h1><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>BERT源代码学习：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/360988428">https://zhuanlan.zhihu.com/p/360988428</a></p>
<p>Attention is all you need： <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a></p>
<p>BERT：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf&amp;usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ">https://arxiv.org/pdf/1810.04805.pdf&amp;usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ</a></p>
<p>Attention机制详解：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47282410">https://zhuanlan.zhihu.com/p/47282410</a></p>
<p>positional embedding absolute&#x2F;relative等不同方式：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/121126531">https://zhuanlan.zhihu.com/p/121126531</a></p>
<p>torch中的einsum：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361209187">https://zhuanlan.zhihu.com/p/361209187</a></p>
<p>Self-Attention with Relative Position Representations: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.02155.pdf">https://arxiv.org/pdf/1803.02155.pdf</a></p>
<h2 id="1-模型结构（论文-amp-Transformer架构截图）"><a href="#1-模型结构（论文-amp-Transformer架构截图）" class="headerlink" title="1. 模型结构（论文&amp;Transformer架构截图）"></a>1. 模型结构（论文&amp;Transformer架构截图）</h2><h3 id="1-1-论文-amp-Transformer架构截图"><a href="#1-1-论文-amp-Transformer架构截图" class="headerlink" title="1.1 论文&amp;Transformer架构截图"></a>1.1 论文&amp;Transformer架构截图</h3><h4 id="1-1-1-Transformer架构图"><a href="#1-1-1-Transformer架构图" class="headerlink" title="1.1.1 Transformer架构图"></a>1.1.1 Transformer架构图</h4><p>左边代表Encoder部分，右边代表Decoder部分。两边的区别个人理解是：</p>
<ul>
<li>Encoder是作为NLU（Natrual Language Understanding）来使用的，所以在输入的时候Encoder是能看到全局信息的。从目前接触到的任务来说还是Encoder这边的结构更加常用一些，大部分任务感觉还是属于在NLU的范畴，NLG那边的有些就显得不太好评测或者不是很靠谱；</li>
<li>但是在输入Decoder的时候，因为Decoder一般被NLG（Natural Language Generation）类的任务来使用，所以其需要根据上文来生成下文，故在输入的时候需要加mask，即 <code>Masked Multi-Head Attention</code>。此外在decoder部分中还有一个接收来自Encoder那边信息的Multi-Head Attention，也被称作 <code>encoder-decoder attention layer</code>，这个地方query来自于前一级的decoder层输出，但其key和value来自于encoder的输出，那么理解来说就是decoder的每一个位置作为key和encoder那边key计算相似度，然后聚合来自encoder那边的value信息；</li>
<li><font color="red">和同学讨论后补充：对于Transformer架构的信息，像T5这样的encoder-decoder模型，或者说像是一类依据文本生成文本的，比如翻译任务，那就是使用到整个Transformer架构，其中的encoder-decoder attention可以理解为我需要看着原来的文本来做生成，然后把query看做普通RNN架构中的x，这样x需要聚合来自全部输入文本的信息做attention；对于BERT这类就是只用到Encoder架构；对于GPT类的可能就只是用Decoder部分，里面就没有encoder-decoder attention那个部分了；</font></li>
</ul>
<img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711105616573.png" srcset="/img/loading.gif" lazyload alt="image-20220711105616573" style="zoom:50%;">

<h4 id="1-1-2-Multi-Head-Self-Attention"><a href="#1-1-2-Multi-Head-Self-Attention" class="headerlink" title="1.1.2 Multi-Head Self Attention"></a>1.1.2 Multi-Head Self Attention</h4><p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711105642016.png" srcset="/img/loading.gif" lazyload alt="image-20220711105642016"></p>
<h4 id="1-1-3-BERT-Embedding"><a href="#1-1-3-BERT-Embedding" class="headerlink" title="1.1.3 BERT Embedding"></a>1.1.3 BERT Embedding</h4><p>这个是bert模型结构的embedding输入，也需要联合代码看一下这个过程是怎么实现的。</p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711110109758.png" srcset="/img/loading.gif" lazyload alt="image-20220711110109758"></p>
<p>这里补充贴一张LUKE的图，虽然没看过但是看起来加了一个Entity Type Embedding，好像还是个比较有名的工作</p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712195033334.png" srcset="/img/loading.gif" lazyload alt="image-20220712195033334"></p>
<h2 id="2-代码学习"><a href="#2-代码学习" class="headerlink" title="2. 代码学习"></a>2. 代码学习</h2><h3 id="2-1-基础简化pipeline代码"><a href="#2-1-基础简化pipeline代码" class="headerlink" title="2.1 基础简化pipeline代码"></a>2.1 基础简化pipeline代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertModel, BertConfig<br><span class="hljs-keyword">from</span> transformers.models.bert <span class="hljs-keyword">import</span> modeling_bert  <span class="hljs-comment"># 从这里看源代码</span><br><br><span class="hljs-comment"># 预训练模型加载</span><br>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;./bert_base_uncased&quot;</span>)<br>model = BertModel.from_pretrained(<span class="hljs-string">&quot;./bert_base_uncased&quot;</span>)<br><br><span class="hljs-comment"># 输入&amp;输出</span><br>text = <span class="hljs-string">&quot;Germany beat Argentina 2-0 in the World Cup Final.&quot;</span><br>encoded_input = tokenizer(text, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>output = model(**encoded_input)<br><br><span class="hljs-comment"># 获取一句话的输出，还有cls token的输出</span><br><span class="hljs-built_in">print</span>(output[<span class="hljs-string">&#x27;pooler_output&#x27;</span>].shape)  <span class="hljs-comment"># torch.Size([1, 768])</span><br><span class="hljs-built_in">print</span>(output[<span class="hljs-string">&#x27;last_hidden_state&#x27;</span>].shape)  <span class="hljs-comment"># torch.Size([1, 14, 768])</span><br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure>

<p>总结：加载<strong>config.json</strong>、<strong>vocab.txt</strong>还有<strong>pytorch_model.bin</strong>三个文件。其中通过 <code>from_pretrained(&quot;./bert_base_uncased&quot;)</code>进行指定路径，如果不指定路径的话好像会从huggingface那边下载model，指定路径的话就需要文件夹下有这三个文件；</p>
<h3 id="2-2-model"><a href="#2-2-model" class="headerlink" title="2.2 model"></a>2.2 model</h3><h4 id="2-2-1-embeddings、encoder、pooler（※重点）"><a href="#2-2-1-embeddings、encoder、pooler（※重点）" class="headerlink" title="2.2.1 embeddings、encoder、pooler（※重点）"></a>2.2.1 embeddings、encoder、pooler（※重点）</h4><p>通过<code>model = BertModel.from_pretrained(&quot;./bert_base_uncased&quot;)</code>加载模型后，首先可以在这里调试model这个对象包含的内容，model是BertModel的实例化，模型结构主要由<code>model.embeddings </code>（BERTEmbeddings类对象），<code>model.encoder</code>（BertEncoder类对象），<code>model.pooler</code>（BertPooler对象）组成。点开后可以看到各个地方的模型结构与层数，之后会随着模型调试查看数据流向和数据维度的变化。</p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711135726421.png" srcset="/img/loading.gif" lazyload alt="image-20220711135726421"></p>
<h5 id="·-class-BertEmbeddings层结构"><a href="#·-class-BertEmbeddings层结构" class="headerlink" title="· class BertEmbeddings层结构"></a>· class BertEmbeddings层结构</h5><blockquote>
<p>&#x2F;Users&#x2F;curious&#x2F;opt&#x2F;miniconda3&#x2F;envs&#x2F;venv2&#x2F;lib&#x2F;python3.9&#x2F;site-packages&#x2F;transformers&#x2F;models&#x2F;bert&#x2F;modeling_bert.py</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertEmbeddings</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>

<h6 id="1-init"><a href="#1-init" class="headerlink" title="1) init"></a>1) <strong>init</strong></h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>    <span class="hljs-built_in">super</span>().__init__()<br>    self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)<br>    self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)<br>    self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)<br><br>    <span class="hljs-comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span><br>    <span class="hljs-comment"># any TensorFlow checkpoint file</span><br>    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)<br>    self.dropout = nn.Dropout(config.hidden_dropout_prob)<br>    <span class="hljs-comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span><br>    self.position_embedding_type = <span class="hljs-built_in">getattr</span>(config, <span class="hljs-string">&quot;position_embedding_type&quot;</span>, <span class="hljs-string">&quot;absolute&quot;</span>)<br>    self.register_buffer(<span class="hljs-string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)))<br>    <span class="hljs-keyword">if</span> version.parse(torch.__version__) &gt; version.parse(<span class="hljs-string">&quot;1.6.0&quot;</span>):<br>        self.register_buffer(<br>            <span class="hljs-string">&quot;token_type_ids&quot;</span>,<br>            torch.zeros(self.position_ids.size(), dtype=torch.long),<br>            persistent=<span class="hljs-literal">False</span>,<br>        )<br></code></pre></td></tr></table></figure>

<p>简单对init进行解释，这里有三个<code>nn.Embedding</code>层：</p>
<ul>
<li>self.word_embeddings：维度从vocab_size（30522）到hidden_size（768）转化，<font color="red">TODO：padding_idx的参数是做什么用的？</font> </li>
<li>self.position_embeddings：维度从max_position_embeddings（512）到hidden_size（768）转化；</li>
<li>self.token_type_embeddings：维度从config.type_vocab_size（2，这里的2代表的是有两种类别的，第一个[SEP]前都是0，第二个[SEP]前都是1，这样交叉的）到到hidden_size（768），或者是用来表示padding地方的差异；</li>
</ul>
<p>这里的<code>self.LayerNorm</code>和<code>self.dropout</code>是剩下两个和forward比较相关的层，初始化都比较正常</p>
<p><font color="red">和同学讨论下这个nn.Embedding层的用处，之前对这个层一直不是太理解，大概目前的理解是传入的一个比如input_ids是[1, 14]这个shape的，首先其被转化成一个one-hot的表示也就是[1, 14, 30522(这个维度类似一个词典大小)]，然后过一个[30522, 768]的，两个乘在一起就有一种对应位把元素取出来的感觉，这样就得到了最终的embedding表示[1, 14, 768]</font> </p>
<p><font color="red">词表大小30522是针对input_ids embedding的，那么针对positional embedding就是max_seq_len，针对token type的就是2（只有0和1代表两类交替的）</font></p>
<h6 id="2-forward"><a href="#2-forward" class="headerlink" title="2) forward"></a>2) forward</h6><p>forward传入的参数中</p>
<ul>
<li>input_ids **[1, seq_len]**：tensor([[ 101, 2762, 3786, 5619, 1016, 1011, 1014, 1998, 2180, 1996, 2088, 2452, 2345,  102]])，这是tokenizer.convert_tokens_to_ids()的结果应该，那边BERT好像还对应了个wordpiecetoken，101是[CLS]，102是[PAD]</li>
<li>token_type_ids **[1, seq_len]**：tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])，传入的没有[SEP]，只有一类token</li>
<li>position_ids：暂时为None</li>
<li>inputs_embeds：暂时为None</li>
</ul>
<p>step1：根据input_ids提取得到的seq_len长度，初始化position_ids **[1, seq_len]**：tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]])</p>
<p>step2：获取input_embeds和token_type_embeddings，通过上面的传入参数以及nn.Embedding层，并把这两个加在一起</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>    inputs_embeds = self.word_embeddings(input_ids)<br>token_type_embeddings = self.token_type_embeddings(token_type_ids)<br><br>embeddings = inputs_embeds + token_type_embeddings<br></code></pre></td></tr></table></figure>

<p>embeddings.shape <strong>[1, seq_len, hidden_dim]</strong></p>
<p>step3：如果self.position_embedding_type是”absolute”绝对的话，就传入后加上position_embeddings，此时embeddings.shape **[1, seq_len, hidden_dim]**没有变化；absolute就是绝对位置编码，理解是[0, 1, 2, 3…]这样的绝对位置；<font color="red">还有一种position_embedding是相对位置编码的embedding，部分代码整合在了BertSelfAttention这个类中，博客参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/chenf1995/article/details/122971023">https://blog.csdn.net/chenf1995/article/details/122971023</a></font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> self.position_embedding_type == <span class="hljs-string">&quot;absolute&quot;</span>:<br>    position_embeddings = self.position_embeddings(position_ids)<br>    embeddings += position_embeddings<br></code></pre></td></tr></table></figure>

<p>step4：过LayerNorm和dropout，维度不会改变，BertEmbeddings这个类最终输出了一个embeddings <strong>[1, seq_len, hidden_dim]<strong>的信息，代表将要输入进入encoder结构部分的embedding</strong>input_embedding+token_type_embedding+position_embedding</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">embeddings = self.LayerNorm(embeddings)<br>embeddings = self.dropout(embeddings)<br><span class="hljs-keyword">return</span> embeddings<br></code></pre></td></tr></table></figure>

<h6 id="3-综合别人博客做一个总结"><a href="#3-综合别人博客做一个总结" class="headerlink" title="3) 综合别人博客做一个总结"></a>3) 综合别人博客做一个总结</h6><p>word_embeddings是上文中subword tokenization对应的词嵌入；</p>
<p>token_type_embeddings是用于表示当前词所在的句子，辅助区别句子与padding，句子对通过[SEP]分隔之间的差异；</p>
<p>position_embeddings是句子中每个词的位置嵌入，用于区别词的顺序，博客说这个地方是训练出来的（从代码看确实如此），而不是计算得到固定嵌入，可能固定嵌入不利于拓展；</p>
<p>三个embedding层不带权重直接加在一起，过LayerNorm+dropout后产生输出，大小为**[batch_size, seq_len, hidden_size]**</p>
<h6 id="4-补充：positional-embedding的不同方式"><a href="#4-补充：positional-embedding的不同方式" class="headerlink" title="4) 补充：positional embedding的不同方式"></a>4) 补充：positional embedding的不同方式</h6><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/121126531">https://zhuanlan.zhihu.com/p/121126531</a></p>
<p>背景：</p>
<p>词与词之间的顺序关系往往影响整个句子的含义，因此在对文本数据进行建模的时候需要考虑词与词之间的顺序关系；</p>
<p>建模文本中的顺序关系必须要使用positional encoding吗？-&gt; 不一定，只有使用位置不敏感的模型对文本数据进行建模的时候，才需要额外使用positional encoding；如果模型的输出会随着输入文本数据顺序的变化而变化，那么这个模型就是关于位置敏感的，反之则是位置不敏感的；</p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220713092751251.png" srcset="/img/loading.gif" lazyload alt="image-20220713092751251"></p>
<p>在常用的文本模型中，RNN类的就是关于位置敏感的，使用RNN类模型对文本数据建模的时候，模型结构天然考虑了文本中词与词之间的顺序关系。<strong>而以attention机制为核心的transformer则是位置不敏感的，使用这一类位置不敏感的模型的时候需要额外加入positional encoding引入文本中词与词的顺序关系；</strong></p>
<p>具体操作：</p>
<p>对于transformer模型的positional encoding有两种主流方式：即绝对位置编码和相对位置编码</p>
<p>其中<strong>absolute positional embedding（绝对位置编码）</strong>是相对简单理解的，直接对不同位置随机初始化一个positional embedding，加到word embedding和token_type embedding上输入模型作为参数进行训练</p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711110109758.png" srcset="/img/loading.gif" lazyload alt="image-20220711110109758"></p>
<p>另一种是<strong>relative positional embedding（相对位置编码）</strong>，首先motivation是不同位置的positional embedding固然不同，但是位置1和位置2的距离比位置3和位置10的距离更近，位置1 2和3 4距离都只差1，这些关于位置的<strong>相对含义</strong>模型通过绝对位置编码是否能够学习？绝对位置编码没有约束位置之间这些隐含关系，只能期待他隐式的学习到，所以是否有更合理的方法能够显式的让模型理解位置的相对关系？</p>
<p><font color="blue">11111</font> </p>
<p>详细看一下huggingface transformer代码中的这个部分，参数有”absolute”、”relative_key”和”relative_key_query”三种，这些参数在<code>class BertSelfAttention(nn.Module)</code>这个类中，而不是在最开始的<code>BertEmbedding</code>那块的</p>
<ul>
<li><code>absolute</code>：默认值，这部分就不用处理（对这个地方的处理在Embedding层）</li>
<li><code>relative_key</code>：对key_layer作处理，将其与这里的<code>positional_embedding</code>和key矩阵相乘作为key相关的位置编码；</li>
<li><code>relative_key_query</code>：对key和value都进行相乘以作为位置编码。</li>
</ul>
<p><font color="red">用下面代码简单加一下注释</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertSelfAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, position_embedding_type=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        ...<br>        <span class="hljs-keyword">if</span> self.position_embedding_type == <span class="hljs-string">&quot;relative_key&quot;</span> <span class="hljs-keyword">or</span> self.position_embedding_type == <span class="hljs-string">&quot;relative_key_query&quot;</span>:<br>          self.max_position_embeddings = config.max_position_embeddings  <span class="hljs-comment"># 512</span><br>          self.distance_embedding = nn.Embedding(<span class="hljs-number">2</span> * config.max_position_embeddings - <span class="hljs-number">1</span>, self.attention_head_size)  <span class="hljs-comment"># [512*2-1, 64 (即hidden//head_num)]</span><br>        ...<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span><br>attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>))<br><br><span class="hljs-keyword">if</span> self.position_embedding_type == <span class="hljs-string">&quot;relative_key&quot;</span> <span class="hljs-keyword">or</span> self.position_embedding_type == <span class="hljs-string">&quot;relative_key_query&quot;</span>:<br>    seq_length = hidden_states.size()[<span class="hljs-number">1</span>]  <span class="hljs-comment"># 14</span><br>    position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># [14, 1]shape的tensor</span><br>    position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>) <span class="hljs-comment"># [1, 14]</span><br>    distance = position_ids_l - position_ids_r  <span class="hljs-comment"># [seqlen, seq_len]</span><br>    positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - <span class="hljs-number">1</span>)  <span class="hljs-comment"># [seqlen, seqlen, hidden_size]</span><br>    positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  <span class="hljs-comment"># fp16 compatibility</span><br><br>    <br>    <span class="hljs-comment"># query_layer : batchsize, seqlen, hidden -&gt; batchsize, head_num, seqlen, hidden//head_num【multi-head】</span><br><br>    <span class="hljs-comment"># bhld,lrd -&gt; bhld不变，lrd去掉一个维度变成rd -&gt; bhld不变, rd转置变成dr -&gt; 乘 -&gt; bhlr</span><br>    <span class="hljs-keyword">if</span> self.position_embedding_type == <span class="hljs-string">&quot;relative_key&quot;</span>:<br>        relative_position_scores = torch.einsum(<span class="hljs-string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding)<br>        attention_scores = attention_scores + relative_position_scores<br>    <span class="hljs-keyword">elif</span> self.position_embedding_type == <span class="hljs-string">&quot;relative_key_query&quot;</span>:<br>        relative_position_scores_query = torch.einsum(<span class="hljs-string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding)<br>        relative_position_scores_key = torch.einsum(<span class="hljs-string">&quot;bhrd,lrd-&gt;bhlr&quot;</span>, key_layer, positional_embedding)<br>        attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key<br></code></pre></td></tr></table></figure>

<ul>
<li><code>seq_length</code>：这句话的长度，比如14</li>
<li><code>position_ids_l</code>：初始化是一个例如[14, 1]的向量，存储的类似于[[0], [1], [2] …]这样的</li>
<li><code>position_ids_r</code>：初始化是一个例如[1, 14]的向量，存储的类似于[[0, 1, 2, 3, 4]]这样的</li>
<li><code>distance</code>：初始化直接用<code>position_ids_l</code>-<code>position_ids_r</code>，这里直接广播减法，是一个[14, 14]维度的</li>
</ul>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220713165208863.png" srcset="/img/loading.gif" lazyload alt="image-20220713165208863"></p>
<p>因为这个地方是在attention这块来做的embedding，attention那边的scoreshape是[batch, head, seq_len, seq_len]的，代表query每个位置处对于key的注意力，那么可以在这里对query和key都搞positional embedding</p>
<p>通过上面几个做操作搞了一个<code>positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)</code>，这个有点不为什么每个要把512-1给加上，这样处理完后distance变成了如下所示的tensor</p>
<p><font color="red">两个距离相隔最远是512，那么这样处理后能保证所有数字都是&gt;&#x3D;0的，因为离的最远的也就是512了，然后最远的将会到达1023那个感觉</font> </p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220713170145685.png" srcset="/img/loading.gif" lazyload alt="image-20220713170145685"></p>
<p>positional_embedding由distance_embedding层后得到，distance_embedding层的传入参数是[512<em>2-1, 64 (即hidden&#x2F;&#x2F;head_num)]也能理解了，因为词表大小是差不多0-1023的；；positional_embedding的shape是</em>*[seq_len, seq_len, hidden]**的，如果是一个batch的话，那么应该是这个batch里面最大的那个seq_len？</p>
<p>下面代码把query_layer[1, 12, 14, 64]和positional_embedding[14, 14, 64]作为这个<code>torch.einsum</code>的输入，这个地方参考文档<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361209187%EF%BC%8C%E5%B0%B1%E6%98%AF%E6%8A%8A%E5%BD%A2%E7%8A%B6bhld,lrd%E7%9A%84%E4%B8%A4%E4%B8%AAtensor%E5%8A%A0%E6%88%90%E4%B8%80%E4%B8%AAbhlr%E7%9A%84%EF%BC%8C%E8%BF%99%E9%87%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B2%A1%E7%94%A8%E4%B8%A4%E4%B8%AAl%E5%8F%AF%E8%83%BD%E6%98%AF%E5%9B%A0%E4%B8%BA%E5%89%8D%E9%9D%A2%E4%B8%A4%E4%B8%AAseq_len%E6%9C%AC%E8%B4%A8%E4%B8%8A%E4%B8%80%E4%B8%AA%E6%9D%A5%E8%87%AA%E4%BA%8Equery%EF%BC%8C%E4%B8%80%E4%B8%AA%E6%9D%A5%E8%87%AA%E4%BA%8Ekey%EF%BC%8C%E8%80%8C%E5%AE%9E%E9%99%85%E4%B8%8A%E6%98%AF%E4%B8%8D%E9%9C%80%E8%A6%81%E7%AD%89%E9%95%BF%E7%9A%84%EF%BC%8C%E5%8F%AA%E6%98%AF%E4%B8%80%E8%88%AC%E6%93%8D%E4%BD%9C%E9%BB%98%E8%AE%A4%E4%B8%BA%E7%AD%89%E9%95%BF%E7%9A%84%E4%BA%86%EF%BC%9B">https://zhuanlan.zhihu.com/p/361209187，就是把形状bhld,lrd的两个tensor加成一个bhlr的，这里为什么没用两个l可能是因为前面两个seq_len本质上一个来自于query，一个来自于key，而实际上是不需要等长的，只是一般操作默认为等长的了；</a></p>
<p>重点：这里以第一个作为示例，l和d在前后的箭头中都出现了，那就是在这两个维度上操作,query_layer[1, 12, <strong>14</strong>, <strong>64</strong>]和positional_embedding[<strong>14</strong>, 14, <strong>64</strong>]，转置乘，出来一个relative_position_scores_query**[1, 12, 14, 14]**的，聚合来自position的信息</p>
<p><font color="red">TODO：还弄得没那么明白，大概明白个意思，之后还要详细看看</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">relative_position_scores_query = torch.einsum(<span class="hljs-string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding)<br>relative_position_scores_key = torch.einsum(<span class="hljs-string">&quot;bhrd,lrd-&gt;bhlr&quot;</span>, key_layer, positional_embedding)<br></code></pre></td></tr></table></figure>

<p>最后，执行下述代码，注意这个<strong>相对位置编码</strong>过程可以只对query做，也可以对query和key同时做</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key<br></code></pre></td></tr></table></figure>

<p><font color="red">TODO：</font> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/121126531%E8%BF%99%E9%87%8C%E8%BF%98%E4%BB%8B%E7%BB%8D%E5%88%B0%E4%BA%86%EF%BC%9ASinusoidal">https://zhuanlan.zhihu.com/p/121126531这里还介绍到了：Sinusoidal</a> Position Encoding和Complex embedding</p>
<h5 id="·class-BertSelfAttention：被BertAttention调用（※重点）"><a href="#·class-BertSelfAttention：被BertAttention调用（※重点）" class="headerlink" title="·class BertSelfAttention：被BertAttention调用（※重点）"></a>·class BertSelfAttention：被BertAttention调用（※重点）</h5><h6 id="1-init-1"><a href="#1-init-1" class="headerlink" title="1) init"></a>1) init</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertSelfAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, position_embedding_type=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(config, <span class="hljs-string">&quot;embedding_size&quot;</span>):<br>            <span class="hljs-keyword">raise</span> ValueError(<br>                <span class="hljs-string">f&quot;The hidden size (<span class="hljs-subst">&#123;config.hidden_size&#125;</span>) is not a multiple of the number of attention &quot;</span><br>                <span class="hljs-string">f&quot;heads (<span class="hljs-subst">&#123;config.num_attention_heads&#125;</span>)&quot;</span><br>            )<br><br>        self.num_attention_heads = config.num_attention_heads<br>        self.attention_head_size = <span class="hljs-built_in">int</span>(config.hidden_size / config.num_attention_heads)<br>        self.all_head_size = self.num_attention_heads * self.attention_head_size<br><br>        self.query = nn.Linear(config.hidden_size, self.all_head_size)<br>        self.key = nn.Linear(config.hidden_size, self.all_head_size)<br>        self.value = nn.Linear(config.hidden_size, self.all_head_size)<br><br>        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)<br>        self.position_embedding_type = position_embedding_type <span class="hljs-keyword">or</span> <span class="hljs-built_in">getattr</span>(<br>            config, <span class="hljs-string">&quot;position_embedding_type&quot;</span>, <span class="hljs-string">&quot;absolute&quot;</span><br>        )<br>        <span class="hljs-keyword">if</span> self.position_embedding_type == <span class="hljs-string">&quot;relative_key&quot;</span> <span class="hljs-keyword">or</span> self.position_embedding_type == <span class="hljs-string">&quot;relative_key_query&quot;</span>:<br>            self.max_position_embeddings = config.max_position_embeddings<br>            self.distance_embedding = nn.Embedding(<span class="hljs-number">2</span> * config.max_position_embeddings - <span class="hljs-number">1</span>, self.attention_head_size)<br><br>        self.is_decoder = config.is_decoder<br></code></pre></td></tr></table></figure>

<p>这个地方是整个BERT架构中非常核心的区域</p>
<ul>
<li>self.num_attention_heads &#x3D; config.num_attention_heads：几头注意力机制，在config文件里这里设置为12（一般BERT也是12）</li>
<li>self.attention_head_size &#x3D; int(config.hidden_size &#x2F; config.num_attention_heads)：config.hidden_size是768，所以每个头的hidden_size将会是768&#x2F;12&#x3D;64；</li>
<li>self.all_head_size是self.num_attention_heads（12）再乘回self.attention_head_size（64），猜测这样的原因是因为整除造成的可能回来后就不是768了；<font color="red">从其他博客也看到和剪枝有关</font> </li>
<li>self.query、self.key、self.value三个权重矩阵，都是一个hidden_size（768）到内部这个all_head_size（可能是768，也可能有损失）的转化；</li>
<li>self.dropout &#x3D; nn.Dropout(config.attention_probs_dropout_prob)：简单的dropout层；</li>
<li>self.position_embedding_type：这与相对&#x2F;绝对位置编码有关，如果是绝对位置编码那么在BertEmbedding层里面已经给结合进去了，<font color="blue">如果是相对位置编码要在这里实现，不过这个地方暂时先跳过了</font>；</li>
<li>self.is_decoder &#x3D; config.is_decoder：标识是否decoder，BERT只是一个encoder就不涉及到这个部分了；</li>
</ul>
<h6 id="2-forward-1"><a href="#2-forward-1" class="headerlink" title="2) forward"></a>2) forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">transpose_for_scores</span>(<span class="hljs-params">self, x</span>):<br>    new_x_shape = x.size()[:-<span class="hljs-number">1</span>] + (self.num_attention_heads, self.attention_head_size)<br>    x = x.view(new_x_shape)<br>    <span class="hljs-keyword">return</span> x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure>

<p>首先是这个函数，这个函数是拆多头用的，输入的x是[batch, seq_len, hidden]的，先指定new_shape是[batch, seq_len, num_attention_heads, attention_head_size]（一般可以认为是[batch, seq_len, 12, 64]），然后.view转化，然后再通过permute改变顺序为**[batch, attention_head_size, seq_len, num_attention_heads]**，这样是因为attention_head_size可以归为”batch“那边的维度了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    hidden_states: torch.Tensor,</span><br><span class="hljs-params">    attention_mask: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    head_mask: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    encoder_hidden_states: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    encoder_attention_mask: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    past_key_value: <span class="hljs-type">Optional</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[torch.FloatTensor]]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    output_attentions: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Tuple</span>[torch.Tensor]:<br>    mixed_query_layer = self.query(hidden_states)<br><br>		<span class="hljs-comment"># 忽略了cross-attention部分</span><br>    <span class="hljs-comment"># hidden_states [batch, seqlen, hidden]</span><br>    key_layer = self.transpose_for_scores(self.key(hidden_states))  <span class="hljs-comment"># [batch, num_head, seqlen_key, hidden//num_head]</span><br>    value_layer = self.transpose_for_scores(self.value(hidden_states))<br>    query_layer = self.transpose_for_scores(mixed_query_layer) <span class="hljs-comment"># [batch, num_head, seqlen_query, hidden//num_head]</span><br><br>    <span class="hljs-comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span><br>    attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>))  <span class="hljs-comment"># </span><br><br>    <span class="hljs-comment"># 忽略了相对位置编码的处理</span><br><br>    attention_scores = attention_scores / math.sqrt(self.attention_head_size)<br>    <span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function)</span><br>        attention_scores = attention_scores + attention_mask<br><br>    <span class="hljs-comment"># Normalize the attention scores to probabilities.</span><br>    attention_probs = nn.functional.softmax(attention_scores, dim=-<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># This is actually dropping out entire tokens to attend to, which might</span><br>    <span class="hljs-comment"># seem a bit unusual, but is taken from the original Transformer paper.</span><br>    attention_probs = self.dropout(attention_probs)<br><br>    <span class="hljs-comment"># Mask heads if we want to</span><br>    <span class="hljs-keyword">if</span> head_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        attention_probs = attention_probs * head_mask<br><br>    context_layer = torch.matmul(attention_probs, value_layer)  <span class="hljs-comment"># [batch, num_head, seqlen, hidden//num_head]</span><br><br>    context_layer = context_layer.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous()<br>    new_context_layer_shape = context_layer.size()[:-<span class="hljs-number">2</span>] + (self.all_head_size,)<br>    context_layer = context_layer.view(new_context_layer_shape)<br><br>    outputs = (context_layer, attention_probs) <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">else</span> (context_layer,)<br><br>    <span class="hljs-comment"># 忽略了is_decoder部分</span><br>    <br>    <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>

<p>上面贴出来的这段代码省略了针对<code>is_cross_attention</code>（即encoder-decoder attention那个部分的一些处理），此外还忽略了<code>if self.is_decoder:</code>部分的处理，并且忽略了<code>if self.position_embedding_type == &quot;relative_key&quot; </code>相对位置编码部分的处理；</p>
<p>step1：首先是这个部分，hidden_states**[batch, seq_len, hidden_size]<strong>这个tensor过了self.query、self.value、self.key三个linear矩阵，由于这三个linear一般不改变hidden_size，这样得到的是三个</strong>[batch, seq_len, hidden_size]<strong>形状的tensor，通过上面提到的transpose_for_scores进行reshape，得到三个</strong>[batch, head_num, seq_len, attention_head_size]**（一般可以是[1, 12, seq_len, 768]）这样的tensor，并且被命名为key_layer、value_layer、query_layer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">mixed_query_layer = self.query(hidden_states)<br>key_layer = self.transpose_for_scores(self.key(hidden_states))<br>value_layer = self.transpose_for_scores(self.value(hidden_states))<br>query_layer = self.transpose_for_scores(mixed_query_layer)<br></code></pre></td></tr></table></figure>

<p>step2：这里就是Q·K^T那个部分了，转置就是在后两个维度上转置，输出的attention_scores是**[batch, head_num, seq_len, seq_len]**形状的tensor，代表query中每个位置处对key全局所有的注意力（后面要过softmax）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure>

<p>step3：依照博客简单理解一下不同的positional_embedding_type，<font color="blue">这个部分暂时忽略了</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">absolute：默认值，这部分就不用处理；<br>relative_key：对key_layer作处理，将其与这里的positional_embedding和key矩阵相乘作为key相关的位置编码；<br>relative_key_query：对key和value都进行相乘以作为位置编码。<br></code></pre></td></tr></table></figure>

<p>step4：计算attention_scores，attention_probs；attention_scores在计算query和key的点乘后除以根号下d_k，<strong>注意这里的self.attention_head_size是64那个地方的，也就是分成12个头后每个头的hidden_size</strong>，如果带有attention_mask的话<font color="red">（注意，一般来说肯定是会有atttention_mask的，应该会在调用这个BertAttention的时候传给他，因为一个batch中大家不等长，肯定要通过mask padding到512这种感觉的）</font> ；；在计算attention_scores时候用的是加法，因为softmax那块要一个很大的负数，比如-1e9这样的，然后过softmax，注意softmax的维度是-1代表query中每个token对所有key位置处的token的attention；；；最后过一个self.dropout，<font color="red">TODO：暂时有点没理解为什么在这里过dropout，而不是乘了之后</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span><br>attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>))<br><br><span class="hljs-comment"># 忽略了相对位置编码的处理</span><br><br>attention_scores = attention_scores / math.sqrt(self.attention_head_size)<br><span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    <span class="hljs-comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function)</span><br>    attention_scores = attention_scores + attention_mask<br><br><span class="hljs-comment"># Normalize the attention scores to probabilities.</span><br>attention_probs = nn.functional.softmax(attention_scores, dim=-<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># This is actually dropping out entire tokens to attend to, which might</span><br><span class="hljs-comment"># seem a bit unusual, but is taken from the original Transformer paper.</span><br>attention_probs = self.dropout(attention_probs)<br></code></pre></td></tr></table></figure>

<p>step5：这里主要就是输出整合了，再reshape回去，变成了**[batch, seq_len, hidden_size]**的这个形状，另外看到config中output_attentions那个参数的作用，要不要把每层的这个attention返回回去，至此<code>class BertSelfAttention(nn.Module)</code>这个地方的forward结束了；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">context_layer = torch.matmul(attention_probs, value_layer)<br><br>context_layer = context_layer.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous()<br>new_context_layer_shape = context_layer.size()[:-<span class="hljs-number">2</span>] + (self.all_head_size,)<br>context_layer = context_layer.view(new_context_layer_shape)<br><br>outputs = (context_layer, attention_probs) <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">else</span> (context_layer,)<br><span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>

<h5 id="·class-BertSelfOutput-被BertAttention调用"><a href="#·class-BertSelfOutput-被BertAttention调用" class="headerlink" title="·class BertSelfOutput: 被BertAttention调用"></a>·class BertSelfOutput: 被BertAttention调用</h5><h6 id="1-init-amp-forward"><a href="#1-init-amp-forward" class="headerlink" title="1) init&amp;forward"></a>1) init&amp;forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertSelfOutput</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dense = nn.Linear(config.hidden_size, config.hidden_size)<br>        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)<br>        self.dropout = nn.Dropout(config.hidden_dropout_prob)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states: torch.Tensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:<br>        hidden_states = self.dense(hidden_states)<br>        hidden_states = self.dropout(hidden_states)<br>        hidden_states = self.LayerNorm(hidden_states + input_tensor)<br>        <span class="hljs-keyword">return</span> hidden_states<br></code></pre></td></tr></table></figure>

<p>这个地方代码结构是相对比较简单的，<font color="red"><strong>这里也展现出了BERT中存在的一层add&amp;norm操作，这里应该还只是attention这个部分的内容</strong></font> </p>
<h5 id="·class-BertAttention：被BertLayer调用"><a href="#·class-BertAttention：被BertLayer调用" class="headerlink" title="·class BertAttention：被BertLayer调用"></a>·class BertAttention：被BertLayer调用</h5><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712110214629.png" srcset="/img/loading.gif" lazyload alt="image-20220712110214629" style="zoom:50%;">

<h6 id="1-init-2"><a href="#1-init-2" class="headerlink" title="1) init"></a>1) init</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, position_embedding_type=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)<br>        self.output = BertSelfOutput(config)<br>        self.pruned_heads = <span class="hljs-built_in">set</span>()<br></code></pre></td></tr></table></figure>

<p>attention的实现还是不在这里，self.self这个是multi-head self attention机制的实现，self.output的操作是第一个这里完成的部分；</p>
<p>该层中使用到了<code>self.pruned_heads = set()</code>这样一种节约显存的技术，暂时没有了解太深；</p>
<h6 id="2-forward-2"><a href="#2-forward-2" class="headerlink" title="2) forward"></a>2) forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    hidden_states: torch.Tensor,</span><br><span class="hljs-params">    attention_mask: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    head_mask: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    encoder_hidden_states: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    encoder_attention_mask: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    past_key_value: <span class="hljs-type">Optional</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[torch.FloatTensor]]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    output_attentions: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Tuple</span>[torch.Tensor]:<br>    self_outputs = self.self(<br>        hidden_states,<br>        attention_mask,<br>        head_mask,<br>        encoder_hidden_states,<br>        encoder_attention_mask,<br>        past_key_value,<br>        output_attentions,<br>    )<br>    attention_output = self.output(self_outputs[<span class="hljs-number">0</span>], hidden_states)<br>    outputs = (attention_output,) + self_outputs[<span class="hljs-number">1</span>:]  <span class="hljs-comment"># add attentions if we output them</span><br>    <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>

<p>有了上面的<code>BertSelfAttention</code>和<code>BertSelfOutput</code>后，这个组件就比较好理解了</p>
<h5 id="·class-BertIntermediate-被BertLayer调用"><a href="#·class-BertIntermediate-被BertLayer调用" class="headerlink" title="·class BertIntermediate: 被BertLayer调用"></a>·class BertIntermediate: 被BertLayer调用</h5><p>在BertAttention这个模块后，还有一个FFNN的操作，这里包含有激活函数；<font color="red">TODO：为什么有些地方需要激活函数，有些地方就不用？像CV那边的话，经常几个层过后就来一个激活，但是这里比如BertAttention里面就没有激活</font> </p>
<h6 id="1-init-amp-forward-1"><a href="#1-init-amp-forward-1" class="headerlink" title="1) init&amp;forward"></a>1) init&amp;forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertIntermediate</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(config.hidden_act, <span class="hljs-built_in">str</span>):<br>            self.intermediate_act_fn = ACT2FN[config.hidden_act]<br>        <span class="hljs-keyword">else</span>:<br>            self.intermediate_act_fn = config.hidden_act<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states: torch.Tensor</span>) -&gt; torch.Tensor:<br>        hidden_states = self.dense(hidden_states)<br>        hidden_states = self.intermediate_act_fn(hidden_states)<br>        <span class="hljs-keyword">return</span> hidden_states<br></code></pre></td></tr></table></figure>

<p>这个里面调用了<code>config.hidden_act</code>，在config文件那边的话这个地方是<code>&quot;gelu&quot;</code>，对应的也就是gelu激活函数，整体来看这个层结构还是很简单的，<font color="red">其中注意dense这个层把768转化为一个config.intermediate_size3072了</font> </p>
<h5 id="·class-BertOutput-被BertLayer调用"><a href="#·class-BertOutput-被BertLayer调用" class="headerlink" title="·class BertOutput: 被BertLayer调用"></a>·class BertOutput: 被BertLayer调用</h5><p><font color="red">注意这里不是BertSelfOutput，刚才那个是中间层的，这个是一个BLOCK的</font> </p>
<h6 id="1-init-amp-forward-2"><a href="#1-init-amp-forward-2" class="headerlink" title="1) init&amp;forward"></a>1) init&amp;forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertOutput</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)<br>        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)<br>        self.dropout = nn.Dropout(config.hidden_dropout_prob)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states: torch.Tensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:<br>        hidden_states = self.dense(hidden_states)<br>        hidden_states = self.dropout(hidden_states)<br>        hidden_states = self.LayerNorm(hidden_states + input_tensor)<br>        <span class="hljs-keyword">return</span> hidden_states<br></code></pre></td></tr></table></figure>

<p>主要负责的也是一些整合，还有residual的部分，<font color="red">其中注意dense层把intermidiate_size又转化会config.hidden_size了</font> </p>
<h5 id="·-class-BertLayer-nn-Module-：被BertEncoder调用"><a href="#·-class-BertLayer-nn-Module-：被BertEncoder调用" class="headerlink" title="· class BertLayer(nn.Module)：被BertEncoder调用"></a>· class BertLayer(nn.Module)：被BertEncoder调用</h5><h6 id="1-init-3"><a href="#1-init-3" class="headerlink" title="1) init"></a>1) init</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>    <span class="hljs-built_in">super</span>().__init__()<br>    self.chunk_size_feed_forward = config.chunk_size_feed_forward<br>    self.seq_len_dim = <span class="hljs-number">1</span><br>    self.attention = BertAttention(config)<br>    self.is_decoder = config.is_decoder<br>    self.add_cross_attention = config.add_cross_attention<br>    <span class="hljs-keyword">if</span> self.add_cross_attention:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.is_decoder:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self&#125;</span> should be used as a decoder model if cross attention is added&quot;</span>)<br>        self.crossattention = BertAttention(config, position_embedding_type=<span class="hljs-string">&quot;absolute&quot;</span>)<br>    self.intermediate = BertIntermediate(config)<br>    self.output = BertOutput(config)<br></code></pre></td></tr></table></figure>

<p>可以简单理解为，依次调用了BertAttention、BertIntermediate、BertOutput完成了一个BLOCK的操作</p>
<h6 id="2-forward-3"><a href="#2-forward-3" class="headerlink" title="2) forward"></a>2) forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    hidden_states: torch.Tensor,</span><br><span class="hljs-params">    attention_mask: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    head_mask: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    encoder_hidden_states: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    encoder_attention_mask: <span class="hljs-type">Optional</span>[torch.FloatTensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    past_key_value: <span class="hljs-type">Optional</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[torch.FloatTensor]]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    output_attentions: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Tuple</span>[torch.Tensor]:<br>    <span class="hljs-comment"># decoder uni-directional self-attention cached key/values tuple is at positions 1,2</span><br>    self_attn_past_key_value = past_key_value[:<span class="hljs-number">2</span>] <span class="hljs-keyword">if</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>    self_attention_outputs = self.attention(<br>        hidden_states,<br>        attention_mask,<br>        head_mask,<br>        output_attentions=output_attentions,<br>        past_key_value=self_attn_past_key_value,<br>    )<br>    <span class="hljs-comment"># 忽略一些is_decoder的操作</span><br>    <br>    attention_output = self_attention_outputs[<span class="hljs-number">0</span>]<br><br>    outputs = self_attention_outputs[<span class="hljs-number">1</span>:]  <span class="hljs-comment"># add self attentions if we output attention weights</span><br><br>    layer_output = apply_chunking_to_forward(<br>        self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output<br>    )<br>    outputs = (layer_output,) + outputs<br><br>    <span class="hljs-comment"># if decoder, return the attn key/values as the last output</span><br>    <span class="hljs-keyword">if</span> self.is_decoder:<br>        outputs = outputs + (present_key_value,)<br><br>    <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>

<p>组装起来</p>
<h5 id="·-class-BertEncoder-nn-Module-层结构"><a href="#·-class-BertEncoder-nn-Module-层结构" class="headerlink" title="· class BertEncoder(nn.Module)层结构"></a>· class BertEncoder(nn.Module)层结构</h5><h6 id="1-init-4"><a href="#1-init-4" class="headerlink" title="1) init"></a>1) init</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>    <span class="hljs-built_in">super</span>().__init__()<br>    self.config = config<br>    self.layer = nn.ModuleList([BertLayer(config) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.num_hidden_layers)])<br>    self.gradient_checkpointing = <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure>

<p>在这里通过<code>config.num_hidden_layers</code>指定了这个<code>BertLayer</code>结构的层数，进一步详细查看<code>BertLayer</code>层的代码，应该对应的就是Transformer架构中如图所示的N×这个部分</p>
<img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712093703201.png" srcset="/img/loading.gif" lazyload alt="image-20220712093703201" style="zoom:50%;">



<h6 id="2-forward-4"><a href="#2-forward-4" class="headerlink" title="2) forward"></a>2) forward</h6><p>主要是把N个Layer串接起来forward，返回值封装了一个类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/modeling_outputs.py</span><br><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BaseModelOutputWithPastAndCrossAttentions</span>(<span class="hljs-title class_ inherited__">ModelOutput</span>):<br></code></pre></td></tr></table></figure>

<h5 id="·-class-BertPooler："><a href="#·-class-BertPooler：" class="headerlink" title="· class BertPooler："></a>· class BertPooler：</h5><p>这个主要是针对[CLS]token又过了一个pooler</p>
<p>禁用的话：bertmodel初始化有一个配置add_pooling_layer默认为True，改成false就行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertPooler</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dense = nn.Linear(config.hidden_size, config.hidden_size)<br>        self.activation = nn.Tanh()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states: torch.Tensor</span>) -&gt; torch.Tensor:<br>        <span class="hljs-comment"># We &quot;pool&quot; the model by simply taking the hidden state corresponding</span><br>        <span class="hljs-comment"># to the first token.</span><br>        first_token_tensor = hidden_states[:, <span class="hljs-number">0</span>]<br>        pooled_output = self.dense(first_token_tensor)<br>        pooled_output = self.activation(pooled_output)<br>        <span class="hljs-keyword">return</span> pooled_output<br></code></pre></td></tr></table></figure>

<p><font color="red">TODO：这里还有些内容不是很明白，待和zkh讨论，比如说为什么叫pool，然后[CLS]这个token为什么要做这些的操作</font> </p>
<h5 id="·-class-BertModel-各层组合在一起整体的说明"><a href="#·-class-BertModel-各层组合在一起整体的说明" class="headerlink" title="· class BertModel(): 各层组合在一起整体的说明"></a>· class BertModel(): 各层组合在一起整体的说明</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertModel</span>(<span class="hljs-title class_ inherited__">BertPreTrainedModel</span>):<br>embedding_output = self.embeddings(<br>    input_ids=input_ids,<br>    position_ids=position_ids,<br>    token_type_ids=token_type_ids,<br>    inputs_embeds=inputs_embeds,<br>    past_key_values_length=past_key_values_length,<br>)<br>encoder_outputs = self.encoder(<br>    embedding_output,<br>    attention_mask=extended_attention_mask,<br>    head_mask=head_mask,<br>    encoder_hidden_states=encoder_hidden_states,<br>    encoder_attention_mask=encoder_extended_attention_mask,<br>    past_key_values=past_key_values,<br>    use_cache=use_cache,<br>    output_attentions=output_attentions,<br>    output_hidden_states=output_hidden_states,<br>    return_dict=return_dict,<br>)<br>sequence_output = encoder_outputs[<span class="hljs-number">0</span>]<br>pooled_output = self.pooler(sequence_output) <span class="hljs-keyword">if</span> self.pooler <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>

<p>重点代码感觉在这个部分，其他部分在制作一些mask类的地方</p>
<h4 id="2-2-2-model-state-dict"><a href="#2-2-2-model-state-dict" class="headerlink" title="2.2.2 model.state_dict()"></a>2.2.2 model.state_dict()</h4><h5 id="·-加载预训练model"><a href="#·-加载预训练model" class="headerlink" title="· 加载预训练model"></a>· 加载预训练model</h5><p><font color="red">这里加载的时候应该是用到了config.json文件和pytorch_model.bin这两个文件，而vocab.txt应该是tokenizer.from_pretrained()时候用到的，这里详细看一下config.json文件和pytorch_model.bin这两个文件是怎么被用到的</font> </p>
<p>在加载模型后，可以通过打印<code>model.state_dict()</code>调试看到模型的各个参数，这里因为是from_pretrained的，所以已经加载了pytorch_model.bin文件中的内容，而且每次加载出来的结果也都是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 预训练版本</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertModel, BertConfig<br>config = BertConfig()<br>model = BertModel.from_pretrained(<span class="hljs-string">&quot;./bert_base_uncased&quot;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711141124709.png" srcset="/img/loading.gif" lazyload alt="image-20220711141124709"></p>
<p>加载model，也就是<code>BertModel.from_pretrained(pretrained_model_name_or_path)</code>对应的函数在如下路径，<strong>这个地方只要是bert的模型结构，不管是bert-base还是bert-large</strong>是都可以通过这里加载的，主要就是读取对应的<strong>config.json文件和pytorch_model.bin这两个文件</strong>：</p>
<blockquote>
<p>&#x2F;Users&#x2F;curious&#x2F;opt&#x2F;miniconda3&#x2F;envs&#x2F;venv2&#x2F;lib&#x2F;python3.9&#x2F;site-packages&#x2F;transformers&#x2F;modeling_utils.py</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@classmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">from_pretrained</span>(<span class="hljs-params">cls, pretrained_model_name_or_path: <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[<span class="hljs-built_in">str</span>, os.PathLike]], *model_args, **kwargs</span>):<br></code></pre></td></tr></table></figure>

<h6 id="1-注解说明"><a href="#1-注解说明" class="headerlink" title="1) 注解说明"></a>1) 注解说明</h6><ul>
<li><p>从预训练的模型配置中实例化预训练的pytorch模型，该模型默认使用 <code>model.eval()</code>设置为评估模式；</p>
<p><font color="red">和同学讨论后补充：model.eval()一般涉及到dropout层与normalization层；；；在BERT和这种NLP领域下，因为BN不怎么用，所以LN实际上只是单个样本内部在seq_len这个维度上做norm，就不涉及到eval这块了，也就是说在NLP任务的eval这里可能只影响到dropout层；</font> </p>
<p><font color="red">再补充一些BN上的细节，BN在做训练的时候，均值和方差来自于这一组batch的计算，在inference的时候，使用<strong>全局</strong>的均值和方差，这个全局的均值和方差由之前的每个mini-batch记录而来。</font> </p>
<p><font color="red">设是[batch, seq_len, hidden]，那么BN会计算出来一个[1, seq_len, hidden]的均值；；；LN就会计算出来一个[batch, 1, hidden]的均值，然后怎么怎么处理</font> </p>
</li>
<li><p>输出的警告<code>Weights from XXX not initialized from pretrained model</code>表示XXX部分的权重没有出现，将使用模型其余部分进行训练，可以通过下游任务来微调这些权重：</p>
<p>如果把config文件的层数增加，比如从12层增加到14层的hidden layer结构，可以触发这个Warning</p>
<blockquote>
<p>Some weights of BertModel were not initialized from the model checkpoint at .&#x2F;bert_base_uncased and are newly initialized: [‘bert.encoder.layer.13.attention.output.dense.weight’, ‘bert.encoder.layer.12.intermediate.dense.bias’, ‘bert.encoder.layer.13.attention.self.key.weight’, ‘bert.encoder.layer.13.attention.output.dense.bias’, ‘bert.encoder.layer.13.attention.self.value.weight’, ‘bert.encoder.layer.12.attention.self.query.weight’, ‘bert.encoder.layer.13.attention.self.value.bias’, ‘bert.encoder.layer.12.attention.self.value.bias’, ‘bert.encoder.layer.12.attention.output.LayerNorm.weight’, ‘bert.encoder.layer.13.output.dense.bias’, ‘bert.encoder.layer.13.intermediate.dense.bias’, ‘bert.encoder.layer.13.output.LayerNorm.bias’, ‘bert.encoder.layer.13.output.dense.weight’, ‘bert.encoder.layer.12.attention.self.value.weight’, ‘bert.encoder.layer.12.attention.self.query.bias’, ‘bert.encoder.layer.13.output.LayerNorm.weight’, ‘bert.encoder.layer.12.output.LayerNorm.weight’, ‘bert.encoder.layer.13.attention.self.query.bias’, ‘bert.encoder.layer.13.attention.self.query.weight’, ‘bert.encoder.layer.12.attention.self.key.weight’, ‘bert.encoder.layer.13.attention.output.LayerNorm.weight’, ‘bert.encoder.layer.12.attention.output.dense.bias’, ‘bert.encoder.layer.12.attention.self.key.bias’, ‘bert.encoder.layer.12.output.dense.weight’, ‘bert.encoder.layer.12.attention.output.LayerNorm.bias’, ‘bert.encoder.layer.13.intermediate.dense.weight’, ‘bert.encoder.layer.12.output.LayerNorm.bias’, ‘bert.encoder.layer.13.attention.self.key.bias’, ‘bert.encoder.layer.12.intermediate.dense.weight’, ‘bert.encoder.layer.13.attention.output.LayerNorm.bias’, ‘bert.encoder.layer.12.output.dense.bias’, ‘bert.encoder.layer.12.attention.output.dense.weight’]</p>
</blockquote>
</li>
<li><p>输出的警告<code>Weights from XXX not used in YYY</code>表示预训练文件中的层XXX不被YYY使用，因此那些权重将被丢弃；</p>
<p>如果把config文件的层数减少，比如从12层减小到10层的hidden layer结构，可以触发这个Warning</p>
<blockquote>
<p>Some weights of the model checkpoint at .&#x2F;bert_base_uncased were not used when initializing BertModel: [‘bert.encoder.layer.10.intermediate.dense.weight’, ‘cls.predictions.decoder.weight’, ‘cls.predictions.transform.dense.bias’, ‘bert.encoder.layer.11.attention.self.value.bias’, ‘bert.encoder.layer.11.attention.output.dense.bias’, ‘bert.encoder.layer.10.output.dense.bias’, ‘bert.encoder.layer.10.attention.self.key.bias’, ‘bert.encoder.layer.10.attention.output.LayerNorm.bias’, ‘bert.encoder.layer.10.attention.self.value.weight’, ‘bert.encoder.layer.11.attention.self.key.bias’, ‘bert.encoder.layer.11.output.LayerNorm.weight’, ‘bert.encoder.layer.10.output.LayerNorm.bias’, ‘bert.encoder.layer.11.output.dense.bias’, ‘cls.predictions.transform.LayerNorm.weight’, ‘bert.encoder.layer.10.attention.output.dense.bias’, ‘cls.seq_relationship.bias’, ‘bert.encoder.layer.10.attention.self.value.bias’, ‘bert.encoder.layer.10.attention.output.dense.weight’, ‘cls.predictions.bias’, ‘bert.encoder.layer.10.attention.self.query.weight’, ‘bert.encoder.layer.11.attention.self.query.bias’, ‘cls.predictions.transform.LayerNorm.bias’, ‘bert.encoder.layer.11.attention.output.LayerNorm.bias’, ‘bert.encoder.layer.10.attention.self.query.bias’, ‘cls.predictions.transform.dense.weight’, ‘bert.encoder.layer.10.attention.output.LayerNorm.weight’, ‘bert.encoder.layer.10.output.dense.weight’, ‘bert.encoder.layer.11.attention.self.key.weight’, ‘bert.encoder.layer.11.attention.self.query.weight’, ‘cls.seq_relationship.weight’, ‘bert.encoder.layer.11.attention.self.value.weight’, ‘bert.encoder.layer.11.intermediate.dense.weight’, ‘bert.encoder.layer.10.output.LayerNorm.weight’, ‘bert.encoder.layer.11.attention.output.dense.weight’, ‘bert.encoder.layer.10.intermediate.dense.bias’, ‘bert.encoder.layer.11.output.dense.weight’, ‘bert.encoder.layer.11.intermediate.dense.bias’, ‘bert.encoder.layer.11.output.LayerNorm.bias’, ‘bert.encoder.layer.10.attention.self.key.weight’, ‘bert.encoder.layer.11.attention.output.LayerNorm.weight’]</p>
</blockquote>
<p>这里额外输出了几个<code>cls.xxx</code>，就是说没有使用这些检查点的权重，从一些解释来看这些内容应该是要被下游分类器用到的，这些内容将被初始化重新训练。目前代码里只是直接简单应用了这个的输出，而没有针对下游任务fine-tune那些的过程；</p>
</li>
</ul>
<h6 id="2-参数说明"><a href="#2-参数说明" class="headerlink" title="2) 参数说明"></a>2) 参数说明</h6><ul>
<li><p><code> pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*)</code></p>
<p>1）可以是一个字符串代表<code>model id</code>，这个model id可以从huggingface.co上获取，比如直接使用<code>bert-base-uncased</code>，或者使用带有用户名称的这个model id例如<code>hfl/chinese-macbert-base</code>，这种使用方法下可能会从huggingface那边完成下载；</p>
<p>2）可以是一个包含有pytorch_model.bin和config.json文件的路径，例如<code>./bert_base_uncased/</code>，注意这个目录下的内容需要通过<code>PreTrainedModel.save_pretrained</code>方法来得到，否则保存出来的文件可能和transformer（huggingface这一套）不太配合；</p>
<p>3）其余用法不太常见或者一般不使用，好像可以从tensorflow和flax的checkpoint进行加载，如果设置为None的话就是通过其他办法已经把config和state_dict给加载进去了；</p>
</li>
<li><p><code>output_attentions</code></p>
<p>用法：<code>model = BertModel.from_pretrained(&quot;./bert_base_uncased&quot;)</code></p>
<p>这是一个可能相对再常用一点的参数，模型输出的output包含了一个<code>output[&#39;attentions&#39;]</code>的参数输出，在调试的时候发现他是一个长度为12的tuple（这里的长度12是bert的层数），tuple中每个位置上是 <code>shape[1,12, seq_len, seq_len]</code>（这里的长度12应该是multi-head的头数目），output_attentions应该是 <code>softmax((query · key)/sqrt(d_k))</code>的结果；注意<code>shape[1,12, seq_len, seq_len]</code>这个地方，softmax应该是在-1dim上做的，代表<strong>query中的每个位置处，对于每一个key的attention score</strong>，所以来做求和的话，应该能得到一个1的结果；</p>
<p><font color="red">在后面看forward代码的时候，还要回来看一下这个地方</font> </p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711172228802.png" srcset="/img/loading.gif" lazyload alt="image-20220711172228802"></p>
</li>
<li><p><code>hidden_states</code></p>
<p>用法：<code>model = BertModel.from_pretrained(&quot;./bert_base_uncased&quot;, output_hidden_states=True)</code></p>
<p>这是中间层（隐层）tensor的output输出，<font color="red">和output_attentions一样，这些内容既可以在from_pretrained中给带过去，<strong>也可以直接写在config.json</strong>文件里</font></p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711173534108.png" srcset="/img/loading.gif" lazyload alt="image-20220711173534108"></p>
</li>
<li><p>上面可能是一些相对常用的参数，暂时理解来说在<code>.from_pretrained(&quot;./bert_base_uncased&quot;)</code>这个方法中带的其他一些参数可以和config加参数起到同样的效果，也就证明这个方法用到了config.json这个文件</p>
</li>
</ul>
<h6 id="3-内部流程说明（※重点）"><a href="#3-内部流程说明（※重点）" class="headerlink" title="3) 内部流程说明（※重点）"></a>3) 内部流程说明（※重点）</h6><p>内部这个地方还是写的比较详细的，像各种Exception也都实现的非常完整，大概理解一下其中的重点部分，主要目标就是加载config.json和pytorch_model.bin两个文件。</p>
<p><font color="red"><strong>config.json和pytorch_model.bin应该只有model这边用到，tokenizer那边只用到vocab.txt；；从model.from_pretrained接收参数是一个路径，而tokenizer.from_pretrained接收参数是一个vocab.txt文件的路径或者上级路径感觉也能证明这一点</strong></font> </p>
<ul>
<li><p>首先加载config.json</p>
<p>在下面这段代码中，config_path加载到了pretrained_model_name_or_path中的内容，也就是<code>&quot;./bert_base_uncased&quot;</code>，向下层<code>cls.config_class.from_pretrained</code>传递</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/modeling_utils.py</span><br><span class="hljs-comment"># Load config if we don&#x27;t provide a configuration</span><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(config, PretrainedConfig):<br>    config_path = config <span class="hljs-keyword">if</span> config <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> pretrained_model_name_or_path<br>    config, model_kwargs = cls.config_class.from_pretrained(<br>        config_path,<br>        cache_dir=cache_dir,<br>        return_unused_kwargs=<span class="hljs-literal">True</span>,<br>        force_download=force_download,<br>        resume_download=resume_download,<br>        proxies=proxies,<br>        local_files_only=local_files_only,<br>        use_auth_token=use_auth_token,<br>        revision=revision,<br>        _from_auto=from_auto_class,<br>        _from_pipeline=from_pipeline,<br>        **kwargs,<br>    )<br><span class="hljs-keyword">else</span>:<br>    model_kwargs = kwargs<br></code></pre></td></tr></table></figure>

<p>这里debug调试了一下<code>cls.config_class</code>：<code>&lt;class &#39;transformers.models.bert.configuration_bert.BertConfig&#39;&gt;</code>，于是在去看<code>BertConfig</code>这块的<code>.from_pretrained</code>，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/configuration_utils.py</span><br><span class="hljs-meta">@classmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">from_pretrained</span>(<span class="hljs-params">cls, pretrained_model_name_or_path: <span class="hljs-type">Union</span>[<span class="hljs-built_in">str</span>, os.PathLike], **kwargs</span>) -&gt; <span class="hljs-string">&quot;PretrainedConfig&quot;</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    一些注释，为了放在md里暂时删除了</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;model_type&quot;</span> <span class="hljs-keyword">in</span> config_dict <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(cls, <span class="hljs-string">&quot;model_type&quot;</span>) <span class="hljs-keyword">and</span> config_dict[<span class="hljs-string">&quot;model_type&quot;</span>] != cls.model_type:<br>        logger.warning(<br>            <span class="hljs-string">f&quot;You are using a model of type <span class="hljs-subst">&#123;config_dict[<span class="hljs-string">&#x27;model_type&#x27;</span>]&#125;</span> to instantiate a model of type &quot;</span><br>            <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;cls.model_type&#125;</span>. This is not supported for all configurations of models and can yield errors.&quot;</span><br>        )<br><br>    <span class="hljs-keyword">return</span> cls.from_dict(config_dict, **kwargs)<br></code></pre></td></tr></table></figure>

<p>在往下看，调用了<code>cls.get_config_dict</code>这个函数，最后一路往下找，直到找到这里，加载json文件，返回一个dict对象，在上面那段代码里最后return了一个<code>cls.from_dict</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/configuration_utils.py</span><br><span class="hljs-meta">@classmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_dict_from_json_file</span>(<span class="hljs-params">cls, json_file: <span class="hljs-type">Union</span>[<span class="hljs-built_in">str</span>, os.PathLike]</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(json_file, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> reader:<br>        text = reader.read()<br>    <span class="hljs-keyword">return</span> json.loads(text)<br></code></pre></td></tr></table></figure>

<p>看了一下<code>cls.from_dict</code>，应该是这里最终返回了一个BertConfig类的对象，<font color="red">这里字典前面加两个*号是将字典解开成为独立的元素作为形参</font> </p>
<img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711232228890.png" srcset="/img/loading.gif" lazyload alt="image-20220711232228890" style="zoom:50%;">
</li>
<li><p>其次加载pytorch_model.bin文件</p>
<p>通过在<code>config_path</code>目录下寻找文件，命中了<code>pytorch_model.bin</code>这个pytorch的checkpoint文件</p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711224750910.png" srcset="/img/loading.gif" lazyload alt="image-20220711224750910"></p>
<p>找到这个文件后，这里做了一个和cache判断的操作，这个和huggingface这里实现可以到远程下载有关，如果过了这个函数后还是本地的路径，那就说明是用的本地的文件实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/modeling_utils.py</span><br><span class="hljs-comment"># Load from URL or cache if already cached</span><br>resolved_archive_file = cached_path(<br>    archive_file,<br>    cache_dir=cache_dir,<br>    force_download=force_download,<br>    proxies=proxies,<br>    resume_download=resume_download,<br>    local_files_only=local_files_only,<br>    use_auth_token=use_auth_token,<br>    user_agent=user_agent,<br>)<br></code></pre></td></tr></table></figure>

<p>因为是pytorch形式的checkpoint，在这里<code>load_state_dict()</code></p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711233904281.png" srcset="/img/loading.gif" lazyload alt="image-20220711233904281"></p>
<p>把<code>state_dict</code>传入这里，进一步进行处理，这里返回就会有<code>missing unexpect</code>这些</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/modeling_utils.py</span><br><span class="hljs-keyword">elif</span> from_pt:<br>    <span class="hljs-keyword">if</span> low_cpu_mem_usage:<br>        cls._load_pretrained_model_low_mem(model, loaded_state_dict_keys, resolved_archive_file)<br>    <span class="hljs-keyword">else</span>:<br>        model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(<br>            model,<br>            state_dict,<br>            resolved_archive_file,<br>            pretrained_model_name_or_path,<br>            ignore_mismatched_sizes=ignore_mismatched_sizes,<br>            sharded_metadata=sharded_metadata,<br>            _fast_init=_fast_init,<br>        )<br></code></pre></td></tr></table></figure>

<p>在如下函数中完成比对操作，<strong>这里一些的输出错误经过<code>state_dict</code>的比对而发现，也就对应了“2.2.2节中，加载预训练model中第一部分，作者在开头给出的注解说明”</strong>，至此这两个文件</p>
<p><font color="blue"></font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/modeling_utils.py</span><br><span class="hljs-meta">@classmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_load_pretrained_model</span>(<span class="hljs-params"></span><br><span class="hljs-params">    cls,</span><br><span class="hljs-params">    model,</span><br><span class="hljs-params">    state_dict,</span><br><span class="hljs-params">    resolved_archive_file,</span><br><span class="hljs-params">    pretrained_model_name_or_path,</span><br><span class="hljs-params">    ignore_mismatched_sizes=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    sharded_metadata=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    _fast_init=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-comment"># Retrieve missing &amp; unexpected_keys</span><br>    model_state_dict = model.state_dict()<br>    expected_keys = <span class="hljs-built_in">list</span>(model_state_dict.keys())<br>    loaded_keys = <span class="hljs-built_in">list</span>(state_dict.keys()) <span class="hljs-keyword">if</span> state_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> sharded_metadata[<span class="hljs-string">&quot;all_checkpoint_keys&quot;</span>]<br>    prefix = model.base_model_prefix<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_fix_key</span>(<span class="hljs-params">key</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;beta&quot;</span> <span class="hljs-keyword">in</span> key:<br>            <span class="hljs-keyword">return</span> key.replace(<span class="hljs-string">&quot;beta&quot;</span>, <span class="hljs-string">&quot;bias&quot;</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;gamma&quot;</span> <span class="hljs-keyword">in</span> key:<br>            <span class="hljs-keyword">return</span> key.replace(<span class="hljs-string">&quot;gamma&quot;</span>, <span class="hljs-string">&quot;weight&quot;</span>)<br>        <span class="hljs-keyword">return</span> key<br><br>    loaded_keys = [_fix_key(key) <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> loaded_keys]<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(prefix) &gt; <span class="hljs-number">0</span>:<br>        has_prefix_module = <span class="hljs-built_in">any</span>(s.startswith(prefix) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> loaded_keys)<br>        expects_prefix_module = <span class="hljs-built_in">any</span>(s.startswith(prefix) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> expected_keys)<br>    <span class="hljs-keyword">else</span>:<br>        has_prefix_module = <span class="hljs-literal">False</span><br>        expects_prefix_module = <span class="hljs-literal">False</span><br><br>    <span class="hljs-comment"># key re-naming operations are never done on the keys</span><br>    <span class="hljs-comment"># that are loaded, but always on the keys of the newly initialized model</span><br>    remove_prefix_from_model = <span class="hljs-keyword">not</span> has_prefix_module <span class="hljs-keyword">and</span> expects_prefix_module<br>    add_prefix_to_model = has_prefix_module <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> expects_prefix_module<br><br>    <span class="hljs-keyword">if</span> remove_prefix_from_model:<br>        expected_keys_not_prefixed = [s <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> expected_keys <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> s.startswith(prefix)]<br>        expected_keys = [<span class="hljs-string">&quot;.&quot;</span>.join(s.split(<span class="hljs-string">&quot;.&quot;</span>)[<span class="hljs-number">1</span>:]) <span class="hljs-keyword">if</span> s.startswith(prefix) <span class="hljs-keyword">else</span> s <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> expected_keys]<br>    <span class="hljs-keyword">elif</span> add_prefix_to_model:<br>        expected_keys = [<span class="hljs-string">&quot;.&quot;</span>.join([prefix, s]) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> expected_keys]<br><br>    missing_keys = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(expected_keys) - <span class="hljs-built_in">set</span>(loaded_keys))<br>    unexpected_keys = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(loaded_keys) - <span class="hljs-built_in">set</span>(expected_keys))<br><br>    <span class="hljs-comment"># Some models may have keys that are not in the state by design, removing them before needlessly warning</span><br>    <span class="hljs-comment"># the user.</span><br>    <span class="hljs-keyword">if</span> cls._keys_to_ignore_on_load_missing <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">for</span> pat <span class="hljs-keyword">in</span> cls._keys_to_ignore_on_load_missing:<br>            missing_keys = [k <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> missing_keys <span class="hljs-keyword">if</span> re.search(pat, k) <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>]<br><br>    <span class="hljs-keyword">if</span> cls._keys_to_ignore_on_load_unexpected <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">for</span> pat <span class="hljs-keyword">in</span> cls._keys_to_ignore_on_load_unexpected:<br>            unexpected_keys = [k <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> unexpected_keys <span class="hljs-keyword">if</span> re.search(pat, k) <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>]<br><br>    <span class="hljs-keyword">if</span> _fast_init:<br>        <span class="hljs-comment"># retrieve unintialized modules and initialize</span><br>        uninitialized_modules = model.retrieve_modules_from_names(<br>            missing_keys, add_prefix=add_prefix_to_model, remove_prefix=remove_prefix_from_model<br>        )<br>        <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> uninitialized_modules:<br>            model._init_weights(module)<br><br>    <span class="hljs-comment"># Make sure we are able to load base models as well as derived models (with heads)</span><br>    start_prefix = <span class="hljs-string">&quot;&quot;</span><br>    model_to_load = model<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(cls.base_model_prefix) &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(model, cls.base_model_prefix) <span class="hljs-keyword">and</span> has_prefix_module:<br>        start_prefix = cls.base_model_prefix + <span class="hljs-string">&quot;.&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(cls.base_model_prefix) &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(model, cls.base_model_prefix) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> has_prefix_module:<br>        model_to_load = <span class="hljs-built_in">getattr</span>(model, cls.base_model_prefix)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">any</span>(key <span class="hljs-keyword">in</span> expected_keys_not_prefixed <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> loaded_keys):<br>            <span class="hljs-keyword">raise</span> ValueError(<br>                <span class="hljs-string">&quot;The state dictionary of the model you are training to load is corrupted. Are you sure it was &quot;</span><br>                <span class="hljs-string">&quot;properly saved?&quot;</span><br>            )<br><br>    <span class="hljs-keyword">if</span> state_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># Whole checkpoint</span><br>        mismatched_keys = []<br>        <span class="hljs-keyword">if</span> ignore_mismatched_sizes:<br>            <span class="hljs-keyword">for</span> checkpoint_key <span class="hljs-keyword">in</span> loaded_keys:<br>                model_key = checkpoint_key<br>                <span class="hljs-keyword">if</span> remove_prefix_from_model:<br>                    <span class="hljs-comment"># The model key starts with `prefix` but `checkpoint_key` doesn&#x27;t so we add it.</span><br>                    model_key = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span>.<span class="hljs-subst">&#123;checkpoint_key&#125;</span>&quot;</span><br>                <span class="hljs-keyword">elif</span> add_prefix_to_model:<br>                    <span class="hljs-comment"># The model key doesn&#x27;t start with `prefix` but `checkpoint_key` does so we remove it.</span><br>                    model_key = <span class="hljs-string">&quot;.&quot;</span>.join(checkpoint_key.split(<span class="hljs-string">&quot;.&quot;</span>)[<span class="hljs-number">1</span>:])<br><br>                <span class="hljs-keyword">if</span> (<br>                    model_key <span class="hljs-keyword">in</span> model_state_dict<br>                    <span class="hljs-keyword">and</span> state_dict[checkpoint_key].shape != model_state_dict[model_key].shape<br>                ):<br>                    mismatched_keys.append(<br>                        (checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape)<br>                    )<br>                    <span class="hljs-keyword">del</span> state_dict[checkpoint_key]<br><br>        error_msgs = _load_state_dict_into_model(model_to_load, state_dict, start_prefix)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Sharded checkpoint</span><br>        <span class="hljs-comment"># This should always be a list but, just to be sure.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(resolved_archive_file, <span class="hljs-built_in">list</span>):<br>            resolved_archive_file = [resolved_archive_file]<br><br>        error_msgs = []<br>        <span class="hljs-keyword">for</span> shard_file <span class="hljs-keyword">in</span> resolved_archive_file:<br>            state_dict = load_state_dict(shard_file)<br>            <span class="hljs-comment"># Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not</span><br>            <span class="hljs-comment"># matching the weights in the model.</span><br>            mismatched_keys = []<br>            <span class="hljs-keyword">if</span> ignore_mismatched_sizes:<br>                <span class="hljs-keyword">for</span> checkpoint_key <span class="hljs-keyword">in</span> loaded_keys:<br>                    model_key = checkpoint_key<br>                    <span class="hljs-keyword">if</span> remove_prefix_from_model:<br>                        <span class="hljs-comment"># The model key starts with `prefix` but `checkpoint_key` doesn&#x27;t so we add it.</span><br>                        model_key = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span>.<span class="hljs-subst">&#123;checkpoint_key&#125;</span>&quot;</span><br>                    <span class="hljs-keyword">elif</span> add_prefix_to_model:<br>                        <span class="hljs-comment"># The model key doesn&#x27;t start with `prefix` but `checkpoint_key` does so we remove it.</span><br>                        model_key = <span class="hljs-string">&quot;.&quot;</span>.join(checkpoint_key.split(<span class="hljs-string">&quot;.&quot;</span>)[<span class="hljs-number">1</span>:])<br><br>                    <span class="hljs-keyword">if</span> (<br>                        model_key <span class="hljs-keyword">in</span> model_state_dict<br>                        <span class="hljs-keyword">and</span> state_dict[checkpoint_key].shape != model_state_dict[model_key].shape<br>                    ):<br>                        mismatched_keys.append(<br>                            (checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape)<br>                        )<br>                        <span class="hljs-keyword">del</span> state_dict[checkpoint_key]<br><br>            error_msgs += _load_state_dict_into_model(model_to_load, state_dict, start_prefix)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(error_msgs) &gt; <span class="hljs-number">0</span>:<br>        error_msg = <span class="hljs-string">&quot;\n\t&quot;</span>.join(error_msgs)<br>        <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">f&quot;Error(s) in loading state_dict for <span class="hljs-subst">&#123;model.__class__.__name__&#125;</span>:\n\t<span class="hljs-subst">&#123;error_msg&#125;</span>&quot;</span>)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(unexpected_keys) &gt; <span class="hljs-number">0</span>:<br>        logger.warning(<br>            <span class="hljs-string">f&quot;Some weights of the model checkpoint at <span class="hljs-subst">&#123;pretrained_model_name_or_path&#125;</span> were not used when &quot;</span><br>            <span class="hljs-string">f&quot;initializing <span class="hljs-subst">&#123;model.__class__.__name__&#125;</span>: <span class="hljs-subst">&#123;unexpected_keys&#125;</span>\n&quot;</span><br>            <span class="hljs-string">f&quot;- This IS expected if you are initializing <span class="hljs-subst">&#123;model.__class__.__name__&#125;</span> from the checkpoint of a model trained on another task &quot;</span><br>            <span class="hljs-string">f&quot;or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n&quot;</span><br>            <span class="hljs-string">f&quot;- This IS NOT expected if you are initializing <span class="hljs-subst">&#123;model.__class__.__name__&#125;</span> from the checkpoint of a model that you expect &quot;</span><br>            <span class="hljs-string">f&quot;to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).&quot;</span><br>        )<br>    <span class="hljs-keyword">else</span>:<br>        logger.info(<span class="hljs-string">f&quot;All model checkpoint weights were used when initializing <span class="hljs-subst">&#123;model.__class__.__name__&#125;</span>.\n&quot;</span>)<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(missing_keys) &gt; <span class="hljs-number">0</span>:<br>        logger.warning(<br>            <span class="hljs-string">f&quot;Some weights of <span class="hljs-subst">&#123;model.__class__.__name__&#125;</span> were not initialized from the model checkpoint at <span class="hljs-subst">&#123;pretrained_model_name_or_path&#125;</span> &quot;</span><br>            <span class="hljs-string">f&quot;and are newly initialized: <span class="hljs-subst">&#123;missing_keys&#125;</span>\n&quot;</span><br>            <span class="hljs-string">f&quot;You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.&quot;</span><br>        )<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">len</span>(mismatched_keys) == <span class="hljs-number">0</span>:<br>        logger.info(<br>            <span class="hljs-string">f&quot;All the weights of <span class="hljs-subst">&#123;model.__class__.__name__&#125;</span> were initialized from the model checkpoint at <span class="hljs-subst">&#123;pretrained_model_name_or_path&#125;</span>.\n&quot;</span><br>            <span class="hljs-string">f&quot;If your task is similar to the task the model of the checkpoint was trained on, &quot;</span><br>            <span class="hljs-string">f&quot;you can already use <span class="hljs-subst">&#123;model.__class__.__name__&#125;</span> for predictions without further training.&quot;</span><br>        )<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(mismatched_keys) &gt; <span class="hljs-number">0</span>:<br>        mismatched_warning = <span class="hljs-string">&quot;\n&quot;</span>.join(<br>            [<br>                <span class="hljs-string">f&quot;- <span class="hljs-subst">&#123;key&#125;</span>: found shape <span class="hljs-subst">&#123;shape1&#125;</span> in the checkpoint and <span class="hljs-subst">&#123;shape2&#125;</span> in the model instantiated&quot;</span><br>                <span class="hljs-keyword">for</span> key, shape1, shape2 <span class="hljs-keyword">in</span> mismatched_keys<br>            ]<br>        )<br>        logger.warning(<br>            <span class="hljs-string">f&quot;Some weights of <span class="hljs-subst">&#123;model.__class__.__name__&#125;</span> were not initialized from the model checkpoint at <span class="hljs-subst">&#123;pretrained_model_name_or_path&#125;</span> &quot;</span><br>            <span class="hljs-string">f&quot;and are newly initialized because the shapes did not match:\n<span class="hljs-subst">&#123;mismatched_warning&#125;</span>\n&quot;</span><br>            <span class="hljs-string">f&quot;You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.&quot;</span><br>        )<br><br>    <span class="hljs-keyword">return</span> model, missing_keys, unexpected_keys, mismatched_keys, error_msgs<br></code></pre></td></tr></table></figure></li>
</ul>
<h6 id="4-额外补充"><a href="#4-额外补充" class="headerlink" title="4) 额外补充"></a>4) 额外补充</h6><p>应该是这里的实例化把config给model传进去了，于是model需要的key可能少于、或者多于提供给他的key（pytorch_model.bin）,这里是一个<code>super().__init__()</code>，可能是调用到<code>nn.Module</code>这个上层了，然后依据传入的config不知道怎么操作，把层数什么的网络结构给拼上了；<font color="red">另：也有可能是要加载到这个&#x2F;Users&#x2F;curious&#x2F;opt&#x2F;miniconda3&#x2F;envs&#x2F;venv2&#x2F;lib&#x2F;python3.9&#x2F;site-packages&#x2F;transformers&#x2F;models&#x2F;bert&#x2F;modeling_bert.py里面的BertModel类，这样BertModel类是super代表的上层？</font> </p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712001457966.png" srcset="/img/loading.gif" lazyload alt="image-20220712001457966"></p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712001307673.png" srcset="/img/loading.gif" lazyload alt="image-20220712001307673"></p>
<h5 id="·-通过config加载空模型并设置seed"><a href="#·-通过config加载空模型并设置seed" class="headerlink" title="· 通过config加载空模型并设置seed"></a>· 通过config加载空模型并设置seed</h5><p>如果是没有from_pretrained，而是通过<code>model = BertModel()</code>加载空模型的话，打印会看到初始化的参数，如下所示（另外注意，这种不从预训练文件中读取的话，需要加载config参数）：<br><font color="blue">BertConfig、BertModel、BertTokenizer</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 空模型加载版本</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertModel, BertConfig<br>config = BertConfig()<br>model = BertModel(config)<br></code></pre></td></tr></table></figure>

<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711141742912.png" srcset="/img/loading.gif" lazyload alt="image-20220711141742912"></p>
<p>这里如果再次尝试加载空模型的时候，因为参数是随机初始化的，所以参数初始化结果可能有所不同，如下图所示</p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220711142519040.png" srcset="/img/loading.gif" lazyload alt="image-20220711142519040"></p>
<p>通过set_seed进行指定，可以保证每次加载空模型时初始化的参数是一样的，set_seed的代码段如下，（实际使用上来说其实不一定需要写成这种函数的方式，直接写个几行就可以）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_seed</span>(<span class="hljs-params">seed</span>):<br>    torch.manual_seed(seed)<br>    torch.cuda.manual_seed_all(seed)<br>    np.random.seed(seed)<br>    random.seed(seed)<br>    torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br>setup_seed(<span class="hljs-number">42</span>)  <span class="hljs-comment"># 设置随机数种子</span><br></code></pre></td></tr></table></figure>

<p><font color="red">这个地方加载config的时候直接用到的是config &#x3D; BertConfig()，也就是BertConfig类中的内容，对这里进行了一下详细的调试，目前的理解是他实现了一个BertConfig类，继承自PretrainedConfig这个大类。平常在fine-tune阶段直接用到的config.json文件应该是从这个BertConfig保存而来的；；；如果要是自己训练的话，可能可以实现一个新的<strong>XxxConfig</strong>类，然后内部把参数什么的都设置为自己想要的，比如层数减少一些什么的</font> </p>
<blockquote>
<p>&#x2F;Users&#x2F;curious&#x2F;opt&#x2F;miniconda3&#x2F;envs&#x2F;venv2&#x2F;lib&#x2F;python3.9&#x2F;site-packages&#x2F;transformers&#x2F;models&#x2F;bert&#x2F;configuration_bert.py</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertConfig</span>(<span class="hljs-title class_ inherited__">PretrainedConfig</span>):<br>  	...<br>    model_type = <span class="hljs-string">&quot;bert&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        vocab_size=<span class="hljs-number">30522</span>,</span><br><span class="hljs-params">        hidden_size=<span class="hljs-number">768</span>,</span><br><span class="hljs-params">        num_hidden_layers=<span class="hljs-number">12</span>,</span><br><span class="hljs-params">        num_attention_heads=<span class="hljs-number">12</span>,</span><br><span class="hljs-params">        intermediate_size=<span class="hljs-number">3072</span>,</span><br><span class="hljs-params">        hidden_act=<span class="hljs-string">&quot;gelu&quot;</span>,</span><br><span class="hljs-params">        hidden_dropout_prob=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">        attention_probs_dropout_prob=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">        max_position_embeddings=<span class="hljs-number">512</span>,</span><br><span class="hljs-params">        type_vocab_size=<span class="hljs-number">2</span>,</span><br><span class="hljs-params">        initializer_range=<span class="hljs-number">0.02</span>,</span><br><span class="hljs-params">        layer_norm_eps=<span class="hljs-number">1e-12</span>,</span><br><span class="hljs-params">        pad_token_id=<span class="hljs-number">0</span>,</span><br><span class="hljs-params">        position_embedding_type=<span class="hljs-string">&quot;absolute&quot;</span>,</span><br><span class="hljs-params">        use_cache=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        classifier_dropout=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        **kwargs</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(pad_token_id=pad_token_id, **kwargs)<br><br>        self.vocab_size = vocab_size<br>        self.hidden_size = hidden_size<br>        self.num_hidden_layers = num_hidden_layers<br>        self.num_attention_heads = num_attention_heads<br>        self.hidden_act = hidden_act<br>        self.intermediate_size = intermediate_size<br>        self.hidden_dropout_prob = hidden_dropout_prob<br>        self.attention_probs_dropout_prob = attention_probs_dropout_prob<br>        self.max_position_embeddings = max_position_embeddings<br>        self.type_vocab_size = type_vocab_size<br>        self.initializer_range = initializer_range<br>        self.layer_norm_eps = layer_norm_eps<br>        self.position_embedding_type = position_embedding_type<br>        self.use_cache = use_cache<br>        self.classifier_dropout = classifier_dropout<br></code></pre></td></tr></table></figure>

<h5 id="·-保存模型"><a href="#·-保存模型" class="headerlink" title="· 保存模型"></a>· 保存模型</h5><p>通过如下命令可以完成一个模型的保存，这样会在目录下生成<strong>config.json</strong>、<strong>pytorch_model.bin</strong>这两个文件，结合上面的BertConfig，如果有一个自己的模型的话，就可以魔改一下那边的XxxConfig，比如减小一些层数训练什么的，下次通过from_pretrained应该就可以加载回来了；；；这里也要集合下上面那个<strong>加载预训练model</strong>来一起看；；；</p>
<p><font color="red">或者说，这套BertConfig和<code>from transformers.models.bert import modeling_bert </code>那边的bert模型是对应的，只要修改BertConfig这些参数就可以制作自己的bert了，比如可能有BertTiny，或者其他版本的，都可以通过Bert这边的pipeline来走这样一个流程</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.save_pretrained(<span class="hljs-string">&quot;directory_on_my_computer&quot;</span>)<br></code></pre></td></tr></table></figure>

<h3 id="2-3-tokenizer"><a href="#2-3-tokenizer" class="headerlink" title="2.3 tokenizer"></a>2.3 tokenizer</h3><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">tokenizer</span> <span class="hljs-operator">=</span> BertTokenizer(<span class="hljs-string">&quot;./bert_base_uncased/vocab.txt&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>tokenizer的from_pretrain在这里</p>
<blockquote>
<p>&#x2F;Users&#x2F;curious&#x2F;opt&#x2F;miniconda3&#x2F;envs&#x2F;venv2&#x2F;lib&#x2F;python3.9&#x2F;site-packages&#x2F;transformers&#x2F;tokenization_utils_base.py</p>
</blockquote>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712122136461.png" srcset="/img/loading.gif" lazyload alt="image-20220712122136461"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@classmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">from_pretrained</span>(<span class="hljs-params">cls, pretrained_model_name_or_path: <span class="hljs-type">Union</span>[<span class="hljs-built_in">str</span>, os.PathLike], *init_inputs, **kwargs</span>):<br><br>    <span class="hljs-comment"># 这里省略了一些类似于本地没读到就远程下载的操作，绥中得到一个resolved_vocab_files文件向_from_pretrained传递</span><br><br>    <span class="hljs-keyword">return</span> cls._from_pretrained(<br>      resolved_vocab_files,<br>      pretrained_model_name_or_path,<br>      init_configuration,<br>      *init_inputs,<br>      use_auth_token=use_auth_token,<br>      cache_dir=cache_dir,<br>      **kwargs,<br>    )<br></code></pre></td></tr></table></figure>

<p><font color="red">TODO：这里是不是还有些加入特殊token的操作，曾经在一些论文代码里见过</font> </p>
<p>tokenizer的作用就是把一句话按照vocab中转成一个id那个感觉，tokenizer.tokenize、tokenizer.convert_tokens_to_ids()和其反向的tokenizer_convert_ids_to_tokens比较常用；</p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712123154538.png" srcset="/img/loading.gif" lazyload alt="image-20220712123154538"></p>
<h2 id="3-Transformer-amp-BERT论文阅读中的重点记录"><a href="#3-Transformer-amp-BERT论文阅读中的重点记录" class="headerlink" title="3. Transformer&amp;BERT论文阅读中的重点记录"></a>3. Transformer&amp;BERT论文阅读中的重点记录</h2><h3 id="3-1-Attention-is-all-you-need"><a href="#3-1-Attention-is-all-you-need" class="headerlink" title="3.1 Attention is all you need"></a>3.1 Attention is all you need</h3><p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712214108979.png" srcset="/img/loading.gif" lazyload alt="image-20220712214108979"></p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712214133925.png" srcset="/img/loading.gif" lazyload alt="image-20220712214133925"></p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712214114211.png" srcset="/img/loading.gif" lazyload alt="image-20220712214114211"></p>
<img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20220712214155672.png" srcset="/img/loading.gif" lazyload alt="image-20220712214155672" style="zoom:50%;">

<p>就是那个指数，如果有一个特别大的，他softmax算出来就很趋向于1了</p>
<p>这个现象会随着指数的增大而明显，比如指数是3的时候，就不明显，指数是20的时候就很明显</p>
<h3 id="3-2-BERT"><a href="#3-2-BERT" class="headerlink" title="3.2 BERT"></a>3.2 BERT</h3><p>TODO</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>BERT源代码阅读学习</div>
      <div>http://example.com/2022/11/19/research/others/BERT源代码阅读学习/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Curious;</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年11月19日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/11/19/mixed/hexo/hexo%E5%9B%BE%E7%89%87%E8%B7%AF%E5%BE%84%E7%9A%84%E9%85%8D%E7%BD%AE/" title="hexo图片路径的配置">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">hexo图片路径的配置</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/11/18/mixed/git/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9E%E6%8E%A5github%E4%B8%8Etoken%E7%9A%84%E9%85%8D%E7%BD%AE/" title="服务器连接github与token的配置">
                        <span class="hidden-mobile">服务器连接github与token的配置</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
