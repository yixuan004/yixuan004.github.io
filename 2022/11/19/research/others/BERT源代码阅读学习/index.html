<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>BERT源代码阅读学习 | Curious;的个人划水博客</title>
  <meta name="description" content="" />
  <meta name="keywords" content="" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="shortcut icon" href="/">
  <link rel="alternate" href="/atom.xml" title="Curious;的个人划水博客" type="application/atom+xml">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="BERT源代码阅读学习，主要是Transformer架构中的Encoder部分，各层的源代码理解与阅读学习">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT源代码阅读学习">
<meta property="og:url" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Curious;的个人划水博客">
<meta property="og:description" content="BERT源代码阅读学习，主要是Transformer架构中的Encoder部分，各层的源代码理解与阅读学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711105616573.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711105642016.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711110109758.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712195033334.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711135726421.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220713092751251.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711110109758.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220713165208863.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220713170145685.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712110214629.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712093703201.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711141124709.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711172228802.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711173534108.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711232228890.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711224750910.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711233904281.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712001457966.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712001307673.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711141742912.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711142519040.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712122136461.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712123154538.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712214108979.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712214133925.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712214114211.png">
<meta property="og:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712214155672.png">
<meta property="article:published_time" content="2022-11-19T03:30:41.000Z">
<meta property="article:modified_time" content="2022-11-20T01:50:08.929Z">
<meta property="article:author" content="Curious;">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711105616573.png">
    
  <link href="https://fonts.googleapis.com/css?family=Inconsolata|Titillium+Web" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono" rel="stylesheet">
  <link href='//cdn.bootcss.com/node-waves/0.7.5/waves.min.css' rel='stylesheet'>
  
<link rel="stylesheet" href="/style.css">

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>


  <script>setLoadingBarProgress(20)</script> 
  <header class="l_header">
	<div class='wrapper'>
		<div class="nav-main container container--flex">
			<a class="logo flat-box" href='/' >
				Curious;的个人划水博客
			</a>
			<div class='menu'>
				<ul class='h-list'>
					
						<li>
							<a class='flat-box nav-home' href='/'>
								Home
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-archives' href='/archives'>
								Archives
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-gallery' target="_blank" rel="noopener" href='https://photos.google.com/album/AF1QipNoqKYgspQo5O1YhlFXGCQ7p575KBH3Yxf8WHL4?hl=zh-CN'>
								Gallery
							</a>
						</li>
					
						<li>
							<a class='flat-box nav-about' href='/about'>
								About
							</a>
						</li>
					
				</ul>
				<div class='underline'></div>
			</div>
			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="Search" />
						<span class="icon icon-search"></span>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a href='javascript:void(0)'><span class="icon icon-search flat-box"></span></a></li>
				
				<li class='s-menu'><a href='javascript:void(0)'><span class="icon icon-menu flat-box"></span></a></li>
			</ul>
		</div>
		
		<div class='nav-sub container container--flex'>
			<a class="logo" class="flat-box" href='javascript:void(0)'>
				Word of Forks
			</a>

			<ul class='switcher h-list'>
				<li class='s-comment'><a href='javascript:void(0)'><span class="icon icon-chat_bubble_outline flat-box"></span></a></li>
				<li class='s-top'><a href='javascript:void(0)'><span class="icon icon-arrow_upward flat-box"></span></a></li>
				<li class='s-toc'><a href='javascript:void(0)'><span class="icon icon-format_list_numbered flat-box"></span></a></li>
			</ul>
		</div>
	</div>
</header>
<aside class="menu-phone">
	<nav>
		
			<a href="/" class="nav-home nav">
				Home
			</a>
		
			<a href="/archives" class="nav-archives nav">
				Archives
			</a>
		
			<a target="_blank" rel="noopener" href="https://photos.google.com/album/AF1QipNoqKYgspQo5O1YhlFXGCQ7p575KBH3Yxf8WHL4?hl=zh-CN" class="nav-gallery nav">
				Gallery
			</a>
		
			<a href="/about" class="nav-about nav">
				About
			</a>
		
	</nav>
</aside>

    <script>setLoadingBarProgress(40);</script>
  <div class="l_body">
    <div class='container clearfix'>
      <div class='l_main'>
        <article id="post-research/others/BERT源代码阅读学习"
  class="post white-box article-type-post"
  itemscope itemprop="blogPost">
	<section class='meta'>
	<h2 class="title">
  	<a href="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/">
    	BERT源代码阅读学习
    </a>
  </h2>
	<time>
	  11月 19, 2022
	</time>
	
	</section>
	
		<section class="toc-wrapper"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">BERT源代码阅读学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">1.1.</span> <span class="toc-text">References</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%EF%BC%88%E8%AE%BA%E6%96%87-amp-Transformer%E6%9E%B6%E6%9E%84%E6%88%AA%E5%9B%BE%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">1. 模型结构（论文&amp;Transformer架构截图）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E8%AE%BA%E6%96%87-amp-Transformer%E6%9E%B6%E6%9E%84%E6%88%AA%E5%9B%BE"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.1 论文&amp;Transformer架构截图</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-1-Transformer%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">1.1.1 Transformer架构图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-2-Multi-Head-Self-Attention"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">1.1.2 Multi-Head Self Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-3-BERT-Embedding"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">1.1.3 BERT Embedding</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.</span> <span class="toc-text">2. 代码学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%9F%BA%E7%A1%80%E7%AE%80%E5%8C%96pipeline%E4%BB%A3%E7%A0%81"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1 基础简化pipeline代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-model"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2 model</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-embeddings%E3%80%81encoder%E3%80%81pooler%EF%BC%88%E2%80%BB%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">2.2.1 embeddings、encoder、pooler（※重点）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7-class-BertEmbeddings%E5%B1%82%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.2.1.1.</span> <span class="toc-text">· class BertEmbeddings层结构</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-init"><span class="toc-number">1.3.2.1.1.1.</span> <span class="toc-text">1) init</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-forward"><span class="toc-number">1.3.2.1.1.2.</span> <span class="toc-text">2) forward</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-%E7%BB%BC%E5%90%88%E5%88%AB%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%81%9A%E4%B8%80%E4%B8%AA%E6%80%BB%E7%BB%93"><span class="toc-number">1.3.2.1.1.3.</span> <span class="toc-text">3) 综合别人博客做一个总结</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#4-%E8%A1%A5%E5%85%85%EF%BC%9Apositional-embedding%E7%9A%84%E4%B8%8D%E5%90%8C%E6%96%B9%E5%BC%8F"><span class="toc-number">1.3.2.1.1.4.</span> <span class="toc-text">4) 补充：positional embedding的不同方式</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7class-BertSelfAttention%EF%BC%9A%E8%A2%ABBertAttention%E8%B0%83%E7%94%A8%EF%BC%88%E2%80%BB%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">1.3.2.1.2.</span> <span class="toc-text">·class BertSelfAttention：被BertAttention调用（※重点）</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-init-1"><span class="toc-number">1.3.2.1.2.1.</span> <span class="toc-text">1) init</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-forward-1"><span class="toc-number">1.3.2.1.2.2.</span> <span class="toc-text">2) forward</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7class-BertSelfOutput-%E8%A2%ABBertAttention%E8%B0%83%E7%94%A8"><span class="toc-number">1.3.2.1.3.</span> <span class="toc-text">·class BertSelfOutput: 被BertAttention调用</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-init-amp-forward"><span class="toc-number">1.3.2.1.3.1.</span> <span class="toc-text">1) init&amp;forward</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7class-BertAttention%EF%BC%9A%E8%A2%ABBertLayer%E8%B0%83%E7%94%A8"><span class="toc-number">1.3.2.1.4.</span> <span class="toc-text">·class BertAttention：被BertLayer调用</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-init-2"><span class="toc-number">1.3.2.1.4.1.</span> <span class="toc-text">1) init</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-forward-2"><span class="toc-number">1.3.2.1.4.2.</span> <span class="toc-text">2) forward</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7class-BertIntermediate-%E8%A2%ABBertLayer%E8%B0%83%E7%94%A8"><span class="toc-number">1.3.2.1.5.</span> <span class="toc-text">·class BertIntermediate: 被BertLayer调用</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-init-amp-forward-1"><span class="toc-number">1.3.2.1.5.1.</span> <span class="toc-text">1) init&amp;forward</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7class-BertOutput-%E8%A2%ABBertLayer%E8%B0%83%E7%94%A8"><span class="toc-number">1.3.2.1.6.</span> <span class="toc-text">·class BertOutput: 被BertLayer调用</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-init-amp-forward-2"><span class="toc-number">1.3.2.1.6.1.</span> <span class="toc-text">1) init&amp;forward</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7-class-BertLayer-nn-Module-%EF%BC%9A%E8%A2%ABBertEncoder%E8%B0%83%E7%94%A8"><span class="toc-number">1.3.2.1.7.</span> <span class="toc-text">· class BertLayer(nn.Module)：被BertEncoder调用</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-init-3"><span class="toc-number">1.3.2.1.7.1.</span> <span class="toc-text">1) init</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-forward-3"><span class="toc-number">1.3.2.1.7.2.</span> <span class="toc-text">2) forward</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7-class-BertEncoder-nn-Module-%E5%B1%82%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.2.1.8.</span> <span class="toc-text">· class BertEncoder(nn.Module)层结构</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-init-4"><span class="toc-number">1.3.2.1.8.1.</span> <span class="toc-text">1) init</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-forward-4"><span class="toc-number">1.3.2.1.8.2.</span> <span class="toc-text">2) forward</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7-class-BertPooler%EF%BC%9A"><span class="toc-number">1.3.2.1.9.</span> <span class="toc-text">· class BertPooler：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7-class-BertModel-%E5%90%84%E5%B1%82%E7%BB%84%E5%90%88%E5%9C%A8%E4%B8%80%E8%B5%B7%E6%95%B4%E4%BD%93%E7%9A%84%E8%AF%B4%E6%98%8E"><span class="toc-number">1.3.2.1.10.</span> <span class="toc-text">· class BertModel(): 各层组合在一起整体的说明</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-model-state-dict"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">2.2.2 model.state_dict()</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7-%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83model"><span class="toc-number">1.3.2.2.1.</span> <span class="toc-text">· 加载预训练model</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1-%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E"><span class="toc-number">1.3.2.2.1.1.</span> <span class="toc-text">1) 注解说明</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#2-%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="toc-number">1.3.2.2.1.2.</span> <span class="toc-text">2) 参数说明</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#3-%E5%86%85%E9%83%A8%E6%B5%81%E7%A8%8B%E8%AF%B4%E6%98%8E%EF%BC%88%E2%80%BB%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">1.3.2.2.1.3.</span> <span class="toc-text">3) 内部流程说明（※重点）</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#4-%E9%A2%9D%E5%A4%96%E8%A1%A5%E5%85%85"><span class="toc-number">1.3.2.2.1.4.</span> <span class="toc-text">4) 额外补充</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7-%E9%80%9A%E8%BF%87config%E5%8A%A0%E8%BD%BD%E7%A9%BA%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%AE%BE%E7%BD%AEseed"><span class="toc-number">1.3.2.2.2.</span> <span class="toc-text">· 通过config加载空模型并设置seed</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%C2%B7-%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.2.2.3.</span> <span class="toc-text">· 保存模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-tokenizer"><span class="toc-number">1.3.3.</span> <span class="toc-text">2.3 tokenizer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Transformer-amp-BERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B8%AD%E7%9A%84%E9%87%8D%E7%82%B9%E8%AE%B0%E5%BD%95"><span class="toc-number">1.4.</span> <span class="toc-text">3. Transformer&amp;BERT论文阅读中的重点记录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Attention-is-all-you-need"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.1 Attention is all you need</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-BERT"><span class="toc-number">1.4.2.</span> <span class="toc-text">3.2 BERT</span></a></li></ol></li></ol></li></ol></section>
	
	<section class="article typo">
  	<div class="article-entry" itemprop="articleBody">
    	<p>BERT源代码阅读学习，主要是Transformer架构中的Encoder部分，各层的源代码理解与阅读学习</p>
<span id="more"></span>

<h1 id="BERT源代码阅读学习"><a href="#BERT源代码阅读学习" class="headerlink" title="BERT源代码阅读学习"></a>BERT源代码阅读学习</h1><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>BERT源代码学习：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/360988428">https://zhuanlan.zhihu.com/p/360988428</a></p>
<p>Attention is all you need： <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a></p>
<p>BERT：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf&amp;usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ">https://arxiv.org/pdf/1810.04805.pdf&amp;usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ</a></p>
<p>Attention机制详解：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47282410">https://zhuanlan.zhihu.com/p/47282410</a></p>
<p>positional embedding absolute/relative等不同方式：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/121126531">https://zhuanlan.zhihu.com/p/121126531</a></p>
<p>torch中的einsum：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361209187">https://zhuanlan.zhihu.com/p/361209187</a></p>
<p>Self-Attention with Relative Position Representations: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.02155.pdf">https://arxiv.org/pdf/1803.02155.pdf</a></p>
<h2 id="1-模型结构（论文-amp-Transformer架构截图）"><a href="#1-模型结构（论文-amp-Transformer架构截图）" class="headerlink" title="1. 模型结构（论文&amp;Transformer架构截图）"></a>1. 模型结构（论文&amp;Transformer架构截图）</h2><h3 id="1-1-论文-amp-Transformer架构截图"><a href="#1-1-论文-amp-Transformer架构截图" class="headerlink" title="1.1 论文&amp;Transformer架构截图"></a>1.1 论文&amp;Transformer架构截图</h3><h4 id="1-1-1-Transformer架构图"><a href="#1-1-1-Transformer架构图" class="headerlink" title="1.1.1 Transformer架构图"></a>1.1.1 Transformer架构图</h4><p>左边代表Encoder部分，右边代表Decoder部分。两边的区别个人理解是：</p>
<ul>
<li>Encoder是作为NLU（Natrual Language Understanding）来使用的，所以在输入的时候Encoder是能看到全局信息的。从目前接触到的任务来说还是Encoder这边的结构更加常用一些，大部分任务感觉还是属于在NLU的范畴，NLG那边的有些就显得不太好评测或者不是很靠谱；</li>
<li>但是在输入Decoder的时候，因为Decoder一般被NLG（Natural Language Generation）类的任务来使用，所以其需要根据上文来生成下文，故在输入的时候需要加mask，即 <code>Masked Multi-Head Attention</code>。此外在decoder部分中还有一个接收来自Encoder那边信息的Multi-Head Attention，也被称作 <code>encoder-decoder attention layer</code>，这个地方query来自于前一级的decoder层输出，但其key和value来自于encoder的输出，那么理解来说就是decoder的每一个位置作为key和encoder那边key计算相似度，然后聚合来自encoder那边的value信息；</li>
<li><font color="red">和同学讨论后补充：对于Transformer架构的信息，像T5这样的encoder-decoder模型，或者说像是一类依据文本生成文本的，比如翻译任务，那就是使用到整个Transformer架构，其中的encoder-decoder attention可以理解为我需要看着原来的文本来做生成，然后把query看做普通RNN架构中的x，这样x需要聚合来自全部输入文本的信息做attention；对于BERT这类就是只用到Encoder架构；对于GPT类的可能就只是用Decoder部分，里面就没有encoder-decoder attention那个部分了；</font></li>
</ul>
<img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711105616573.png" alt="image-20220711105616573" style="zoom:50%;">

<h4 id="1-1-2-Multi-Head-Self-Attention"><a href="#1-1-2-Multi-Head-Self-Attention" class="headerlink" title="1.1.2 Multi-Head Self Attention"></a>1.1.2 Multi-Head Self Attention</h4><p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711105642016.png" alt="image-20220711105642016"></p>
<h4 id="1-1-3-BERT-Embedding"><a href="#1-1-3-BERT-Embedding" class="headerlink" title="1.1.3 BERT Embedding"></a>1.1.3 BERT Embedding</h4><p>这个是bert模型结构的embedding输入，也需要联合代码看一下这个过程是怎么实现的。</p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711110109758.png" alt="image-20220711110109758"></p>
<p>这里补充贴一张LUKE的图，虽然没看过但是看起来加了一个Entity Type Embedding，好像还是个比较有名的工作</p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712195033334.png" alt="image-20220712195033334"></p>
<h2 id="2-代码学习"><a href="#2-代码学习" class="headerlink" title="2. 代码学习"></a>2. 代码学习</h2><h3 id="2-1-基础简化pipeline代码"><a href="#2-1-基础简化pipeline代码" class="headerlink" title="2.1 基础简化pipeline代码"></a>2.1 基础简化pipeline代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel, BertConfig</span><br><span class="line"><span class="keyword">from</span> transformers.models.bert <span class="keyword">import</span> modeling_bert  <span class="comment"># 从这里看源代码</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预训练模型加载</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;./bert_base_uncased&quot;</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&quot;./bert_base_uncased&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入&amp;输出</span></span><br><span class="line">text = <span class="string">&quot;Germany beat Argentina 2-0 in the World Cup Final.&quot;</span></span><br><span class="line">encoded_input = tokenizer(text, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line">output = model(**encoded_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取一句话的输出，还有cls token的输出</span></span><br><span class="line"><span class="built_in">print</span>(output[<span class="string">&#x27;pooler_output&#x27;</span>].shape)  <span class="comment"># torch.Size([1, 768])</span></span><br><span class="line"><span class="built_in">print</span>(output[<span class="string">&#x27;last_hidden_state&#x27;</span>].shape)  <span class="comment"># torch.Size([1, 14, 768])</span></span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<p>总结：加载<strong>config.json</strong>、<strong>vocab.txt</strong>还有<strong>pytorch_model.bin</strong>三个文件。其中通过 <code>from_pretrained(&quot;./bert_base_uncased&quot;)</code>进行指定路径，如果不指定路径的话好像会从huggingface那边下载model，指定路径的话就需要文件夹下有这三个文件；</p>
<h3 id="2-2-model"><a href="#2-2-model" class="headerlink" title="2.2 model"></a>2.2 model</h3><h4 id="2-2-1-embeddings、encoder、pooler（※重点）"><a href="#2-2-1-embeddings、encoder、pooler（※重点）" class="headerlink" title="2.2.1 embeddings、encoder、pooler（※重点）"></a>2.2.1 embeddings、encoder、pooler（※重点）</h4><p>通过<code>model = BertModel.from_pretrained(&quot;./bert_base_uncased&quot;)</code>加载模型后，首先可以在这里调试model这个对象包含的内容，model是BertModel的实例化，模型结构主要由<code>model.embeddings </code>（BERTEmbeddings类对象），<code>model.encoder</code>（BertEncoder类对象），<code>model.pooler</code>（BertPooler对象）组成。点开后可以看到各个地方的模型结构与层数，之后会随着模型调试查看数据流向和数据维度的变化。</p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711135726421.png" alt="image-20220711135726421"></p>
<h5 id="·-class-BertEmbeddings层结构"><a href="#·-class-BertEmbeddings层结构" class="headerlink" title="· class BertEmbeddings层结构"></a>· class BertEmbeddings层结构</h5><blockquote>
<p>/Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertEmbeddings</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h6 id="1-init"><a href="#1-init" class="headerlink" title="1) init"></a>1) <strong>init</strong></h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">    self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">    self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span></span><br><span class="line">    <span class="comment"># any TensorFlow checkpoint file</span></span><br><span class="line">    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">    self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line">    <span class="comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span></span><br><span class="line">    self.position_embedding_type = <span class="built_in">getattr</span>(config, <span class="string">&quot;position_embedding_type&quot;</span>, <span class="string">&quot;absolute&quot;</span>)</span><br><span class="line">    self.register_buffer(<span class="string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">if</span> version.parse(torch.__version__) &gt; version.parse(<span class="string">&quot;1.6.0&quot;</span>):</span><br><span class="line">        self.register_buffer(</span><br><span class="line">            <span class="string">&quot;token_type_ids&quot;</span>,</span><br><span class="line">            torch.zeros(self.position_ids.size(), dtype=torch.long),</span><br><span class="line">            persistent=<span class="literal">False</span>,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p>简单对init进行解释，这里有三个<code>nn.Embedding</code>层：</p>
<ul>
<li>self.word_embeddings：维度从vocab_size（30522）到hidden_size（768）转化，<font color="red">TODO：padding_idx的参数是做什么用的？</font> </li>
<li>self.position_embeddings：维度从max_position_embeddings（512）到hidden_size（768）转化；</li>
<li>self.token_type_embeddings：维度从config.type_vocab_size（2，这里的2代表的是有两种类别的，第一个[SEP]前都是0，第二个[SEP]前都是1，这样交叉的）到到hidden_size（768），或者是用来表示padding地方的差异；</li>
</ul>
<p>这里的<code>self.LayerNorm</code>和<code>self.dropout</code>是剩下两个和forward比较相关的层，初始化都比较正常</p>
<p><font color="red">和同学讨论下这个nn.Embedding层的用处，之前对这个层一直不是太理解，大概目前的理解是传入的一个比如input_ids是[1, 14]这个shape的，首先其被转化成一个one-hot的表示也就是[1, 14, 30522(这个维度类似一个词典大小)]，然后过一个[30522, 768]的，两个乘在一起就有一种对应位把元素取出来的感觉，这样就得到了最终的embedding表示[1, 14, 768]</font> </p>
<p><font color="red">词表大小30522是针对input_ids embedding的，那么针对positional embedding就是max_seq_len，针对token type的就是2（只有0和1代表两类交替的）</font></p>
<h6 id="2-forward"><a href="#2-forward" class="headerlink" title="2) forward"></a>2) forward</h6><p>forward传入的参数中</p>
<ul>
<li>input_ids **[1, seq_len]**：tensor([[ 101, 2762, 3786, 5619, 1016, 1011, 1014, 1998, 2180, 1996, 2088, 2452, 2345,  102]])，这是tokenizer.convert_tokens_to_ids()的结果应该，那边BERT好像还对应了个wordpiecetoken，101是[CLS]，102是[PAD]</li>
<li>token_type_ids **[1, seq_len]**：tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])，传入的没有[SEP]，只有一类token</li>
<li>position_ids：暂时为None</li>
<li>inputs_embeds：暂时为None</li>
</ul>
<p>step1：根据input_ids提取得到的seq_len长度，初始化position_ids **[1, seq_len]**：tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]])</p>
<p>step2：获取input_embeds和token_type_embeddings，通过上面的传入参数以及nn.Embedding层，并把这两个加在一起</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">embeddings = inputs_embeds + token_type_embeddings</span><br></pre></td></tr></table></figure>

<p>embeddings.shape <strong>[1, seq_len, hidden_dim]</strong></p>
<p>step3：如果self.position_embedding_type是”absolute”绝对的话，就传入后加上position_embeddings，此时embeddings.shape **[1, seq_len, hidden_dim]**没有变化；absolute就是绝对位置编码，理解是[0, 1, 2, 3…]这样的绝对位置；<font color="red">还有一种position_embedding是相对位置编码的embedding，部分代码整合在了BertSelfAttention这个类中，博客参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/chenf1995/article/details/122971023">https://blog.csdn.net/chenf1995/article/details/122971023</a></font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;absolute&quot;</span>:</span><br><span class="line">    position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">    embeddings += position_embeddings</span><br></pre></td></tr></table></figure>

<p>step4：过LayerNorm和dropout，维度不会改变，BertEmbeddings这个类最终输出了一个embeddings <strong>[1, seq_len, hidden_dim]<strong>的信息，代表将要输入进入encoder结构部分的embedding</strong>input_embedding+token_type_embedding+position_embedding</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">embeddings = self.LayerNorm(embeddings)</span><br><span class="line">embeddings = self.dropout(embeddings)</span><br><span class="line"><span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>

<h6 id="3-综合别人博客做一个总结"><a href="#3-综合别人博客做一个总结" class="headerlink" title="3) 综合别人博客做一个总结"></a>3) 综合别人博客做一个总结</h6><p>word_embeddings是上文中subword tokenization对应的词嵌入；</p>
<p>token_type_embeddings是用于表示当前词所在的句子，辅助区别句子与padding，句子对通过[SEP]分隔之间的差异；</p>
<p>position_embeddings是句子中每个词的位置嵌入，用于区别词的顺序，博客说这个地方是训练出来的（从代码看确实如此），而不是计算得到固定嵌入，可能固定嵌入不利于拓展；</p>
<p>三个embedding层不带权重直接加在一起，过LayerNorm+dropout后产生输出，大小为**[batch_size, seq_len, hidden_size]**</p>
<h6 id="4-补充：positional-embedding的不同方式"><a href="#4-补充：positional-embedding的不同方式" class="headerlink" title="4) 补充：positional embedding的不同方式"></a>4) 补充：positional embedding的不同方式</h6><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/121126531">https://zhuanlan.zhihu.com/p/121126531</a></p>
<p>背景：</p>
<p>词与词之间的顺序关系往往影响整个句子的含义，因此在对文本数据进行建模的时候需要考虑词与词之间的顺序关系；</p>
<p>建模文本中的顺序关系必须要使用positional encoding吗？-&gt; 不一定，只有使用位置不敏感的模型对文本数据进行建模的时候，才需要额外使用positional encoding；如果模型的输出会随着输入文本数据顺序的变化而变化，那么这个模型就是关于位置敏感的，反之则是位置不敏感的；</p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220713092751251.png" alt="image-20220713092751251"></p>
<p>在常用的文本模型中，RNN类的就是关于位置敏感的，使用RNN类模型对文本数据建模的时候，模型结构天然考虑了文本中词与词之间的顺序关系。<strong>而以attention机制为核心的transformer则是位置不敏感的，使用这一类位置不敏感的模型的时候需要额外加入positional encoding引入文本中词与词的顺序关系；</strong></p>
<p>具体操作：</p>
<p>对于transformer模型的positional encoding有两种主流方式：即绝对位置编码和相对位置编码</p>
<p>其中<strong>absolute positional embedding（绝对位置编码）</strong>是相对简单理解的，直接对不同位置随机初始化一个positional embedding，加到word embedding和token_type embedding上输入模型作为参数进行训练</p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711110109758.png" alt="image-20220711110109758"></p>
<p>另一种是<strong>relative positional embedding（相对位置编码）</strong>，首先motivation是不同位置的positional embedding固然不同，但是位置1和位置2的距离比位置3和位置10的距离更近，位置1 2和3 4距离都只差1，这些关于位置的<strong>相对含义</strong>模型通过绝对位置编码是否能够学习？绝对位置编码没有约束位置之间这些隐含关系，只能期待他隐式的学习到，所以是否有更合理的方法能够显式的让模型理解位置的相对关系？</p>
<p><font color="blue">11111</font> </p>
<p>详细看一下huggingface transformer代码中的这个部分，参数有”absolute”、”relative_key”和”relative_key_query”三种，这些参数在<code>class BertSelfAttention(nn.Module)</code>这个类中，而不是在最开始的<code>BertEmbedding</code>那块的</p>
<ul>
<li><code>absolute</code>：默认值，这部分就不用处理（对这个地方的处理在Embedding层）</li>
<li><code>relative_key</code>：对key_layer作处理，将其与这里的<code>positional_embedding</code>和key矩阵相乘作为key相关的位置编码；</li>
<li><code>relative_key_query</code>：对key和value都进行相乘以作为位置编码。</li>
</ul>
<p><font color="red">用下面代码简单加一下注释</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertSelfAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, position_embedding_type=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span> <span class="keyword">or</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">          self.max_position_embeddings = config.max_position_embeddings  <span class="comment"># 512</span></span><br><span class="line">          self.distance_embedding = nn.Embedding(<span class="number">2</span> * config.max_position_embeddings - <span class="number">1</span>, self.attention_head_size)  <span class="comment"># [512*2-1, 64 (即hidden//head_num)]</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span></span><br><span class="line">attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="number">1</span>, -<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span> <span class="keyword">or</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">    seq_length = hidden_states.size()[<span class="number">1</span>]  <span class="comment"># 14</span></span><br><span class="line">    position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># [14, 1]shape的tensor</span></span><br><span class="line">    position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(<span class="number">1</span>, -<span class="number">1</span>) <span class="comment"># [1, 14]</span></span><br><span class="line">    distance = position_ids_l - position_ids_r  <span class="comment"># [seqlen, seq_len]</span></span><br><span class="line">    positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - <span class="number">1</span>)  <span class="comment"># [seqlen, seqlen, hidden_size]</span></span><br><span class="line">    positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  <span class="comment"># fp16 compatibility</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># query_layer : batchsize, seqlen, hidden -&gt; batchsize, head_num, seqlen, hidden//head_num【multi-head】</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># bhld,lrd -&gt; bhld不变，lrd去掉一个维度变成rd -&gt; bhld不变, rd转置变成dr -&gt; 乘 -&gt; bhlr</span></span><br><span class="line">    <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span>:</span><br><span class="line">        relative_position_scores = torch.einsum(<span class="string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding)</span><br><span class="line">        attention_scores = attention_scores + relative_position_scores</span><br><span class="line">    <span class="keyword">elif</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">        relative_position_scores_query = torch.einsum(<span class="string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding)</span><br><span class="line">        relative_position_scores_key = torch.einsum(<span class="string">&quot;bhrd,lrd-&gt;bhlr&quot;</span>, key_layer, positional_embedding)</span><br><span class="line">        attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key</span><br></pre></td></tr></table></figure>

<ul>
<li><code>seq_length</code>：这句话的长度，比如14</li>
<li><code>position_ids_l</code>：初始化是一个例如[14, 1]的向量，存储的类似于[[0], [1], [2] …]这样的</li>
<li><code>position_ids_r</code>：初始化是一个例如[1, 14]的向量，存储的类似于[[0, 1, 2, 3, 4]]这样的</li>
<li><code>distance</code>：初始化直接用<code>position_ids_l</code>-<code>position_ids_r</code>，这里直接广播减法，是一个[14, 14]维度的</li>
</ul>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220713165208863.png" alt="image-20220713165208863"></p>
<p>因为这个地方是在attention这块来做的embedding，attention那边的scoreshape是[batch, head, seq_len, seq_len]的，代表query每个位置处对于key的注意力，那么可以在这里对query和key都搞positional embedding</p>
<p>通过上面几个做操作搞了一个<code>positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)</code>，这个有点不为什么每个要把512-1给加上，这样处理完后distance变成了如下所示的tensor</p>
<p><font color="red">两个距离相隔最远是512，那么这样处理后能保证所有数字都是&gt;=0的，因为离的最远的也就是512了，然后最远的将会到达1023那个感觉</font> </p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220713170145685.png" alt="image-20220713170145685"></p>
<p>positional_embedding由distance_embedding层后得到，distance_embedding层的传入参数是[512<em>2-1, 64 (即hidden//head_num)]也能理解了，因为词表大小是差不多0-1023的；；positional_embedding的shape是</em>*[seq_len, seq_len, hidden]**的，如果是一个batch的话，那么应该是这个batch里面最大的那个seq_len？</p>
<p>下面代码把query_layer[1, 12, 14, 64]和positional_embedding[14, 14, 64]作为这个<code>torch.einsum</code>的输入，这个地方参考文档<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361209187%EF%BC%8C%E5%B0%B1%E6%98%AF%E6%8A%8A%E5%BD%A2%E7%8A%B6bhld,lrd%E7%9A%84%E4%B8%A4%E4%B8%AAtensor%E5%8A%A0%E6%88%90%E4%B8%80%E4%B8%AAbhlr%E7%9A%84%EF%BC%8C%E8%BF%99%E9%87%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B2%A1%E7%94%A8%E4%B8%A4%E4%B8%AAl%E5%8F%AF%E8%83%BD%E6%98%AF%E5%9B%A0%E4%B8%BA%E5%89%8D%E9%9D%A2%E4%B8%A4%E4%B8%AAseq_len%E6%9C%AC%E8%B4%A8%E4%B8%8A%E4%B8%80%E4%B8%AA%E6%9D%A5%E8%87%AA%E4%BA%8Equery%EF%BC%8C%E4%B8%80%E4%B8%AA%E6%9D%A5%E8%87%AA%E4%BA%8Ekey%EF%BC%8C%E8%80%8C%E5%AE%9E%E9%99%85%E4%B8%8A%E6%98%AF%E4%B8%8D%E9%9C%80%E8%A6%81%E7%AD%89%E9%95%BF%E7%9A%84%EF%BC%8C%E5%8F%AA%E6%98%AF%E4%B8%80%E8%88%AC%E6%93%8D%E4%BD%9C%E9%BB%98%E8%AE%A4%E4%B8%BA%E7%AD%89%E9%95%BF%E7%9A%84%E4%BA%86%EF%BC%9B">https://zhuanlan.zhihu.com/p/361209187，就是把形状bhld,lrd的两个tensor加成一个bhlr的，这里为什么没用两个l可能是因为前面两个seq_len本质上一个来自于query，一个来自于key，而实际上是不需要等长的，只是一般操作默认为等长的了；</a></p>
<p>重点：这里以第一个作为示例，l和d在前后的箭头中都出现了，那就是在这两个维度上操作,query_layer[1, 12, <strong>14</strong>, <strong>64</strong>]和positional_embedding[<strong>14</strong>, 14, <strong>64</strong>]，转置乘，出来一个relative_position_scores_query**[1, 12, 14, 14]**的，聚合来自position的信息</p>
<p><font color="red">TODO：还弄得没那么明白，大概明白个意思，之后还要详细看看</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">relative_position_scores_query = torch.einsum(<span class="string">&quot;bhld,lrd-&gt;bhlr&quot;</span>, query_layer, positional_embedding)</span><br><span class="line">relative_position_scores_key = torch.einsum(<span class="string">&quot;bhrd,lrd-&gt;bhlr&quot;</span>, key_layer, positional_embedding)</span><br></pre></td></tr></table></figure>

<p>最后，执行下述代码，注意这个<strong>相对位置编码</strong>过程可以只对query做，也可以对query和key同时做</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key</span><br></pre></td></tr></table></figure>

<p><font color="red">TODO：</font> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/121126531%E8%BF%99%E9%87%8C%E8%BF%98%E4%BB%8B%E7%BB%8D%E5%88%B0%E4%BA%86%EF%BC%9ASinusoidal">https://zhuanlan.zhihu.com/p/121126531这里还介绍到了：Sinusoidal</a> Position Encoding和Complex embedding</p>
<h5 id="·class-BertSelfAttention：被BertAttention调用（※重点）"><a href="#·class-BertSelfAttention：被BertAttention调用（※重点）" class="headerlink" title="·class BertSelfAttention：被BertAttention调用（※重点）"></a>·class BertSelfAttention：被BertAttention调用（※重点）</h5><h6 id="1-init-1"><a href="#1-init-1" class="headerlink" title="1) init"></a>1) init</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertSelfAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, position_embedding_type=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> config.hidden_size % config.num_attention_heads != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(config, <span class="string">&quot;embedding_size&quot;</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;The hidden size (<span class="subst">&#123;config.hidden_size&#125;</span>) is not a multiple of the number of attention &quot;</span></span><br><span class="line">                <span class="string">f&quot;heads (<span class="subst">&#123;config.num_attention_heads&#125;</span>)&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        self.attention_head_size = <span class="built_in">int</span>(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line"></span><br><span class="line">        self.query = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.key = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.value = nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</span><br><span class="line">        self.position_embedding_type = position_embedding_type <span class="keyword">or</span> <span class="built_in">getattr</span>(</span><br><span class="line">            config, <span class="string">&quot;position_embedding_type&quot;</span>, <span class="string">&quot;absolute&quot;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;relative_key&quot;</span> <span class="keyword">or</span> self.position_embedding_type == <span class="string">&quot;relative_key_query&quot;</span>:</span><br><span class="line">            self.max_position_embeddings = config.max_position_embeddings</span><br><span class="line">            self.distance_embedding = nn.Embedding(<span class="number">2</span> * config.max_position_embeddings - <span class="number">1</span>, self.attention_head_size)</span><br><span class="line"></span><br><span class="line">        self.is_decoder = config.is_decoder</span><br></pre></td></tr></table></figure>

<p>这个地方是整个BERT架构中非常核心的区域</p>
<ul>
<li>self.num_attention_heads = config.num_attention_heads：几头注意力机制，在config文件里这里设置为12（一般BERT也是12）</li>
<li>self.attention_head_size = int(config.hidden_size / config.num_attention_heads)：config.hidden_size是768，所以每个头的hidden_size将会是768/12=64；</li>
<li>self.all_head_size是self.num_attention_heads（12）再乘回self.attention_head_size（64），猜测这样的原因是因为整除造成的可能回来后就不是768了；<font color="red">从其他博客也看到和剪枝有关</font> </li>
<li>self.query、self.key、self.value三个权重矩阵，都是一个hidden_size（768）到内部这个all_head_size（可能是768，也可能有损失）的转化；</li>
<li>self.dropout = nn.Dropout(config.attention_probs_dropout_prob)：简单的dropout层；</li>
<li>self.position_embedding_type：这与相对/绝对位置编码有关，如果是绝对位置编码那么在BertEmbedding层里面已经给结合进去了，<font color="blue">如果是相对位置编码要在这里实现，不过这个地方暂时先跳过了</font>；</li>
<li>self.is_decoder = config.is_decoder：标识是否decoder，BERT只是一个encoder就不涉及到这个部分了；</li>
</ul>
<h6 id="2-forward-1"><a href="#2-forward-1" class="headerlink" title="2) forward"></a>2) forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_for_scores</span>(<span class="params">self, x</span>):</span><br><span class="line">    new_x_shape = x.size()[:-<span class="number">1</span>] + (self.num_attention_heads, self.attention_head_size)</span><br><span class="line">    x = x.view(new_x_shape)</span><br><span class="line">    <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<p>首先是这个函数，这个函数是拆多头用的，输入的x是[batch, seq_len, hidden]的，先指定new_shape是[batch, seq_len, num_attention_heads, attention_head_size]（一般可以认为是[batch, seq_len, 12, 64]），然后.view转化，然后再通过permute改变顺序为**[batch, attention_head_size, seq_len, num_attention_heads]**，这样是因为attention_head_size可以归为”batch“那边的维度了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">    attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    head_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    encoder_hidden_states: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    encoder_attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    past_key_value: <span class="type">Optional</span>[<span class="type">Tuple</span>[<span class="type">Tuple</span>[torch.FloatTensor]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    output_attentions: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor]:</span><br><span class="line">    mixed_query_layer = self.query(hidden_states)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 忽略了cross-attention部分</span></span><br><span class="line">    <span class="comment"># hidden_states [batch, seqlen, hidden]</span></span><br><span class="line">    key_layer = self.transpose_for_scores(self.key(hidden_states))  <span class="comment"># [batch, num_head, seqlen_key, hidden//num_head]</span></span><br><span class="line">    value_layer = self.transpose_for_scores(self.value(hidden_states))</span><br><span class="line">    query_layer = self.transpose_for_scores(mixed_query_layer) <span class="comment"># [batch, num_head, seqlen_query, hidden//num_head]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span></span><br><span class="line">    attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="number">1</span>, -<span class="number">2</span>))  <span class="comment"># </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 忽略了相对位置编码的处理</span></span><br><span class="line"></span><br><span class="line">    attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line">    <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function)</span></span><br><span class="line">        attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">    attention_probs = nn.functional.softmax(attention_scores, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">    <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">    attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask heads if we want to</span></span><br><span class="line">    <span class="keyword">if</span> head_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        attention_probs = attention_probs * head_mask</span><br><span class="line"></span><br><span class="line">    context_layer = torch.matmul(attention_probs, value_layer)  <span class="comment"># [batch, num_head, seqlen, hidden//num_head]</span></span><br><span class="line"></span><br><span class="line">    context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">    new_context_layer_shape = context_layer.size()[:-<span class="number">2</span>] + (self.all_head_size,)</span><br><span class="line">    context_layer = context_layer.view(new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">    outputs = (context_layer, attention_probs) <span class="keyword">if</span> output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 忽略了is_decoder部分</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<p>上面贴出来的这段代码省略了针对<code>is_cross_attention</code>（即encoder-decoder attention那个部分的一些处理），此外还忽略了<code>if self.is_decoder:</code>部分的处理，并且忽略了<code>if self.position_embedding_type == &quot;relative_key&quot; </code>相对位置编码部分的处理；</p>
<p>step1：首先是这个部分，hidden_states**[batch, seq_len, hidden_size]<strong>这个tensor过了self.query、self.value、self.key三个linear矩阵，由于这三个linear一般不改变hidden_size，这样得到的是三个</strong>[batch, seq_len, hidden_size]<strong>形状的tensor，通过上面提到的transpose_for_scores进行reshape，得到三个</strong>[batch, head_num, seq_len, attention_head_size]**（一般可以是[1, 12, seq_len, 768]）这样的tensor，并且被命名为key_layer、value_layer、query_layer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mixed_query_layer = self.query(hidden_states)</span><br><span class="line">key_layer = self.transpose_for_scores(self.key(hidden_states))</span><br><span class="line">value_layer = self.transpose_for_scores(self.value(hidden_states))</span><br><span class="line">query_layer = self.transpose_for_scores(mixed_query_layer)</span><br></pre></td></tr></table></figure>

<p>step2：这里就是Q·K^T那个部分了，转置就是在后两个维度上转置，输出的attention_scores是**[batch, head_num, seq_len, seq_len]**形状的tensor，代表query中每个位置处对key全局所有的注意力（后面要过softmax）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="number">1</span>, -<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p>step3：依照博客简单理解一下不同的positional_embedding_type，<font color="blue">这个部分暂时忽略了</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">absolute：默认值，这部分就不用处理；</span><br><span class="line">relative_key：对key_layer作处理，将其与这里的positional_embedding和key矩阵相乘作为key相关的位置编码；</span><br><span class="line">relative_key_query：对key和value都进行相乘以作为位置编码。</span><br></pre></td></tr></table></figure>

<p>step4：计算attention_scores，attention_probs；attention_scores在计算query和key的点乘后除以根号下d_k，<strong>注意这里的self.attention_head_size是64那个地方的，也就是分成12个头后每个头的hidden_size</strong>，如果带有attention_mask的话<font color="red">（注意，一般来说肯定是会有atttention_mask的，应该会在调用这个BertAttention的时候传给他，因为一个batch中大家不等长，肯定要通过mask padding到512这种感觉的）</font> ；；在计算attention_scores时候用的是加法，因为softmax那块要一个很大的负数，比如-1e9这样的，然后过softmax，注意softmax的维度是-1代表query中每个token对所有key位置处的token的attention；；；最后过一个self.dropout，<font color="red">TODO：暂时有点没理解为什么在这里过dropout，而不是乘了之后</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span></span><br><span class="line">attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="number">1</span>, -<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 忽略了相对位置编码的处理</span></span><br><span class="line"></span><br><span class="line">attention_scores = attention_scores / math.sqrt(self.attention_head_size)</span><br><span class="line"><span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function)</span></span><br><span class="line">    attention_scores = attention_scores + attention_mask</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">attention_probs = nn.functional.softmax(attention_scores, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line"><span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">attention_probs = self.dropout(attention_probs)</span><br></pre></td></tr></table></figure>

<p>step5：这里主要就是输出整合了，再reshape回去，变成了**[batch, seq_len, hidden_size]**的这个形状，另外看到config中output_attentions那个参数的作用，要不要把每层的这个attention返回回去，至此<code>class BertSelfAttention(nn.Module)</code>这个地方的forward结束了；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">new_context_layer_shape = context_layer.size()[:-<span class="number">2</span>] + (self.all_head_size,)</span><br><span class="line">context_layer = context_layer.view(new_context_layer_shape)</span><br><span class="line"></span><br><span class="line">outputs = (context_layer, attention_probs) <span class="keyword">if</span> output_attentions <span class="keyword">else</span> (context_layer,)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<h5 id="·class-BertSelfOutput-被BertAttention调用"><a href="#·class-BertSelfOutput-被BertAttention调用" class="headerlink" title="·class BertSelfOutput: 被BertAttention调用"></a>·class BertSelfOutput: 被BertAttention调用</h5><h6 id="1-init-amp-forward"><a href="#1-init-amp-forward" class="headerlink" title="1) init&amp;forward"></a>1) init&amp;forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertSelfOutput</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states: torch.Tensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<p>这个地方代码结构是相对比较简单的，<font color="red"><strong>这里也展现出了BERT中存在的一层add&amp;norm操作，这里应该还只是attention这个部分的内容</strong></font> </p>
<h5 id="·class-BertAttention：被BertLayer调用"><a href="#·class-BertAttention：被BertLayer调用" class="headerlink" title="·class BertAttention：被BertLayer调用"></a>·class BertAttention：被BertLayer调用</h5><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712110214629.png" alt="image-20220712110214629" style="zoom:50%;">

<h6 id="1-init-2"><a href="#1-init-2" class="headerlink" title="1) init"></a>1) init</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, position_embedding_type=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)</span><br><span class="line">        self.output = BertSelfOutput(config)</span><br><span class="line">        self.pruned_heads = <span class="built_in">set</span>()</span><br></pre></td></tr></table></figure>

<p>attention的实现还是不在这里，self.self这个是multi-head self attention机制的实现，self.output的操作是第一个这里完成的部分；</p>
<p>该层中使用到了<code>self.pruned_heads = set()</code>这样一种节约显存的技术，暂时没有了解太深；</p>
<h6 id="2-forward-2"><a href="#2-forward-2" class="headerlink" title="2) forward"></a>2) forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">    attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    head_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    encoder_hidden_states: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    encoder_attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    past_key_value: <span class="type">Optional</span>[<span class="type">Tuple</span>[<span class="type">Tuple</span>[torch.FloatTensor]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    output_attentions: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor]:</span><br><span class="line">    self_outputs = self.self(</span><br><span class="line">        hidden_states,</span><br><span class="line">        attention_mask,</span><br><span class="line">        head_mask,</span><br><span class="line">        encoder_hidden_states,</span><br><span class="line">        encoder_attention_mask,</span><br><span class="line">        past_key_value,</span><br><span class="line">        output_attentions,</span><br><span class="line">    )</span><br><span class="line">    attention_output = self.output(self_outputs[<span class="number">0</span>], hidden_states)</span><br><span class="line">    outputs = (attention_output,) + self_outputs[<span class="number">1</span>:]  <span class="comment"># add attentions if we output them</span></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<p>有了上面的<code>BertSelfAttention</code>和<code>BertSelfOutput</code>后，这个组件就比较好理解了</p>
<h5 id="·class-BertIntermediate-被BertLayer调用"><a href="#·class-BertIntermediate-被BertLayer调用" class="headerlink" title="·class BertIntermediate: 被BertLayer调用"></a>·class BertIntermediate: 被BertLayer调用</h5><p>在BertAttention这个模块后，还有一个FFNN的操作，这里包含有激活函数；<font color="red">TODO：为什么有些地方需要激活函数，有些地方就不用？像CV那边的话，经常几个层过后就来一个激活，但是这里比如BertAttention里面就没有激活</font> </p>
<h6 id="1-init-amp-forward-1"><a href="#1-init-amp-forward-1" class="headerlink" title="1) init&amp;forward"></a>1) init&amp;forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertIntermediate</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(config.hidden_act, <span class="built_in">str</span>):</span><br><span class="line">            self.intermediate_act_fn = ACT2FN[config.hidden_act]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.intermediate_act_fn = config.hidden_act</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.intermediate_act_fn(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<p>这个里面调用了<code>config.hidden_act</code>，在config文件那边的话这个地方是<code>&quot;gelu&quot;</code>，对应的也就是gelu激活函数，整体来看这个层结构还是很简单的，<font color="red">其中注意dense这个层把768转化为一个config.intermediate_size3072了</font> </p>
<h5 id="·class-BertOutput-被BertLayer调用"><a href="#·class-BertOutput-被BertLayer调用" class="headerlink" title="·class BertOutput: 被BertLayer调用"></a>·class BertOutput: 被BertLayer调用</h5><p><font color="red">注意这里不是BertSelfOutput，刚才那个是中间层的，这个是一个BLOCK的</font> </p>
<h6 id="1-init-amp-forward-2"><a href="#1-init-amp-forward-2" class="headerlink" title="1) init&amp;forward"></a>1) init&amp;forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertOutput</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states: torch.Tensor, input_tensor: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        hidden_states = self.dense(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.LayerNorm(hidden_states + input_tensor)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>

<p>主要负责的也是一些整合，还有residual的部分，<font color="red">其中注意dense层把intermidiate_size又转化会config.hidden_size了</font> </p>
<h5 id="·-class-BertLayer-nn-Module-：被BertEncoder调用"><a href="#·-class-BertLayer-nn-Module-：被BertEncoder调用" class="headerlink" title="· class BertLayer(nn.Module)：被BertEncoder调用"></a>· class BertLayer(nn.Module)：被BertEncoder调用</h5><h6 id="1-init-3"><a href="#1-init-3" class="headerlink" title="1) init"></a>1) init</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.chunk_size_feed_forward = config.chunk_size_feed_forward</span><br><span class="line">    self.seq_len_dim = <span class="number">1</span></span><br><span class="line">    self.attention = BertAttention(config)</span><br><span class="line">    self.is_decoder = config.is_decoder</span><br><span class="line">    self.add_cross_attention = config.add_cross_attention</span><br><span class="line">    <span class="keyword">if</span> self.add_cross_attention:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.is_decoder:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;<span class="subst">&#123;self&#125;</span> should be used as a decoder model if cross attention is added&quot;</span>)</span><br><span class="line">        self.crossattention = BertAttention(config, position_embedding_type=<span class="string">&quot;absolute&quot;</span>)</span><br><span class="line">    self.intermediate = BertIntermediate(config)</span><br><span class="line">    self.output = BertOutput(config)</span><br></pre></td></tr></table></figure>

<p>可以简单理解为，依次调用了BertAttention、BertIntermediate、BertOutput完成了一个BLOCK的操作</p>
<h6 id="2-forward-3"><a href="#2-forward-3" class="headerlink" title="2) forward"></a>2) forward</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">    attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    head_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    encoder_hidden_states: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    encoder_attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    past_key_value: <span class="type">Optional</span>[<span class="type">Tuple</span>[<span class="type">Tuple</span>[torch.FloatTensor]]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    output_attentions: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor]:</span><br><span class="line">    <span class="comment"># decoder uni-directional self-attention cached key/values tuple is at positions 1,2</span></span><br><span class="line">    self_attn_past_key_value = past_key_value[:<span class="number">2</span>] <span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    self_attention_outputs = self.attention(</span><br><span class="line">        hidden_states,</span><br><span class="line">        attention_mask,</span><br><span class="line">        head_mask,</span><br><span class="line">        output_attentions=output_attentions,</span><br><span class="line">        past_key_value=self_attn_past_key_value,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 忽略一些is_decoder的操作</span></span><br><span class="line">    </span><br><span class="line">    attention_output = self_attention_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    outputs = self_attention_outputs[<span class="number">1</span>:]  <span class="comment"># add self attentions if we output attention weights</span></span><br><span class="line"></span><br><span class="line">    layer_output = apply_chunking_to_forward(</span><br><span class="line">        self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output</span><br><span class="line">    )</span><br><span class="line">    outputs = (layer_output,) + outputs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if decoder, return the attn key/values as the last output</span></span><br><span class="line">    <span class="keyword">if</span> self.is_decoder:</span><br><span class="line">        outputs = outputs + (present_key_value,)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<p>组装起来</p>
<h5 id="·-class-BertEncoder-nn-Module-层结构"><a href="#·-class-BertEncoder-nn-Module-层结构" class="headerlink" title="· class BertEncoder(nn.Module)层结构"></a>· class BertEncoder(nn.Module)层结构</h5><h6 id="1-init-4"><a href="#1-init-4" class="headerlink" title="1) init"></a>1) init</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.config = config</span><br><span class="line">    self.layer = nn.ModuleList([BertLayer(config) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(config.num_hidden_layers)])</span><br><span class="line">    self.gradient_checkpointing = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p>在这里通过<code>config.num_hidden_layers</code>指定了这个<code>BertLayer</code>结构的层数，进一步详细查看<code>BertLayer</code>层的代码，应该对应的就是Transformer架构中如图所示的N×这个部分</p>
<img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712093703201.png" alt="image-20220712093703201" style="zoom:50%;">



<h6 id="2-forward-4"><a href="#2-forward-4" class="headerlink" title="2) forward"></a>2) forward</h6><p>主要是把N个Layer串接起来forward，返回值封装了一个类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/modeling_outputs.py</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaseModelOutputWithPastAndCrossAttentions</span>(<span class="title class_ inherited__">ModelOutput</span>):</span><br></pre></td></tr></table></figure>

<h5 id="·-class-BertPooler："><a href="#·-class-BertPooler：" class="headerlink" title="· class BertPooler："></a>· class BertPooler：</h5><p>这个主要是针对[CLS]token又过了一个pooler</p>
<p>禁用的话：bertmodel初始化有一个配置add_pooling_layer默认为True，改成false就行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertPooler</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.activation = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># We &quot;pool&quot; the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token.</span></span><br><span class="line">        first_token_tensor = hidden_states[:, <span class="number">0</span>]</span><br><span class="line">        pooled_output = self.dense(first_token_tensor)</span><br><span class="line">        pooled_output = self.activation(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> pooled_output</span><br></pre></td></tr></table></figure>

<p><font color="red">TODO：这里还有些内容不是很明白，待和zkh讨论，比如说为什么叫pool，然后[CLS]这个token为什么要做这些的操作</font> </p>
<h5 id="·-class-BertModel-各层组合在一起整体的说明"><a href="#·-class-BertModel-各层组合在一起整体的说明" class="headerlink" title="· class BertModel(): 各层组合在一起整体的说明"></a>· class BertModel(): 各层组合在一起整体的说明</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BertModel</span>(<span class="title class_ inherited__">BertPreTrainedModel</span>):</span><br><span class="line">embedding_output = self.embeddings(</span><br><span class="line">    input_ids=input_ids,</span><br><span class="line">    position_ids=position_ids,</span><br><span class="line">    token_type_ids=token_type_ids,</span><br><span class="line">    inputs_embeds=inputs_embeds,</span><br><span class="line">    past_key_values_length=past_key_values_length,</span><br><span class="line">)</span><br><span class="line">encoder_outputs = self.encoder(</span><br><span class="line">    embedding_output,</span><br><span class="line">    attention_mask=extended_attention_mask,</span><br><span class="line">    head_mask=head_mask,</span><br><span class="line">    encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">    encoder_attention_mask=encoder_extended_attention_mask,</span><br><span class="line">    past_key_values=past_key_values,</span><br><span class="line">    use_cache=use_cache,</span><br><span class="line">    output_attentions=output_attentions,</span><br><span class="line">    output_hidden_states=output_hidden_states,</span><br><span class="line">    return_dict=return_dict,</span><br><span class="line">)</span><br><span class="line">sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line">pooled_output = self.pooler(sequence_output) <span class="keyword">if</span> self.pooler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p>重点代码感觉在这个部分，其他部分在制作一些mask类的地方</p>
<h4 id="2-2-2-model-state-dict"><a href="#2-2-2-model-state-dict" class="headerlink" title="2.2.2 model.state_dict()"></a>2.2.2 model.state_dict()</h4><h5 id="·-加载预训练model"><a href="#·-加载预训练model" class="headerlink" title="· 加载预训练model"></a>· 加载预训练model</h5><p><font color="red">这里加载的时候应该是用到了config.json文件和pytorch_model.bin这两个文件，而vocab.txt应该是tokenizer.from_pretrained()时候用到的，这里详细看一下config.json文件和pytorch_model.bin这两个文件是怎么被用到的</font> </p>
<p>在加载模型后，可以通过打印<code>model.state_dict()</code>调试看到模型的各个参数，这里因为是from_pretrained的，所以已经加载了pytorch_model.bin文件中的内容，而且每次加载出来的结果也都是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预训练版本</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel, BertConfig</span><br><span class="line">config = BertConfig()</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&quot;./bert_base_uncased&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711141124709.png" alt="image-20220711141124709"></p>
<p>加载model，也就是<code>BertModel.from_pretrained(pretrained_model_name_or_path)</code>对应的函数在如下路径，<strong>这个地方只要是bert的模型结构，不管是bert-base还是bert-large</strong>是都可以通过这里加载的，主要就是读取对应的<strong>config.json文件和pytorch_model.bin这两个文件</strong>：</p>
<blockquote>
<p>/Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/modeling_utils.py</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">from_pretrained</span>(<span class="params">cls, pretrained_model_name_or_path: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, os.PathLike]], *model_args, **kwargs</span>):</span><br></pre></td></tr></table></figure>

<h6 id="1-注解说明"><a href="#1-注解说明" class="headerlink" title="1) 注解说明"></a>1) 注解说明</h6><ul>
<li><p>从预训练的模型配置中实例化预训练的pytorch模型，该模型默认使用 <code>model.eval()</code>设置为评估模式；</p>
<p><font color="red">和同学讨论后补充：model.eval()一般涉及到dropout层与normalization层；；；在BERT和这种NLP领域下，因为BN不怎么用，所以LN实际上只是单个样本内部在seq_len这个维度上做norm，就不涉及到eval这块了，也就是说在NLP任务的eval这里可能只影响到dropout层；</font> </p>
<p><font color="red">再补充一些BN上的细节，BN在做训练的时候，均值和方差来自于这一组batch的计算，在inference的时候，使用<strong>全局</strong>的均值和方差，这个全局的均值和方差由之前的每个mini-batch记录而来。</font> </p>
<p><font color="red">设是[batch, seq_len, hidden]，那么BN会计算出来一个[1, seq_len, hidden]的均值；；；LN就会计算出来一个[batch, 1, hidden]的均值，然后怎么怎么处理</font> </p>
</li>
<li><p>输出的警告<code>Weights from XXX not initialized from pretrained model</code>表示XXX部分的权重没有出现，将使用模型其余部分进行训练，可以通过下游任务来微调这些权重：</p>
<p>如果把config文件的层数增加，比如从12层增加到14层的hidden layer结构，可以触发这个Warning</p>
<blockquote>
<p>Some weights of BertModel were not initialized from the model checkpoint at ./bert_base_uncased and are newly initialized: [‘bert.encoder.layer.13.attention.output.dense.weight’, ‘bert.encoder.layer.12.intermediate.dense.bias’, ‘bert.encoder.layer.13.attention.self.key.weight’, ‘bert.encoder.layer.13.attention.output.dense.bias’, ‘bert.encoder.layer.13.attention.self.value.weight’, ‘bert.encoder.layer.12.attention.self.query.weight’, ‘bert.encoder.layer.13.attention.self.value.bias’, ‘bert.encoder.layer.12.attention.self.value.bias’, ‘bert.encoder.layer.12.attention.output.LayerNorm.weight’, ‘bert.encoder.layer.13.output.dense.bias’, ‘bert.encoder.layer.13.intermediate.dense.bias’, ‘bert.encoder.layer.13.output.LayerNorm.bias’, ‘bert.encoder.layer.13.output.dense.weight’, ‘bert.encoder.layer.12.attention.self.value.weight’, ‘bert.encoder.layer.12.attention.self.query.bias’, ‘bert.encoder.layer.13.output.LayerNorm.weight’, ‘bert.encoder.layer.12.output.LayerNorm.weight’, ‘bert.encoder.layer.13.attention.self.query.bias’, ‘bert.encoder.layer.13.attention.self.query.weight’, ‘bert.encoder.layer.12.attention.self.key.weight’, ‘bert.encoder.layer.13.attention.output.LayerNorm.weight’, ‘bert.encoder.layer.12.attention.output.dense.bias’, ‘bert.encoder.layer.12.attention.self.key.bias’, ‘bert.encoder.layer.12.output.dense.weight’, ‘bert.encoder.layer.12.attention.output.LayerNorm.bias’, ‘bert.encoder.layer.13.intermediate.dense.weight’, ‘bert.encoder.layer.12.output.LayerNorm.bias’, ‘bert.encoder.layer.13.attention.self.key.bias’, ‘bert.encoder.layer.12.intermediate.dense.weight’, ‘bert.encoder.layer.13.attention.output.LayerNorm.bias’, ‘bert.encoder.layer.12.output.dense.bias’, ‘bert.encoder.layer.12.attention.output.dense.weight’]</p>
</blockquote>
</li>
<li><p>输出的警告<code>Weights from XXX not used in YYY</code>表示预训练文件中的层XXX不被YYY使用，因此那些权重将被丢弃；</p>
<p>如果把config文件的层数减少，比如从12层减小到10层的hidden layer结构，可以触发这个Warning</p>
<blockquote>
<p>Some weights of the model checkpoint at ./bert_base_uncased were not used when initializing BertModel: [‘bert.encoder.layer.10.intermediate.dense.weight’, ‘cls.predictions.decoder.weight’, ‘cls.predictions.transform.dense.bias’, ‘bert.encoder.layer.11.attention.self.value.bias’, ‘bert.encoder.layer.11.attention.output.dense.bias’, ‘bert.encoder.layer.10.output.dense.bias’, ‘bert.encoder.layer.10.attention.self.key.bias’, ‘bert.encoder.layer.10.attention.output.LayerNorm.bias’, ‘bert.encoder.layer.10.attention.self.value.weight’, ‘bert.encoder.layer.11.attention.self.key.bias’, ‘bert.encoder.layer.11.output.LayerNorm.weight’, ‘bert.encoder.layer.10.output.LayerNorm.bias’, ‘bert.encoder.layer.11.output.dense.bias’, ‘cls.predictions.transform.LayerNorm.weight’, ‘bert.encoder.layer.10.attention.output.dense.bias’, ‘cls.seq_relationship.bias’, ‘bert.encoder.layer.10.attention.self.value.bias’, ‘bert.encoder.layer.10.attention.output.dense.weight’, ‘cls.predictions.bias’, ‘bert.encoder.layer.10.attention.self.query.weight’, ‘bert.encoder.layer.11.attention.self.query.bias’, ‘cls.predictions.transform.LayerNorm.bias’, ‘bert.encoder.layer.11.attention.output.LayerNorm.bias’, ‘bert.encoder.layer.10.attention.self.query.bias’, ‘cls.predictions.transform.dense.weight’, ‘bert.encoder.layer.10.attention.output.LayerNorm.weight’, ‘bert.encoder.layer.10.output.dense.weight’, ‘bert.encoder.layer.11.attention.self.key.weight’, ‘bert.encoder.layer.11.attention.self.query.weight’, ‘cls.seq_relationship.weight’, ‘bert.encoder.layer.11.attention.self.value.weight’, ‘bert.encoder.layer.11.intermediate.dense.weight’, ‘bert.encoder.layer.10.output.LayerNorm.weight’, ‘bert.encoder.layer.11.attention.output.dense.weight’, ‘bert.encoder.layer.10.intermediate.dense.bias’, ‘bert.encoder.layer.11.output.dense.weight’, ‘bert.encoder.layer.11.intermediate.dense.bias’, ‘bert.encoder.layer.11.output.LayerNorm.bias’, ‘bert.encoder.layer.10.attention.self.key.weight’, ‘bert.encoder.layer.11.attention.output.LayerNorm.weight’]</p>
</blockquote>
<p>这里额外输出了几个<code>cls.xxx</code>，就是说没有使用这些检查点的权重，从一些解释来看这些内容应该是要被下游分类器用到的，这些内容将被初始化重新训练。目前代码里只是直接简单应用了这个的输出，而没有针对下游任务fine-tune那些的过程；</p>
</li>
</ul>
<h6 id="2-参数说明"><a href="#2-参数说明" class="headerlink" title="2) 参数说明"></a>2) 参数说明</h6><ul>
<li><p><code> pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*)</code></p>
<p>1）可以是一个字符串代表<code>model id</code>，这个model id可以从huggingface.co上获取，比如直接使用<code>bert-base-uncased</code>，或者使用带有用户名称的这个model id例如<code>hfl/chinese-macbert-base</code>，这种使用方法下可能会从huggingface那边完成下载；</p>
<p>2）可以是一个包含有pytorch_model.bin和config.json文件的路径，例如<code>./bert_base_uncased/</code>，注意这个目录下的内容需要通过<code>PreTrainedModel.save_pretrained</code>方法来得到，否则保存出来的文件可能和transformer（huggingface这一套）不太配合；</p>
<p>3）其余用法不太常见或者一般不使用，好像可以从tensorflow和flax的checkpoint进行加载，如果设置为None的话就是通过其他办法已经把config和state_dict给加载进去了；</p>
</li>
<li><p><code>output_attentions</code></p>
<p>用法：<code>model = BertModel.from_pretrained(&quot;./bert_base_uncased&quot;)</code></p>
<p>这是一个可能相对再常用一点的参数，模型输出的output包含了一个<code>output[&#39;attentions&#39;]</code>的参数输出，在调试的时候发现他是一个长度为12的tuple（这里的长度12是bert的层数），tuple中每个位置上是 <code>shape[1,12, seq_len, seq_len]</code>（这里的长度12应该是multi-head的头数目），output_attentions应该是 <code>softmax((query · key)/sqrt(d_k))</code>的结果；注意<code>shape[1,12, seq_len, seq_len]</code>这个地方，softmax应该是在-1dim上做的，代表<strong>query中的每个位置处，对于每一个key的attention score</strong>，所以来做求和的话，应该能得到一个1的结果；</p>
<p><font color="red">在后面看forward代码的时候，还要回来看一下这个地方</font> </p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711172228802.png" alt="image-20220711172228802"></p>
</li>
<li><p><code>hidden_states</code></p>
<p>用法：<code>model = BertModel.from_pretrained(&quot;./bert_base_uncased&quot;, output_hidden_states=True)</code></p>
<p>这是中间层（隐层）tensor的output输出，<font color="red">和output_attentions一样，这些内容既可以在from_pretrained中给带过去，<strong>也可以直接写在config.json</strong>文件里</font></p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711173534108.png" alt="image-20220711173534108"></p>
</li>
<li><p>上面可能是一些相对常用的参数，暂时理解来说在<code>.from_pretrained(&quot;./bert_base_uncased&quot;)</code>这个方法中带的其他一些参数可以和config加参数起到同样的效果，也就证明这个方法用到了config.json这个文件</p>
</li>
</ul>
<h6 id="3-内部流程说明（※重点）"><a href="#3-内部流程说明（※重点）" class="headerlink" title="3) 内部流程说明（※重点）"></a>3) 内部流程说明（※重点）</h6><p>内部这个地方还是写的比较详细的，像各种Exception也都实现的非常完整，大概理解一下其中的重点部分，主要目标就是加载config.json和pytorch_model.bin两个文件。</p>
<p><font color="red"><strong>config.json和pytorch_model.bin应该只有model这边用到，tokenizer那边只用到vocab.txt；；从model.from_pretrained接收参数是一个路径，而tokenizer.from_pretrained接收参数是一个vocab.txt文件的路径或者上级路径感觉也能证明这一点</strong></font> </p>
<ul>
<li><p>首先加载config.json</p>
<p>在下面这段代码中，config_path加载到了pretrained_model_name_or_path中的内容，也就是<code>&quot;./bert_base_uncased&quot;</code>，向下层<code>cls.config_class.from_pretrained</code>传递</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/modeling_utils.py</span></span><br><span class="line"><span class="comment"># Load config if we don&#x27;t provide a configuration</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(config, PretrainedConfig):</span><br><span class="line">    config_path = config <span class="keyword">if</span> config <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> pretrained_model_name_or_path</span><br><span class="line">    config, model_kwargs = cls.config_class.from_pretrained(</span><br><span class="line">        config_path,</span><br><span class="line">        cache_dir=cache_dir,</span><br><span class="line">        return_unused_kwargs=<span class="literal">True</span>,</span><br><span class="line">        force_download=force_download,</span><br><span class="line">        resume_download=resume_download,</span><br><span class="line">        proxies=proxies,</span><br><span class="line">        local_files_only=local_files_only,</span><br><span class="line">        use_auth_token=use_auth_token,</span><br><span class="line">        revision=revision,</span><br><span class="line">        _from_auto=from_auto_class,</span><br><span class="line">        _from_pipeline=from_pipeline,</span><br><span class="line">        **kwargs,</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model_kwargs = kwargs</span><br></pre></td></tr></table></figure>

<p>这里debug调试了一下<code>cls.config_class</code>：<code>&lt;class &#39;transformers.models.bert.configuration_bert.BertConfig&#39;&gt;</code>，于是在去看<code>BertConfig</code>这块的<code>.from_pretrained</code>，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/configuration_utils.py</span></span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">from_pretrained</span>(<span class="params">cls, pretrained_model_name_or_path: <span class="type">Union</span>[<span class="built_in">str</span>, os.PathLike], **kwargs</span>) -&gt; <span class="string">&quot;PretrainedConfig&quot;</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    一些注释，为了放在md里暂时删除了</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;model_type&quot;</span> <span class="keyword">in</span> config_dict <span class="keyword">and</span> <span class="built_in">hasattr</span>(cls, <span class="string">&quot;model_type&quot;</span>) <span class="keyword">and</span> config_dict[<span class="string">&quot;model_type&quot;</span>] != cls.model_type:</span><br><span class="line">        logger.warning(</span><br><span class="line">            <span class="string">f&quot;You are using a model of type <span class="subst">&#123;config_dict[<span class="string">&#x27;model_type&#x27;</span>]&#125;</span> to instantiate a model of type &quot;</span></span><br><span class="line">            <span class="string">f&quot;<span class="subst">&#123;cls.model_type&#125;</span>. This is not supported for all configurations of models and can yield errors.&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cls.from_dict(config_dict, **kwargs)</span><br></pre></td></tr></table></figure>

<p>在往下看，调用了<code>cls.get_config_dict</code>这个函数，最后一路往下找，直到找到这里，加载json文件，返回一个dict对象，在上面那段代码里最后return了一个<code>cls.from_dict</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/configuration_utils.py</span></span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_dict_from_json_file</span>(<span class="params">cls, json_file: <span class="type">Union</span>[<span class="built_in">str</span>, os.PathLike]</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(json_file, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">        text = reader.read()</span><br><span class="line">    <span class="keyword">return</span> json.loads(text)</span><br></pre></td></tr></table></figure>

<p>看了一下<code>cls.from_dict</code>，应该是这里最终返回了一个BertConfig类的对象，<font color="red">这里字典前面加两个*号是将字典解开成为独立的元素作为形参</font> </p>
<img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711232228890.png" alt="image-20220711232228890" style="zoom:50%;">
</li>
<li><p>其次加载pytorch_model.bin文件</p>
<p>通过在<code>config_path</code>目录下寻找文件，命中了<code>pytorch_model.bin</code>这个pytorch的checkpoint文件</p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711224750910.png" alt="image-20220711224750910"></p>
<p>找到这个文件后，这里做了一个和cache判断的操作，这个和huggingface这里实现可以到远程下载有关，如果过了这个函数后还是本地的路径，那就说明是用的本地的文件实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/modeling_utils.py</span></span><br><span class="line"><span class="comment"># Load from URL or cache if already cached</span></span><br><span class="line">resolved_archive_file = cached_path(</span><br><span class="line">    archive_file,</span><br><span class="line">    cache_dir=cache_dir,</span><br><span class="line">    force_download=force_download,</span><br><span class="line">    proxies=proxies,</span><br><span class="line">    resume_download=resume_download,</span><br><span class="line">    local_files_only=local_files_only,</span><br><span class="line">    use_auth_token=use_auth_token,</span><br><span class="line">    user_agent=user_agent,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>因为是pytorch形式的checkpoint，在这里<code>load_state_dict()</code></p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711233904281.png" alt="image-20220711233904281"></p>
<p>把<code>state_dict</code>传入这里，进一步进行处理，这里返回就会有<code>missing unexpect</code>这些</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/modeling_utils.py</span></span><br><span class="line"><span class="keyword">elif</span> from_pt:</span><br><span class="line">    <span class="keyword">if</span> low_cpu_mem_usage:</span><br><span class="line">        cls._load_pretrained_model_low_mem(model, loaded_state_dict_keys, resolved_archive_file)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(</span><br><span class="line">            model,</span><br><span class="line">            state_dict,</span><br><span class="line">            resolved_archive_file,</span><br><span class="line">            pretrained_model_name_or_path,</span><br><span class="line">            ignore_mismatched_sizes=ignore_mismatched_sizes,</span><br><span class="line">            sharded_metadata=sharded_metadata,</span><br><span class="line">            _fast_init=_fast_init,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p>在如下函数中完成比对操作，<strong>这里一些的输出错误经过<code>state_dict</code>的比对而发现，也就对应了“2.2.2节中，加载预训练model中第一部分，作者在开头给出的注解说明”</strong>，至此这两个文件</p>
<p><font color="blue"></font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/modeling_utils.py</span></span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_load_pretrained_model</span>(<span class="params"></span></span><br><span class="line"><span class="params">    cls,</span></span><br><span class="line"><span class="params">    model,</span></span><br><span class="line"><span class="params">    state_dict,</span></span><br><span class="line"><span class="params">    resolved_archive_file,</span></span><br><span class="line"><span class="params">    pretrained_model_name_or_path,</span></span><br><span class="line"><span class="params">    ignore_mismatched_sizes=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    sharded_metadata=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    _fast_init=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="comment"># Retrieve missing &amp; unexpected_keys</span></span><br><span class="line">    model_state_dict = model.state_dict()</span><br><span class="line">    expected_keys = <span class="built_in">list</span>(model_state_dict.keys())</span><br><span class="line">    loaded_keys = <span class="built_in">list</span>(state_dict.keys()) <span class="keyword">if</span> state_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> sharded_metadata[<span class="string">&quot;all_checkpoint_keys&quot;</span>]</span><br><span class="line">    prefix = model.base_model_prefix</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_fix_key</span>(<span class="params">key</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;beta&quot;</span> <span class="keyword">in</span> key:</span><br><span class="line">            <span class="keyword">return</span> key.replace(<span class="string">&quot;beta&quot;</span>, <span class="string">&quot;bias&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;gamma&quot;</span> <span class="keyword">in</span> key:</span><br><span class="line">            <span class="keyword">return</span> key.replace(<span class="string">&quot;gamma&quot;</span>, <span class="string">&quot;weight&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> key</span><br><span class="line"></span><br><span class="line">    loaded_keys = [_fix_key(key) <span class="keyword">for</span> key <span class="keyword">in</span> loaded_keys]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(prefix) &gt; <span class="number">0</span>:</span><br><span class="line">        has_prefix_module = <span class="built_in">any</span>(s.startswith(prefix) <span class="keyword">for</span> s <span class="keyword">in</span> loaded_keys)</span><br><span class="line">        expects_prefix_module = <span class="built_in">any</span>(s.startswith(prefix) <span class="keyword">for</span> s <span class="keyword">in</span> expected_keys)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        has_prefix_module = <span class="literal">False</span></span><br><span class="line">        expects_prefix_module = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># key re-naming operations are never done on the keys</span></span><br><span class="line">    <span class="comment"># that are loaded, but always on the keys of the newly initialized model</span></span><br><span class="line">    remove_prefix_from_model = <span class="keyword">not</span> has_prefix_module <span class="keyword">and</span> expects_prefix_module</span><br><span class="line">    add_prefix_to_model = has_prefix_module <span class="keyword">and</span> <span class="keyword">not</span> expects_prefix_module</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> remove_prefix_from_model:</span><br><span class="line">        expected_keys_not_prefixed = [s <span class="keyword">for</span> s <span class="keyword">in</span> expected_keys <span class="keyword">if</span> <span class="keyword">not</span> s.startswith(prefix)]</span><br><span class="line">        expected_keys = [<span class="string">&quot;.&quot;</span>.join(s.split(<span class="string">&quot;.&quot;</span>)[<span class="number">1</span>:]) <span class="keyword">if</span> s.startswith(prefix) <span class="keyword">else</span> s <span class="keyword">for</span> s <span class="keyword">in</span> expected_keys]</span><br><span class="line">    <span class="keyword">elif</span> add_prefix_to_model:</span><br><span class="line">        expected_keys = [<span class="string">&quot;.&quot;</span>.join([prefix, s]) <span class="keyword">for</span> s <span class="keyword">in</span> expected_keys]</span><br><span class="line"></span><br><span class="line">    missing_keys = <span class="built_in">list</span>(<span class="built_in">set</span>(expected_keys) - <span class="built_in">set</span>(loaded_keys))</span><br><span class="line">    unexpected_keys = <span class="built_in">list</span>(<span class="built_in">set</span>(loaded_keys) - <span class="built_in">set</span>(expected_keys))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Some models may have keys that are not in the state by design, removing them before needlessly warning</span></span><br><span class="line">    <span class="comment"># the user.</span></span><br><span class="line">    <span class="keyword">if</span> cls._keys_to_ignore_on_load_missing <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> pat <span class="keyword">in</span> cls._keys_to_ignore_on_load_missing:</span><br><span class="line">            missing_keys = [k <span class="keyword">for</span> k <span class="keyword">in</span> missing_keys <span class="keyword">if</span> re.search(pat, k) <span class="keyword">is</span> <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cls._keys_to_ignore_on_load_unexpected <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> pat <span class="keyword">in</span> cls._keys_to_ignore_on_load_unexpected:</span><br><span class="line">            unexpected_keys = [k <span class="keyword">for</span> k <span class="keyword">in</span> unexpected_keys <span class="keyword">if</span> re.search(pat, k) <span class="keyword">is</span> <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> _fast_init:</span><br><span class="line">        <span class="comment"># retrieve unintialized modules and initialize</span></span><br><span class="line">        uninitialized_modules = model.retrieve_modules_from_names(</span><br><span class="line">            missing_keys, add_prefix=add_prefix_to_model, remove_prefix=remove_prefix_from_model</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> uninitialized_modules:</span><br><span class="line">            model._init_weights(module)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make sure we are able to load base models as well as derived models (with heads)</span></span><br><span class="line">    start_prefix = <span class="string">&quot;&quot;</span></span><br><span class="line">    model_to_load = model</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(cls.base_model_prefix) &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(model, cls.base_model_prefix) <span class="keyword">and</span> has_prefix_module:</span><br><span class="line">        start_prefix = cls.base_model_prefix + <span class="string">&quot;.&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(cls.base_model_prefix) &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">hasattr</span>(model, cls.base_model_prefix) <span class="keyword">and</span> <span class="keyword">not</span> has_prefix_module:</span><br><span class="line">        model_to_load = <span class="built_in">getattr</span>(model, cls.base_model_prefix)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">any</span>(key <span class="keyword">in</span> expected_keys_not_prefixed <span class="keyword">for</span> key <span class="keyword">in</span> loaded_keys):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;The state dictionary of the model you are training to load is corrupted. Are you sure it was &quot;</span></span><br><span class="line">                <span class="string">&quot;properly saved?&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> state_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Whole checkpoint</span></span><br><span class="line">        mismatched_keys = []</span><br><span class="line">        <span class="keyword">if</span> ignore_mismatched_sizes:</span><br><span class="line">            <span class="keyword">for</span> checkpoint_key <span class="keyword">in</span> loaded_keys:</span><br><span class="line">                model_key = checkpoint_key</span><br><span class="line">                <span class="keyword">if</span> remove_prefix_from_model:</span><br><span class="line">                    <span class="comment"># The model key starts with `prefix` but `checkpoint_key` doesn&#x27;t so we add it.</span></span><br><span class="line">                    model_key = <span class="string">f&quot;<span class="subst">&#123;prefix&#125;</span>.<span class="subst">&#123;checkpoint_key&#125;</span>&quot;</span></span><br><span class="line">                <span class="keyword">elif</span> add_prefix_to_model:</span><br><span class="line">                    <span class="comment"># The model key doesn&#x27;t start with `prefix` but `checkpoint_key` does so we remove it.</span></span><br><span class="line">                    model_key = <span class="string">&quot;.&quot;</span>.join(checkpoint_key.split(<span class="string">&quot;.&quot;</span>)[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (</span><br><span class="line">                    model_key <span class="keyword">in</span> model_state_dict</span><br><span class="line">                    <span class="keyword">and</span> state_dict[checkpoint_key].shape != model_state_dict[model_key].shape</span><br><span class="line">                ):</span><br><span class="line">                    mismatched_keys.append(</span><br><span class="line">                        (checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape)</span><br><span class="line">                    )</span><br><span class="line">                    <span class="keyword">del</span> state_dict[checkpoint_key]</span><br><span class="line"></span><br><span class="line">        error_msgs = _load_state_dict_into_model(model_to_load, state_dict, start_prefix)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Sharded checkpoint</span></span><br><span class="line">        <span class="comment"># This should always be a list but, just to be sure.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(resolved_archive_file, <span class="built_in">list</span>):</span><br><span class="line">            resolved_archive_file = [resolved_archive_file]</span><br><span class="line"></span><br><span class="line">        error_msgs = []</span><br><span class="line">        <span class="keyword">for</span> shard_file <span class="keyword">in</span> resolved_archive_file:</span><br><span class="line">            state_dict = load_state_dict(shard_file)</span><br><span class="line">            <span class="comment"># Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not</span></span><br><span class="line">            <span class="comment"># matching the weights in the model.</span></span><br><span class="line">            mismatched_keys = []</span><br><span class="line">            <span class="keyword">if</span> ignore_mismatched_sizes:</span><br><span class="line">                <span class="keyword">for</span> checkpoint_key <span class="keyword">in</span> loaded_keys:</span><br><span class="line">                    model_key = checkpoint_key</span><br><span class="line">                    <span class="keyword">if</span> remove_prefix_from_model:</span><br><span class="line">                        <span class="comment"># The model key starts with `prefix` but `checkpoint_key` doesn&#x27;t so we add it.</span></span><br><span class="line">                        model_key = <span class="string">f&quot;<span class="subst">&#123;prefix&#125;</span>.<span class="subst">&#123;checkpoint_key&#125;</span>&quot;</span></span><br><span class="line">                    <span class="keyword">elif</span> add_prefix_to_model:</span><br><span class="line">                        <span class="comment"># The model key doesn&#x27;t start with `prefix` but `checkpoint_key` does so we remove it.</span></span><br><span class="line">                        model_key = <span class="string">&quot;.&quot;</span>.join(checkpoint_key.split(<span class="string">&quot;.&quot;</span>)[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (</span><br><span class="line">                        model_key <span class="keyword">in</span> model_state_dict</span><br><span class="line">                        <span class="keyword">and</span> state_dict[checkpoint_key].shape != model_state_dict[model_key].shape</span><br><span class="line">                    ):</span><br><span class="line">                        mismatched_keys.append(</span><br><span class="line">                            (checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape)</span><br><span class="line">                        )</span><br><span class="line">                        <span class="keyword">del</span> state_dict[checkpoint_key]</span><br><span class="line"></span><br><span class="line">            error_msgs += _load_state_dict_into_model(model_to_load, state_dict, start_prefix)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(error_msgs) &gt; <span class="number">0</span>:</span><br><span class="line">        error_msg = <span class="string">&quot;\n\t&quot;</span>.join(error_msgs)</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;Error(s) in loading state_dict for <span class="subst">&#123;model.__class__.__name__&#125;</span>:\n\t<span class="subst">&#123;error_msg&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(unexpected_keys) &gt; <span class="number">0</span>:</span><br><span class="line">        logger.warning(</span><br><span class="line">            <span class="string">f&quot;Some weights of the model checkpoint at <span class="subst">&#123;pretrained_model_name_or_path&#125;</span> were not used when &quot;</span></span><br><span class="line">            <span class="string">f&quot;initializing <span class="subst">&#123;model.__class__.__name__&#125;</span>: <span class="subst">&#123;unexpected_keys&#125;</span>\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;- This IS expected if you are initializing <span class="subst">&#123;model.__class__.__name__&#125;</span> from the checkpoint of a model trained on another task &quot;</span></span><br><span class="line">            <span class="string">f&quot;or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;- This IS NOT expected if you are initializing <span class="subst">&#123;model.__class__.__name__&#125;</span> from the checkpoint of a model that you expect &quot;</span></span><br><span class="line">            <span class="string">f&quot;to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).&quot;</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        logger.info(<span class="string">f&quot;All model checkpoint weights were used when initializing <span class="subst">&#123;model.__class__.__name__&#125;</span>.\n&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(missing_keys) &gt; <span class="number">0</span>:</span><br><span class="line">        logger.warning(</span><br><span class="line">            <span class="string">f&quot;Some weights of <span class="subst">&#123;model.__class__.__name__&#125;</span> were not initialized from the model checkpoint at <span class="subst">&#123;pretrained_model_name_or_path&#125;</span> &quot;</span></span><br><span class="line">            <span class="string">f&quot;and are newly initialized: <span class="subst">&#123;missing_keys&#125;</span>\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.&quot;</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">len</span>(mismatched_keys) == <span class="number">0</span>:</span><br><span class="line">        logger.info(</span><br><span class="line">            <span class="string">f&quot;All the weights of <span class="subst">&#123;model.__class__.__name__&#125;</span> were initialized from the model checkpoint at <span class="subst">&#123;pretrained_model_name_or_path&#125;</span>.\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;If your task is similar to the task the model of the checkpoint was trained on, &quot;</span></span><br><span class="line">            <span class="string">f&quot;you can already use <span class="subst">&#123;model.__class__.__name__&#125;</span> for predictions without further training.&quot;</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(mismatched_keys) &gt; <span class="number">0</span>:</span><br><span class="line">        mismatched_warning = <span class="string">&quot;\n&quot;</span>.join(</span><br><span class="line">            [</span><br><span class="line">                <span class="string">f&quot;- <span class="subst">&#123;key&#125;</span>: found shape <span class="subst">&#123;shape1&#125;</span> in the checkpoint and <span class="subst">&#123;shape2&#125;</span> in the model instantiated&quot;</span></span><br><span class="line">                <span class="keyword">for</span> key, shape1, shape2 <span class="keyword">in</span> mismatched_keys</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">        logger.warning(</span><br><span class="line">            <span class="string">f&quot;Some weights of <span class="subst">&#123;model.__class__.__name__&#125;</span> were not initialized from the model checkpoint at <span class="subst">&#123;pretrained_model_name_or_path&#125;</span> &quot;</span></span><br><span class="line">            <span class="string">f&quot;and are newly initialized because the shapes did not match:\n<span class="subst">&#123;mismatched_warning&#125;</span>\n&quot;</span></span><br><span class="line">            <span class="string">f&quot;You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model, missing_keys, unexpected_keys, mismatched_keys, error_msgs</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="4-额外补充"><a href="#4-额外补充" class="headerlink" title="4) 额外补充"></a>4) 额外补充</h6><p>应该是这里的实例化把config给model传进去了，于是model需要的key可能少于、或者多于提供给他的key（pytorch_model.bin）,这里是一个<code>super().__init__()</code>，可能是调用到<code>nn.Module</code>这个上层了，然后依据传入的config不知道怎么操作，把层数什么的网络结构给拼上了；<font color="red">另：也有可能是要加载到这个/Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py里面的BertModel类，这样BertModel类是super代表的上层？</font> </p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712001457966.png" alt="image-20220712001457966"></p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712001307673.png" alt="image-20220712001307673"></p>
<h5 id="·-通过config加载空模型并设置seed"><a href="#·-通过config加载空模型并设置seed" class="headerlink" title="· 通过config加载空模型并设置seed"></a>· 通过config加载空模型并设置seed</h5><p>如果是没有from_pretrained，而是通过<code>model = BertModel()</code>加载空模型的话，打印会看到初始化的参数，如下所示（另外注意，这种不从预训练文件中读取的话，需要加载config参数）：<br><font color="blue">BertConfig、BertModel、BertTokenizer</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 空模型加载版本</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel, BertConfig</span><br><span class="line">config = BertConfig()</span><br><span class="line">model = BertModel(config)</span><br></pre></td></tr></table></figure>

<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711141742912.png" alt="image-20220711141742912"></p>
<p>这里如果再次尝试加载空模型的时候，因为参数是随机初始化的，所以参数初始化结果可能有所不同，如下图所示</p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220711142519040.png" alt="image-20220711142519040"></p>
<p>通过set_seed进行指定，可以保证每次加载空模型时初始化的参数是一样的，set_seed的代码段如下，（实际使用上来说其实不一定需要写成这种函数的方式，直接写个几行就可以）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">setup_seed</span>(<span class="params">seed</span>):</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">setup_seed(<span class="number">42</span>)  <span class="comment"># 设置随机数种子</span></span><br></pre></td></tr></table></figure>

<p><font color="red">这个地方加载config的时候直接用到的是config = BertConfig()，也就是BertConfig类中的内容，对这里进行了一下详细的调试，目前的理解是他实现了一个BertConfig类，继承自PretrainedConfig这个大类。平常在fine-tune阶段直接用到的config.json文件应该是从这个BertConfig保存而来的；；；如果要是自己训练的话，可能可以实现一个新的<strong>XxxConfig</strong>类，然后内部把参数什么的都设置为自己想要的，比如层数减少一些什么的</font> </p>
<blockquote>
<p>/Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/models/bert/configuration_bert.py</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertConfig</span>(<span class="title class_ inherited__">PretrainedConfig</span>):</span><br><span class="line">  	...</span><br><span class="line">    model_type = <span class="string">&quot;bert&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        vocab_size=<span class="number">30522</span>,</span></span><br><span class="line"><span class="params">        hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">        num_hidden_layers=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">        num_attention_heads=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">        intermediate_size=<span class="number">3072</span>,</span></span><br><span class="line"><span class="params">        hidden_act=<span class="string">&quot;gelu&quot;</span>,</span></span><br><span class="line"><span class="params">        hidden_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        max_position_embeddings=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">        type_vocab_size=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">        initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">        layer_norm_eps=<span class="number">1e-12</span>,</span></span><br><span class="line"><span class="params">        pad_token_id=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">        position_embedding_type=<span class="string">&quot;absolute&quot;</span>,</span></span><br><span class="line"><span class="params">        use_cache=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        classifier_dropout=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        **kwargs</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(pad_token_id=pad_token_id, **kwargs)</span><br><span class="line"></span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_hidden_layers = num_hidden_layers</span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        self.hidden_act = hidden_act</span><br><span class="line">        self.intermediate_size = intermediate_size</span><br><span class="line">        self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">        self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">        self.max_position_embeddings = max_position_embeddings</span><br><span class="line">        self.type_vocab_size = type_vocab_size</span><br><span class="line">        self.initializer_range = initializer_range</span><br><span class="line">        self.layer_norm_eps = layer_norm_eps</span><br><span class="line">        self.position_embedding_type = position_embedding_type</span><br><span class="line">        self.use_cache = use_cache</span><br><span class="line">        self.classifier_dropout = classifier_dropout</span><br></pre></td></tr></table></figure>

<h5 id="·-保存模型"><a href="#·-保存模型" class="headerlink" title="· 保存模型"></a>· 保存模型</h5><p>通过如下命令可以完成一个模型的保存，这样会在目录下生成<strong>config.json</strong>、<strong>pytorch_model.bin</strong>这两个文件，结合上面的BertConfig，如果有一个自己的模型的话，就可以魔改一下那边的XxxConfig，比如减小一些层数训练什么的，下次通过from_pretrained应该就可以加载回来了；；；这里也要集合下上面那个<strong>加载预训练model</strong>来一起看；；；</p>
<p><font color="red">或者说，这套BertConfig和<code>from transformers.models.bert import modeling_bert </code>那边的bert模型是对应的，只要修改BertConfig这些参数就可以制作自己的bert了，比如可能有BertTiny，或者其他版本的，都可以通过Bert这边的pipeline来走这样一个流程</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save_pretrained(<span class="string">&quot;directory_on_my_computer&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-3-tokenizer"><a href="#2-3-tokenizer" class="headerlink" title="2.3 tokenizer"></a>2.3 tokenizer</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = BertTokenizer(&quot;./bert_base_uncased/vocab.txt&quot;)</span><br></pre></td></tr></table></figure>

<p>tokenizer的from_pretrain在这里</p>
<blockquote>
<p>/Users/curious/opt/miniconda3/envs/venv2/lib/python3.9/site-packages/transformers/tokenization_utils_base.py</p>
</blockquote>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712122136461.png" alt="image-20220712122136461"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">from_pretrained</span>(<span class="params">cls, pretrained_model_name_or_path: <span class="type">Union</span>[<span class="built_in">str</span>, os.PathLike], *init_inputs, **kwargs</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里省略了一些类似于本地没读到就远程下载的操作，绥中得到一个resolved_vocab_files文件向_from_pretrained传递</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cls._from_pretrained(</span><br><span class="line">      resolved_vocab_files,</span><br><span class="line">      pretrained_model_name_or_path,</span><br><span class="line">      init_configuration,</span><br><span class="line">      *init_inputs,</span><br><span class="line">      use_auth_token=use_auth_token,</span><br><span class="line">      cache_dir=cache_dir,</span><br><span class="line">      **kwargs,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p><font color="red">TODO：这里是不是还有些加入特殊token的操作，曾经在一些论文代码里见过</font> </p>
<p>tokenizer的作用就是把一句话按照vocab中转成一个id那个感觉，tokenizer.tokenize、tokenizer.convert_tokens_to_ids()和其反向的tokenizer_convert_ids_to_tokens比较常用；</p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712123154538.png" alt="image-20220712123154538"></p>
<h2 id="3-Transformer-amp-BERT论文阅读中的重点记录"><a href="#3-Transformer-amp-BERT论文阅读中的重点记录" class="headerlink" title="3. Transformer&amp;BERT论文阅读中的重点记录"></a>3. Transformer&amp;BERT论文阅读中的重点记录</h2><h3 id="3-1-Attention-is-all-you-need"><a href="#3-1-Attention-is-all-you-need" class="headerlink" title="3.1 Attention is all you need"></a>3.1 Attention is all you need</h3><p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712214108979.png" alt="image-20220712214108979"></p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712214133925.png" alt="image-20220712214133925"></p>
<p><img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712214114211.png" alt="image-20220712214114211"></p>
<img src="/2022/11/19/research/others/BERT%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0/image-20220712214155672.png" alt="image-20220712214155672" style="zoom:50%;">

<p>就是那个指数，如果有一个特别大的，他softmax算出来就很趋向于1了</p>
<p>这个现象会随着指数的增大而明显，比如指数是3的时候，就不明显，指数是20的时候就很明显</p>
<h3 id="3-2-BERT"><a href="#3-2-BERT" class="headerlink" title="3.2 BERT"></a>3.2 BERT</h3><p>TODO</p>

  	</div>
	  
		
	
		<div class="art-item-footer">
				
					<span class="art-item-left"><i class="icon icon-chevron-thin-left"></i>prev：<a href="/2022/11/19/mixed/hexo/hexo%E5%9B%BE%E7%89%87%E8%B7%AF%E5%BE%84%E7%9A%84%E9%85%8D%E7%BD%AE/" rel="prev"  title="hexo图片路径的配置">
						hexo图片路径的配置 
					</a></span>
				
				
					<span class="art-item-right">next：<a href="/2022/11/18/mixed/git/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9E%E6%8E%A5github%E4%B8%8Etoken%E7%9A%84%E9%85%8D%E7%BD%AE/" rel="next"  title="服务器连接github与token的配置">
						服务器连接github与token的配置
					</a><i class="icon icon-chevron-thin-right"></i></span>
				
		</div>
	
	</section>
	
</article>
<script>
	window.subData = {
		title: 'BERT源代码阅读学习',
		tools: true
	}
</script>

      </div>
      <aside class='l_side'>
        
  <section class='m_widget about'>

<div class='header'>Curious;</div>
<div class='content'>
<div class='desc'>BUPT, Computer Science and Technology, 2021-2024; BJUT, Information Security, 2017-2021</div>
</div>
</section>

  <section class='m_widget links'>
<div class='header'>Links</div>
<div class='content'>
    <ul class="entry">
    
        <li><a class="flat-box" target="_blank" href="https://github.com/yixuan004">
            <div class='name'>yixuan004</div>
        </a></li>
    
    </ul>
</div>
</section>

  <section class='m_widget categories'>
<div class='header'>Categories</div>
<div class='content'>
    
    <ul class="entry">
    
        <li><a class="flat-box" href="/categories/Crsenal/"><div class='name'>Crsenal</div><div class='badget'>16</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/"><div class='name'>LeetCode-python</div><div class='badget'>72</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/LeetCode-%E7%83%AD%E9%A2%98-HOT-100/"><div class='name'>LeetCode-热题 HOT 100</div><div class='badget'>29</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/LeetCode%E5%91%A8%E8%B5%9B/"><div class='name'>LeetCode周赛</div><div class='badget'>26</div></a></li>
    
        <li><a class="flat-box" href="/categories/LeetCode-python/LeetCode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"><div class='name'>LeetCode每日一题</div><div class='badget'>4</div></a></li>
    
        <li><a class="flat-box" href="/categories/NLP/"><div class='name'>NLP</div><div class='badget'>6</div></a></li>
    
        <li><a class="flat-box" href="/categories/docker/"><div class='name'>docker</div><div class='badget'>4</div></a></li>
    
        <li><a class="flat-box" href="/categories/%E7%AC%94%E8%AF%95%E7%BB%83%E4%B9%A0-python/"><div class='name'>笔试练习-python</div><div class='badget'>1</div></a></li>
    
    </ul>
    
</div>
</section>

  
<div class="m_widget tagcloud">
    <div class="header">Tags</div>
    <div class='content'>
        <a href="/tags/Dataset/" style="font-size: 14px; color: #808080">Dataset</a> <a href="/tags/Dialogue/" style="font-size: 14px; color: #808080">Dialogue</a> <a href="/tags/Dialogue-State-Tracking/" style="font-size: 14.75px; color: #707070">Dialogue State Tracking</a> <a href="/tags/EASY/" style="font-size: 19.63px; color: #080808">EASY</a> <a href="/tags/HARD/" style="font-size: 16.63px; color: #484848">HARD</a> <a href="/tags/LeetCode-python/" style="font-size: 14px; color: #808080">LeetCode-python</a> <a href="/tags/MEDIUM/" style="font-size: 20px; color: #000">MEDIUM</a> <a href="/tags/NLP/" style="font-size: 15.88px; color: #585858">NLP</a> <a href="/tags/TODO%E4%BC%98%E5%8C%96/" style="font-size: 14.38px; color: #787878">TODO优化</a> <a href="/tags/Transformer/" style="font-size: 14px; color: #808080">Transformer</a> <a href="/tags/git/" style="font-size: 14.75px; color: #707070">git</a> <a href="/tags/hexo/" style="font-size: 14.75px; color: #707070">hexo</a> <a href="/tags/macOS%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/" style="font-size: 16.63px; color: #484848">macOS基础操作</a> <a href="/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/" style="font-size: 14.38px; color: #787878">二分查找</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/" style="font-size: 14.75px; color: #707070">二叉搜索树</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 16.63px; color: #484848">二叉树</a> <a href="/tags/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/" style="font-size: 15.13px; color: #686868">优先队列</a> <a href="/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/" style="font-size: 16.25px; color: #505050">位运算</a> <a href="/tags/%E5%87%A0%E4%BD%95/" style="font-size: 14px; color: #808080">几何</a> <a href="/tags/%E5%88%86%E6%B2%BB/" style="font-size: 14px; color: #808080">分治</a> <a href="/tags/%E5%89%8D%E7%BC%80%E5%92%8C/" style="font-size: 15.13px; color: #686868">前缀和</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 18.13px; color: #282828">动态规划</a> <a href="/tags/%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/" style="font-size: 14px; color: #808080">双向链表</a> <a href="/tags/%E5%8F%8C%E6%8C%87%E9%92%88/" style="font-size: 15.88px; color: #585858">双指针</a> <a href="/tags/%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/" style="font-size: 14px; color: #808080">哈希函数</a> <a href="/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/" style="font-size: 18.5px; color: #202020">哈希表</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size: 15.5px; color: #606060">回溯</a> <a href="/tags/%E5%A0%86/" style="font-size: 15.13px; color: #686868">堆</a> <a href="/tags/%E5%AD%97%E5%85%B8%E6%A0%91/" style="font-size: 14.75px; color: #707070">字典树</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size: 18.88px; color: #181818">字符串</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D/" style="font-size: 14px; color: #808080">字符串匹配</a> <a href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" style="font-size: 14px; color: #808080">并查集</a> <a href="/tags/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/" style="font-size: 15.88px; color: #585858">广度优先搜索</a> <a href="/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/" style="font-size: 14px; color: #808080">归并排序</a> <a href="/tags/%E6%8E%92%E5%BA%8F/" style="font-size: 17.75px; color: #303030">排序</a> <a href="/tags/%E6%95%B0%E5%AD%A6/" style="font-size: 18.13px; color: #282828">数学</a> <a href="/tags/%E6%95%B0%E7%BB%84/" style="font-size: 19.25px; color: #101010">数组</a> <a href="/tags/%E6%95%B0%E8%AE%BA/" style="font-size: 14px; color: #808080">数论</a> <a href="/tags/%E6%9E%9A%E4%B8%BE/" style="font-size: 15.13px; color: #686868">枚举</a> <a href="/tags/%E6%A0%88/" style="font-size: 15.13px; color: #686868">栈</a> <a href="/tags/%E6%A0%91/" style="font-size: 17.75px; color: #303030">树</a> <a href="/tags/%E6%A8%A1%E6%8B%9F/" style="font-size: 17px; color: #404040">模拟</a> <a href="/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" style="font-size: 14px; color: #808080">正则表达式</a> <a href="/tags/%E6%B0%B4%E5%A1%98%E6%8A%BD%E6%A0%B7/" style="font-size: 14px; color: #808080">水塘抽样</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/" style="font-size: 17.38px; color: #383838">深度优先搜索</a> <a href="/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/" style="font-size: 15.5px; color: #606060">滑动窗口</a> <a href="/tags/%E6%BB%9A%E5%8A%A8%E5%93%88%E5%B8%8C/" style="font-size: 14px; color: #808080">滚动哈希</a> <a href="/tags/%E7%8A%B6%E6%80%81%E5%8E%8B%E7%BC%A9/" style="font-size: 14px; color: #808080">状态压缩</a> <a href="/tags/%E7%9F%A9%E9%98%B5/" style="font-size: 16.25px; color: #505050">矩阵</a> <a href="/tags/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/" style="font-size: 14px; color: #808080">组合数学</a> <a href="/tags/%E8%AE%A1%E6%95%B0/" style="font-size: 16.25px; color: #505050">计数</a> <a href="/tags/%E8%AE%B0%E5%BF%86%E5%8C%96%E6%90%9C%E7%B4%A2/" style="font-size: 14px; color: #808080">记忆化搜索</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size: 14px; color: #808080">论文笔记</a> <a href="/tags/%E8%AE%BE%E8%AE%A1/" style="font-size: 14.75px; color: #707070">设计</a> <a href="/tags/%E8%B4%AA%E5%BF%83/" style="font-size: 17.75px; color: #303030">贪心</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size: 14px; color: #808080">递归</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 15.5px; color: #606060">链表</a> <a href="/tags/%E9%9A%8F%E6%9C%BA%E5%8C%96/" style="font-size: 14px; color: #808080">随机化</a>
    </div>
</div>



      </aside>
      <script>setLoadingBarProgress(60);</script>
    </div>
  </div>
  <footer id="footer" class="clearfix">

	<div class="social-wrapper">
  	
      
        <a href="https://github.com/yixuan004" class="social github"
          target="_blank" rel="external">
          <span class="icon icon-github"></span>
        </a>
      
        <a href="https://twitter.com/kevinsfork" class="social twitter"
          target="_blank" rel="external">
          <span class="icon icon-twitter"></span>
        </a>
      
        <a href="/atom.xml" class="social rss"
          target="_blank" rel="external">
          <span class="icon icon-rss"></span>
        </a>
      
    
  </div>
  
  <div>Theme <a target="_blank" rel="noopener" href='https://github.com/stkevintan/hexo-theme-material-flow' class="codename">MaterialFlow</a> designed by <a href="http://keyin.me/" target="_blank">Kevin Tan</a>.</div>
  
</footer>


  <script>setLoadingBarProgress(80);</script>
  

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src='//cdn.bootcss.com/node-waves/0.7.5/waves.min.js'></script>
<script src="//cdn.bootcss.com/scrollReveal.js/3.3.2/scrollreveal.min.js"></script>

<script src="/js/jquery.fitvids.js"></script>

<script>
	var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
	var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
	var ALGOLIA_API_KEY = "";
	var ALGOLIA_APP_ID = "";
	var ALGOLIA_INDEX_NAME = "";
  var AZURE_SERVICE_NAME = "";
  var AZURE_INDEX_NAME = "";
  var AZURE_QUERY_KEY = "";
  var BAIDU_API_ID = "";
  var SEARCH_SERVICE = "hexo";
  var ROOT = "/"||"/";
  if(!ROOT.endsWith('/'))ROOT += '/';
</script>

<script src="/js/search.js"></script>


<script src="/js/app.js"></script>



  <script>setLoadingBarProgress(100);</script>
</body>
</html>
