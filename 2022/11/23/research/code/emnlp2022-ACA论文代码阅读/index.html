

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Curious;">
  <meta name="keywords" content="">
  
    <meta name="description" content="emnlp2022-ACA论文代码阅读">
<meta property="og:type" content="article">
<meta property="og:title" content="emnlp2022-ACA论文代码阅读">
<meta property="og:url" content="http://example.com/2022/11/23/research/code/emnlp2022-ACA%E8%AE%BA%E6%96%87%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/index.html">
<meta property="og:site_name" content="Curious;的个人划水博客">
<meta property="og:description" content="emnlp2022-ACA论文代码阅读">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20221123094216806.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20221123100057193.png">
<meta property="og:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20221123110136151.png">
<meta property="article:published_time" content="2022-11-23T01:31:41.000Z">
<meta property="article:modified_time" content="2022-11-23T12:52:44.121Z">
<meta property="article:author" content="Curious;">
<meta property="article:tag" content="code-reading">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20221123094216806.png">
  
  
  
  <title>emnlp2022-ACA论文代码阅读 - Curious;的个人划水博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Curious;的个人划水博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="emnlp2022-ACA论文代码阅读"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-11-23 09:31" pubdate>
          2022年11月23日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          31k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          255 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">emnlp2022-ACA论文代码阅读</h1>
            
            
              <div class="markdown-body">
                
                <p>emnlp2022-ACA论文代码阅读</p>
<span id="more"></span>

<h1 id="emnlp2022-ACA论文代码阅读"><a href="#emnlp2022-ACA论文代码阅读" class="headerlink" title="emnlp2022-ACA论文代码阅读"></a>emnlp2022-ACA论文代码阅读</h1><p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20221123094216806.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="1-入口引起的全局流程"><a href="#1-入口引起的全局流程" class="headerlink" title="1. 入口引起的全局流程"></a>1. 入口引起的全局流程</h2><p>以tacred数据集为例，首先，执行的脚本都在<code>/bash/tacred/</code>这个路径下，其中带<code>_aca</code>后缀的就代表使用了作者的方法，然后前缀的<code>emar/re_cre</code>代表作者这种model-agnostic方法使用的baseline；</p>
<p>以<code>emar_aca.sh</code>为例，在单卡GPU上运行，执行的是main&#x2F;emar.py这个文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">export PYTHONPATH=`pwd`<br><br>CUDA_VISIBLE_DEVICES=$1 python3 main/emar.py \<br>  --memory_size 10 \<br>  --total_round 5 \<br>  --task_name TACRED \<br>  --data_file data/data_with_marker_tacred.json \<br>  --relation_file data/id2rel_tacred.json \<br>  --num_of_train 420 \<br>  --num_of_val 140 \<br>  --num_of_test 140 \<br>  --batch_size_per_step 32 \<br>  --exp_name EMAR \<br>  --num_of_relation 40  \<br>  --cache_file data/TACRED_data.pt \<br>  --rel_per_task 4 \<br>  --aca 1 <br></code></pre></td></tr></table></figure>

<p>这里详解一下各个参数的意思</p>
<h3 id="1-1-main-x2F-emar-py"><a href="#1-1-main-x2F-emar-py" class="headerlink" title="1.1 main&#x2F;emar.py"></a>1.1 main&#x2F;emar.py</h3><p>从<code>if __name__ == &#39;__main__&#39;</code>开始看，首先使用<code>get_config()</code>函数，主要就是读取shell文件中配置的各个超参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    config = get_config()<br><br><span class="hljs-keyword">import</span> argparse<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_config</span>():<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(<span class="hljs-string">&#x27;--log_dir&#x27;</span>, default=<span class="hljs-string">&#x27;./logs&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--exp_name&#x27;</span>, default=<span class="hljs-literal">None</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--total_round&#x27;</span>, default=<span class="hljs-number">5</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--seed&#x27;</span>, default=<span class="hljs-number">2021</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--num_of_relation&#x27;</span>, default=<span class="hljs-number">80</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--max_length&#x27;</span>, default=<span class="hljs-number">256</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--rel_per_task&#x27;</span>, default=<span class="hljs-number">4</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--num_of_train&#x27;</span>, default=<span class="hljs-number">420</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--num_of_val&#x27;</span>, default=<span class="hljs-number">140</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--num_of_test&#x27;</span>, default=<span class="hljs-number">140</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--task_name&#x27;</span>, default=<span class="hljs-string">&#x27;TACRED&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--data_file&#x27;</span>, default=<span class="hljs-string">&#x27;./data/data_with_marker_tacred.json&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--cache_file&#x27;</span>, default=<span class="hljs-string">&#x27;./data/TACRED_data.pt&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--relation_file&#x27;</span>, default=<span class="hljs-string">&#x27;./data/id2rel_tacred.json&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--model_name&#x27;</span>, default=<span class="hljs-string">&#x27;FEA&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--bert_path&#x27;</span>, default=<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--max_grad_norm&#x27;</span>, default=<span class="hljs-number">10</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--device&#x27;</span>, default=<span class="hljs-string">&#x27;cuda&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--vocab_size&#x27;</span>, default=<span class="hljs-number">30522</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--memory_size&#x27;</span>, default=<span class="hljs-number">10</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--step1_epochs&#x27;</span>, default=<span class="hljs-number">10</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--step2_epochs&#x27;</span>, default=<span class="hljs-number">10</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--step3_epochs&#x27;</span>, default=<span class="hljs-number">10</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--batch_size_per_step&#x27;</span>, default=<span class="hljs-number">32</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--drop_out&#x27;</span>, default=<span class="hljs-number">0.5</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">float</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--pattern&#x27;</span>, default=<span class="hljs-string">&#x27;entity_marker&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--key_size&#x27;</span>, default=<span class="hljs-number">256</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--head_size&#x27;</span>, default=<span class="hljs-number">768</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--encoder_output_size&#x27;</span>, default=<span class="hljs-number">768</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--mem_size&#x27;</span>, default=<span class="hljs-number">768</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--aca&#x27;</span>, default=<span class="hljs-number">0</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)  <span class="hljs-comment"># 是否加入作者的ACA模块 </span><br>    parser.add_argument(<span class="hljs-string">&quot;--optim&quot;</span>, default=<span class="hljs-string">&#x27;adam&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)<br><br>    config = parser.parse_args()<br>    <span class="hljs-keyword">return</span> config<br></code></pre></td></tr></table></figure>

<p>因为是EMAR.py这个文件，那就首先拼接<code>config.exp_name</code>，这个之后将作为logs文件夹中记录的log</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">config.exp_name = f&#x27;EMAR&#x27;<br>if config.aca:<br>    config.exp_name += &#x27;-aca&#x27;<br>config.exp_name += f&#x27;-&#123;config.task_name&#125;-M_&#123;config.memory_size&#125;&#x27;<br></code></pre></td></tr></table></figure>

<p>如果不存在log_dir那个路径，则mkdir，这里logs是一个config中的默认路径<code>./logs</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">if not os.path.exists(config.log_dir):<br>	os.mkdir(config.log_dir)<br>log_path = os.path.join(config.log_dir, &quot;&#123;&#125;&quot;.format(config.exp_name) + &#x27;.txt&#x27;)<br></code></pre></td></tr></table></figure>

<p>如果不存在这个reps目录下的路径则make，从后续跑出来的文件来说，这里像是一个模型的存储，每次跑出来存储一个pt文件，实验是跑5次<font color="red">（不同顺序的？）</font> ,然后每个分成10个阶段的</p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20221123100057193.png" srcset="/img/loading.gif" lazyload alt="image-20221123100057193"></p>
<p>设置logging</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">Setup logging</span><br>logging.basicConfig(<br>    format=&quot;%(asctime)s - %(levelname)s - %(name)s -   %(message)s&quot;,<br>    datefmt=&quot;%m/%d/%Y %H:%M:%S&quot;,<br>    handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler(log_path)],<br>)<br>logger.setLevel(logging.INFO)<br>logger.info(config.exp_name)<br></code></pre></td></tr></table></figure>

<p>设置tokenizer，并加入作者在论文中说的speical token</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tokenizer = BertTokenizer.from_pretrained(config.bert_path, additional_special_tokens=[&quot;[E11]&quot;, &quot;[E12]&quot;, &quot;[E21]&quot;, &quot;[E22]&quot;])<br></code></pre></td></tr></table></figure>

<p>设置record列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">test_cur_record = []<br>test_total_record = []<br></code></pre></td></tr></table></figure>

<p>如果是<code>fewrel</code>数据集，要加载一个<code>data/pid2name.json</code>，fewrel数据集把每种关系编号为一个比如<code>P126</code>，然后这里有一个dict的对应</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;P126&quot;</span>: [<span class="hljs-string">&quot;maintained by&quot;</span>, <span class="hljs-string">&quot;person or organization in charge of keeping the subject (for instance an infrastructure) in functioning order&quot;</span>]<br></code></pre></td></tr></table></figure>

<p>P126，maintained by，负责保持主体（例如基础设施）正常运行的人员或组织</p>
<h4 id="1-1-1-开启针对于total-round（5次实验）的循环"><a href="#1-1-1-开启针对于total-round（5次实验）的循环" class="headerlink" title="1.1.1 开启针对于total_round（5次实验）的循环"></a>1.1.1 开启针对于total_round（5次实验）的循环</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">for i in range(config.total_round)<br></code></pre></td></tr></table></figure>

<h5 id="1-1-1-1-设置基础的记录信息和保存信息，以及seed"><a href="#1-1-1-1-设置基础的记录信息和保存信息，以及seed" class="headerlink" title="1.1.1.1 设置基础的记录信息和保存信息，以及seed"></a>1.1.1.1 设置基础的记录信息和保存信息，以及seed</h5><p>首先设置基础的记录信息和保存信息，以及seed <font color="red">这里这些都要设置吗？之前感觉只有几行</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">f&#x27;reps/<span class="hljs-subst">&#123;config.exp_name&#125;</span>/<span class="hljs-subst">&#123;i&#125;</span>&#x27;</span>):<br>    os.mkdir(<span class="hljs-string">f&#x27;reps/<span class="hljs-subst">&#123;config.exp_name&#125;</span>/<span class="hljs-subst">&#123;i&#125;</span>&#x27;</span>)<br><br>test_cur = []<br>test_total = []<br>set_seed(config.seed + i * <span class="hljs-number">100</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">set_seed</span>(<span class="hljs-params">seed</span>):<br>    torch.manual_seed(seed)<br>    torch.cuda.manual_seed_all(seed)<br>    torch.cuda.manual_seed(seed)<br>    np.random.seed(seed)<br>    random.seed(seed)<br>    torch.backends.cudnn.benchmark = <span class="hljs-literal">False</span><br>    torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure>

<h5 id="1-1-1-2-设置data的sampler（采样器），在sampler-py文件中"><a href="#1-1-1-2-设置data的sampler（采样器），在sampler-py文件中" class="headerlink" title="1.1.1.2 设置data的sampler（采样器），在sampler.py文件中"></a>1.1.1.2 设置data的sampler（采样器），在<code>sampler.py</code>文件中</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">sampler setup</span><br>sampler = data_sampler(config=config, seed=config.seed + i * 100, tokenizer=tokenizer)<br></code></pre></td></tr></table></figure>

<ul>
<li>下面是这个init的定义，初始化</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">data_sampler</span>(<span class="hljs-title class_ inherited__">object</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config=<span class="hljs-literal">None</span>, seed=<span class="hljs-literal">None</span>, tokenizer=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        这是一个类（类名没有大写hh）</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        self.config = config<br><br>        self.tokenizer = tokenizer  <span class="hljs-comment"># 加入了special token后的BertEncoder</span><br><br>        <span class="hljs-comment"># read relation data，id2rel是一个list（id索引关系名称），rel2id是一个dict（关系名称索引id）</span><br>        self.id2rel, self.rel2id = self._read_relations(config.relation_file)<br><br>        <span class="hljs-comment"># random sampling，看起来就是5次实验的时候，每次设置的这个任务的排列顺序都不一样</span><br>        self.seed = seed<br>        <span class="hljs-keyword">if</span> self.seed != <span class="hljs-literal">None</span>:<br>            random.seed(self.seed)<br>        self.shuffle_index = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(self.id2rel)))  <span class="hljs-comment"># 多少种关系</span><br>        random.shuffle(self.shuffle_index)<br>        self.shuffle_index = np.argsort(self.shuffle_index)<br><br>        <span class="hljs-comment"># regenerate data</span><br>      	<span class="hljs-comment">## 如果不存在这个cache_file（TACRED_data.pt），那么就重新生成，</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(config.cache_file):<br>            dataset = self._read_data(self.config.data_file)<br>            torch.save(dataset, config.cache_file)<br>        <span class="hljs-keyword">else</span>:<br>            dataset = torch.load(config.cache_file)<br><br>        self.training_dataset, self.valid_dataset, self.test_dataset = dataset  <span class="hljs-comment"># 取出来</span><br><br>        <span class="hljs-comment"># generate the task number</span><br>        self.batch = <span class="hljs-number">0</span><br>        self.task_length = <span class="hljs-built_in">len</span>(self.id2rel) // self.config.rel_per_task  <span class="hljs-comment"># 整除（下取整），多少种关系，每个任务学几个关系</span><br><br>        <span class="hljs-comment"># record relations，记录关系，暂时在这里初始化的时候用不到</span><br>        self.seen_relations = []<br>        self.history_test_data = &#123;&#125;<br></code></pre></td></tr></table></figure>

<ul>
<li>读取关系的时候用的是下面这个函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_read_relations</span>(<span class="hljs-params">self, file</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    :param file: input relation file</span><br><span class="hljs-string">    :return:  a list of relations, and a mapping from relations to their ids.</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    id2rel = json.load(<span class="hljs-built_in">open</span>(file, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>))<br>    rel2id = &#123;&#125;<br>    <span class="hljs-keyword">for</span> i, x <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(id2rel):<br>        rel2id[x] = i<br>    <span class="hljs-keyword">return</span> id2rel, rel2id<br></code></pre></td></tr></table></figure>

<p><code>config.relation_file</code>配置的是下面这个文件<code>data/id2rel_tacred.json</code>，一个列表</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[&quot;org:founded_by&quot;, &quot;per:employee_of&quot;, &quot;org:alternate_names&quot;, &quot;per:cities_of_residence&quot;, &quot;per:children&quot;, &quot;per:title&quot;, &quot;per:siblings&quot;, &quot;per:religion&quot;, &quot;per:age&quot;, &quot;org:website&quot;, &quot;per:stateorprovinces_of_residence&quot;, &quot;org:member_of&quot;, &quot;org:top_members/employees&quot;, &quot;per:countries_of_residence&quot;, &quot;org:city_of_headquarters&quot;, &quot;org:members&quot;, &quot;org:country_of_headquarters&quot;, &quot;per:spouse&quot;, &quot;org:stateorprovince_of_headquarters&quot;, &quot;org:number_of_employees/members&quot;, &quot;org:parents&quot;, &quot;org:subsidiaries&quot;, &quot;per:origin&quot;, &quot;org:political/religious_affiliation&quot;, &quot;per:other_family&quot;, &quot;per:stateorprovince_of_birth&quot;, &quot;org:dissolved&quot;, &quot;per:date_of_death&quot;, &quot;org:shareholders&quot;, &quot;per:alternate_names&quot;, &quot;per:parents&quot;, &quot;per:schools_attended&quot;, &quot;per:cause_of_death&quot;, &quot;per:city_of_death&quot;, &quot;per:stateorprovince_of_death&quot;, &quot;org:founded&quot;, &quot;per:country_of_birth&quot;, &quot;per:date_of_birth&quot;, &quot;per:city_of_birth&quot;, &quot;per:charges&quot;]<br></code></pre></td></tr></table></figure>

<p>原来是通过id索引关系（id2rel，是一个list），现在制作一个反向的mapping通过关系索引id（rel2id，是一个dict）</p>
<ul>
<li><code>_read_data</code>的逻辑是下面这样的</li>
</ul>
<p>加载的数据文件是<code>config.data_file</code>，也就是<code>data/data_with_marker_tacred.json</code>，这个数据格式化看起来是这样的，作者按照这个关系，比如说<code>org:founded_by</code>进行第一层级的建模，然后是已经把tacred数据集进行与处理后了（所以可能可以公开发布出来吧），这里比如<code>[E21]/[E22]</code>这些special token就和论文里面说的一样</p>
<p><img src="http://yixuan004.oss-cn-hangzhou.aliyuncs.com/img/image-20221123110136151.png" srcset="/img/loading.gif" lazyload alt="image-20221123110136151"></p>
<p><font color="red">这里因为是按照关系来的，对于CRE任务来说不会出现串了的情况</font> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_read_data</span>(<span class="hljs-params">self, file</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    :param file: the input sample file</span><br><span class="hljs-string">    :return: samples for the model: [relation label, text]</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    data = json.load(<span class="hljs-built_in">open</span>(file, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>))<br>    train_dataset = [[] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.config.num_of_relation)]  <span class="hljs-comment"># TACRED 40个</span><br>    val_dataset = [[] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.config.num_of_relation)]  <span class="hljs-comment"># TACRED 40种关系</span><br>    test_dataset = [[] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.config.num_of_relation)]<br><br>    <span class="hljs-comment"># random sample 40 samples for test, and 320 samples for train（这个是对于每个关系而来的）</span><br>    <span class="hljs-keyword">for</span> relation <span class="hljs-keyword">in</span> tqdm(data.keys(), desc=<span class="hljs-string">&quot;Load data&quot;</span>):  <span class="hljs-comment"># TACRED 40个</span><br>        rel_samples = data[relation]  <span class="hljs-comment"># 属于这个关系的所有类都取出来，</span><br>        <span class="hljs-keyword">if</span> self.seed != <span class="hljs-literal">None</span>:<br>            random.seed(self.seed)<br>        random.shuffle(rel_samples)  <span class="hljs-comment"># 随机shuffle属于这个关系的列表，很多条数据，随机打乱保证了随机性</span><br>        count = <span class="hljs-number">0</span><br>        count1 = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i, sample <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(rel_samples):<br>            tokenized_sample = &#123;&#125;<br>            tokenized_sample[<span class="hljs-string">&#x27;relation&#x27;</span>] = self.rel2id[sample[<span class="hljs-string">&#x27;relation&#x27;</span>]]  <span class="hljs-comment"># 转成之前关系-&gt;id对应的id（rel2id是dict）</span><br>            tokenized_sample[<span class="hljs-string">&#x27;tokens&#x27;</span>] = self.tokenizer.encode(<span class="hljs-string">&#x27; &#x27;</span>.join(sample[<span class="hljs-string">&#x27;tokens&#x27;</span>]))  <span class="hljs-comment"># 空格连接，然后encode</span><br>            tokenized_sample[<span class="hljs-string">&#x27;string&#x27;</span>] = <span class="hljs-string">&#x27;[RelData] &#x27;</span> + sample[<span class="hljs-string">&#x27;relation&#x27;</span>] + <span class="hljs-string">&#x27; &#x27;</span> + <span class="hljs-string">&#x27; &#x27;</span>.join(sample[<span class="hljs-string">&#x27;tokens&#x27;</span>])  <span class="hljs-comment"># 拼接出一个 [RelData] + 关系 + tokens的</span><br>           <br>            <span class="hljs-keyword">if</span> self.config.task_name == <span class="hljs-string">&#x27;FewRel&#x27;</span>:<br>                <span class="hljs-keyword">if</span> i &lt; self.config.num_of_train:<br>                    train_dataset[self.rel2id[relation]].append(tokenized_sample)<br>                <span class="hljs-keyword">elif</span> i &lt; self.config.num_of_train + self.config.num_of_val:<br>                    val_dataset[self.rel2id[relation]].append(tokenized_sample)<br>                <span class="hljs-keyword">else</span>:<br>                    test_dataset[self.rel2id[relation]].append(tokenized_sample)<br>            <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># TACRED，只分train和test，作者说的是40个用来test，320个用来train，这里就是直接强制</span><br>                <span class="hljs-keyword">if</span> i &lt; <span class="hljs-built_in">len</span>(rel_samples) // <span class="hljs-number">5</span> <span class="hljs-keyword">and</span> count &lt;= <span class="hljs-number">40</span>:  <span class="hljs-comment"># 这个&lt;=有bug，应该是&lt;才对吧，不知道这里和其他有没有对齐</span><br>                    count += <span class="hljs-number">1</span><br>                    test_dataset[self.rel2id[relation]].append(tokenized_sample)<br>                <span class="hljs-keyword">else</span>:<br>                    count1 += <span class="hljs-number">1</span><br>                    train_dataset[self.rel2id[relation]].append(tokenized_sample)<br>                    <span class="hljs-keyword">if</span> count1 &gt;= <span class="hljs-number">320</span>:<br>                        <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">return</span> train_dataset, val_dataset, test_dataset  <span class="hljs-comment"># 返回三个dataset</span><br></code></pre></td></tr></table></figure>

<p>这里制作了dataset之后，会存成一个pt文件，下次用到的时候不用再制作了，直接读取pt文件就可以</p>
<ul>
<li>重定义<code>data_sampler</code>类的<code>__iter__</code>与<code>__next__</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__iter__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-keyword">return</span> self<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__next__</span>(<span class="hljs-params">self</span>):<br><br>    <span class="hljs-keyword">if</span> self.batch == self.task_length:<br>        <span class="hljs-keyword">raise</span> StopIteration()<br>	<span class="hljs-comment"># self.config.rel_per_task是4（每个task relation 4个），每次截断这个batch区间内的4个关系（也就是分成10个task的过程？）</span><br>    <span class="hljs-comment"># 注意这里是shuffle_index，已经把relation给打乱了</span><br>    indices = self.shuffle_index[self.config.rel_per_task*self.batch: self.config.rel_per_task*(self.batch+<span class="hljs-number">1</span>)]<br>    self.batch += <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># 在每次next的时候会清空</span><br>    current_relations = []<br>    cur_training_data = &#123;&#125;<br>    cur_valid_data = &#123;&#125;<br>    cur_test_data = &#123;&#125;<br><br>    <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> indices:<br>        current_relations.append(self.id2rel[index])  <span class="hljs-comment"># 当前有哪些关系</span><br>        self.seen_relations.append(self.id2rel[index])  <span class="hljs-comment"># seen_relations是整体的，已经见过哪些了</span><br><br>        cur_training_data[self.id2rel[index]] = self.training_dataset[index]<br>        cur_valid_data[self.id2rel[index]] = self.valid_dataset[index]<br>        cur_test_data[self.id2rel[index]] = self.test_dataset[index]<br>        self.history_test_data[self.id2rel[index]] = self.test_dataset[index]  <span class="hljs-comment"># 把test给加进来</span><br><br>    <span class="hljs-keyword">return</span> cur_training_data, cur_valid_data, cur_test_data, current_relations, self.history_test_data, self.seen_relations<br></code></pre></td></tr></table></figure>

<h5 id="1-1-1-3-传出来id2rel-rel2id，然后初始化一个BertEncoder"><a href="#1-1-1-3-传出来id2rel-rel2id，然后初始化一个BertEncoder" class="headerlink" title="1.1.1.3 传出来id2rel, rel2id，然后初始化一个BertEncoder"></a>1.1.1.3 传出来id2rel, rel2id，然后初始化一个BertEncoder</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">id2rel = sampler.id2rel<br>rel2id = sampler.rel2id<br>encoder = Bert_Encoder(config=config).cuda()<br></code></pre></td></tr></table></figure>

<p>这里BertEncoder是一个整体的大结构，作者自己实现了换一下，不只是一个普通的层</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs shell">class Bert_Encoder(base_model):<br><span class="hljs-meta prompt_"># </span><span class="language-bash"> load_config.attention_probs_dropout_prob = config.monto_drop_ratio</span><br>    # load_config.hidden_dropout_prob = config.monto_drop_ratio<br>    def __init__(self, config, attention_probs_dropout_prob=None, hidden_dropout_prob=None, drop_out=None): <br>        super(Bert_Encoder, self).__init__()<br><br>        # load model，加载模型，encoder和bert_config<br>        self.encoder = BertModel.from_pretrained(config.bert_path).cuda()<br>        self.bert_config = BertConfig.from_pretrained(config.bert_path)<br><br>        # for monto kalo<br>        # 这里感觉有3个能设置的dropout比率，两个bert内部的，一个自己的dropout层<br>        if attention_probs_dropout_prob is not None:<br>            assert hidden_dropout_prob is not None and drop_out is not None<br>            self.bert_config.attention_probs_dropout_prob = attention_probs_dropout_prob<br>            self.bert_config.hidden_dropout_prob = hidden_dropout_prob<br>            config.drop_out = drop_out<br><br>        # the dimension for the final outputs，硬编码在这里了，输出是768的<br>        self.output_size = 768<br><br>        self.drop = nn.Dropout(config.drop_out)  # 自己的一个dropout层<br><br>        # find which encoding is used<br>        # 这里相当于是一个作者自己的实现，在config里面默认设置的是entity_marker这个<br><span class="hljs-meta prompt_">		# </span><span class="language-bash">entity_marker就是加载作者这个自己处理后的TACRED数据集，带着speical token的（[E11]/[E12]/[E21]/[E22]）</span><br>        if config.pattern in [&#x27;standard&#x27;, &#x27;entity_marker&#x27;]:<br>            self.pattern = config.pattern<br>        else:<br>            raise Exception(&#x27;Wrong encoding.&#x27;)<br><span class="hljs-meta prompt_">		# </span><span class="language-bash">如果是加上这些special token，就+4个，然后最后是2*hidden_size到output_size</span><br>        if self.pattern == &#x27;entity_marker&#x27;:  <br>            self.encoder.resize_token_embeddings(config.vocab_size + 4)<br>            self.linear_transform = nn.Linear(self.bert_config.hidden_size*2, self.output_size, bias=True)<br>        else:<br>            self.linear_transform = nn.Linear(self.bert_config.hidden_size, self.output_size, bias=True)<br>     	# layer norm<br>        self.layer_normalization = nn.LayerNorm([self.output_size])<br><br><br>    def get_output_size(self):<br>        return self.output_size  # 输出output_size<br><br>    def forward(self, inputs):<br>        &#x27;&#x27;&#x27;<br>        :param inputs: of dimension [B, N]  # 前向传播 [batch_size, seq_len]<br>        :return: a result of size [B, H*2] or [B, H], according to different strategy<br>        &#x27;&#x27;&#x27;<br>        # generate representation under a certain encoding strategy<br>        if self.pattern == &#x27;standard&#x27;:<br>            # in the standard mode, the representation is generated according to<br>            #  the representation of[CLS] mark.<br>            output = self.encoder(inputs)[1]<br>        else:  # 现在是走的这个逻辑，因为这个是作者的ACA逻辑，加入了special token，作者的数据已经处理好了<br>            # in the entity_marker mode, the representation is generated from the representations of<br>            #  marks [E11] and [E21] of the head and tail entities.<br>            e11 = []  # 第一个实体的start<br>            e21 = []  # 第二个实体的start<br>            # for each sample in the batch, acquire the positions of its [E11] and [E21]<br>            # 对于每个batch中的sanple<br>            for i in range(inputs.size()[0]):  # <br>                tokens = inputs[i].cpu().numpy()  # 每个sample<br>                e11.append(np.argwhere(tokens == 30522)[0][0])  # 0-30511（30522个），之后是按照顺序的加入&quot;[E11]522&quot;, &quot;[E12]523&quot;, &quot;[E21]524&quot;, &quot;[E22]525&quot;<br>                e21.append(np.argwhere(tokens == 30524)[0][0])<br><br>            # input the sample to BERT<br>            attention_mask = inputs != 0<br>            tokens_output = self.encoder(inputs, attention_mask=attention_mask)[0] # [B,N] --&gt; [B,N,H] hidden_size<br>            output = []<br><br>            # for each sample in the batch, acquire its representations for [E11] and [E21]<br>            for i in range(len(e11)):<br>                instance_output = torch.index_select(tokens_output, 0, torch.tensor(i).cuda())<br>                instance_output = torch.index_select(instance_output, 1, torch.tensor([e11[i], e21[i]]).cuda())<br>                output.append(instance_output) # [B,N] --&gt; [B,2,H] 把[E11] [E21]的embedding给拿出来<br><br>            # for each sample in the batch, concatenate the representations of [E11] and [E21], and reshape<br>            # 把这两个concate后，消掉一个维度<br>            output = torch.cat(output, dim=0)<br>            output = output.view(output.size()[0], -1) # [B,N] --&gt; [B,H*2]<br>            <br>            # the output dimension is [B, H*2], B: batchsize, H: hiddensize<br>            output = self.drop(output)<br>            output = self.linear_transform(output)<br>            output = F.gelu(output)<br>            output = self.layer_normalization(output) <br>        return output  # E11和E21的表征，用这两个表征来判断关系？<br></code></pre></td></tr></table></figure>

<h5 id="1-1-1-4-针对ACA的一些配置处理，并且建立proto-softmax-layer，设置memorized-samples字典memorized-samples-x3D"><a href="#1-1-1-4-针对ACA的一些配置处理，并且建立proto-softmax-layer，设置memorized-samples字典memorized-samples-x3D" class="headerlink" title="1.1.1.4 针对ACA的一些配置处理，并且建立proto_softmax_layer，设置memorized_samples字典memorized_samples &#x3D; {}"></a>1.1.1.4 针对ACA的一些配置处理，并且建立proto_softmax_layer，设置memorized_samples字典memorized_samples &#x3D; {}</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> config.aca:<br>    add_relation_num = config.rel_per_task * <span class="hljs-number">3</span>  <span class="hljs-comment"># 每个是4种关系，这里变成12，</span><br>    model = proto_softmax_layer(<br>    encoder, <br>    num_class = <span class="hljs-built_in">len</span>(sampler.id2rel) + add_relation_num,   <span class="hljs-comment"># 40种关系，加上12种关系？</span><br>    id2rel = sampler.id2rel, <br>    drop = <span class="hljs-number">0</span>, <br>    config = config).cuda()<br><span class="hljs-keyword">else</span>:<br>    model = proto_softmax_layer(<br>        encoder, <br>        num_class = <span class="hljs-built_in">len</span>(sampler.id2rel), <br>        id2rel = sampler.id2rel, <br>        drop = <span class="hljs-number">0</span>, <br>        config = config).cuda()<br></code></pre></td></tr></table></figure>

<ul>
<li>内部的这个模型（emar）的复现，这里大概来说一下，主要分为<code>base_model</code>类还有<code>proto_softmax_layer</code>类</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">base_model</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        0和pi的常数</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(base_model, self).__init__()<br>        self.zero_const = nn.Parameter(torch.Tensor([<span class="hljs-number">0</span>]))<br>        self.zero_const.requires_grad = <span class="hljs-literal">False</span><br>        self.pi_const = nn.Parameter(torch.Tensor([<span class="hljs-number">3.14159265358979323846</span>]))<br>        self.pi_const.requires_grad = <span class="hljs-literal">False</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_checkpoint</span>(<span class="hljs-params">self, path</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        保存checkpoint</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        torch.save(self.state_dict(), path)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_checkpoint</span>(<span class="hljs-params">self, path</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        加载checkpoint，并且设置为evaluate模式</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.load_state_dict(torch.load(os.path.join(path)))<br>        self.<span class="hljs-built_in">eval</span>()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_parameters</span>(<span class="hljs-params">self, path</span>):<br>        f = <span class="hljs-built_in">open</span>(path, <span class="hljs-string">&quot;w&quot;</span>)<br>        f.write(json.dumps(self.get_parameters(<span class="hljs-string">&quot;list&quot;</span>)))<br>        f.close()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_parameters</span>(<span class="hljs-params">self, path</span>):<br>        f = <span class="hljs-built_in">open</span>(path, <span class="hljs-string">&quot;r&quot;</span>)<br>        parameters = json.loads(f.read())<br>        f.close()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> parameters:<br>            parameters[i] = torch.Tensor(parameters[i])<br>        self.load_state_dict(parameters, strict=<span class="hljs-literal">False</span>)<br>        self.<span class="hljs-built_in">eval</span>()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_parameters</span>(<span class="hljs-params">self, mode=<span class="hljs-string">&quot;numpy&quot;</span>, param_dict=<span class="hljs-literal">None</span></span>):<br>        all_param_dict = self.state_dict()<br>        <span class="hljs-keyword">if</span> param_dict == <span class="hljs-literal">None</span>:<br>            param_dict = all_param_dict.keys()<br>        res = &#123;&#125;<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> param_dict:<br>            <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&quot;numpy&quot;</span>:<br>                res[param] = all_param_dict[param].cpu().numpy()<br>            <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&quot;list&quot;</span>:<br>                res[param] = all_param_dict[param].cpu().numpy().tolist()<br>            <span class="hljs-keyword">else</span>:<br>                res[param] = all_param_dict[param]<br>        <span class="hljs-keyword">return</span> res<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_parameters</span>(<span class="hljs-params">self, parameters</span>):<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> parameters:<br>            parameters[i] = torch.Tensor(parameters[i])<br>        self.load_state_dict(parameters, strict = <span class="hljs-literal">False</span>)<br>        self.<span class="hljs-built_in">eval</span>()<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">proto_softmax_layer</span>(<span class="hljs-title class_ inherited__">base_model</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Softmax classifier for sentence-level relation extraction.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__distance__</span>(<span class="hljs-params">self, rep, rel</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        算一个距离，点乘后求和？</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        rep_ = rep.view(rep.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>, rep.shape[-<span class="hljs-number">1</span>])<br>        rel_ = rel.view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, rel.shape[-<span class="hljs-number">1</span>])<br>        dis = (rep_ * rel_).<span class="hljs-built_in">sum</span>(-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> dis<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, sentence_encoder, num_class, id2rel, drop = <span class="hljs-number">0</span>, config = <span class="hljs-literal">None</span>, rate = <span class="hljs-number">1.0</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            sentence_encoder: encoder for sentences</span><br><span class="hljs-string">            num_class: number of classes，在emar-aca中扩充到40 + 12个类别了</span><br><span class="hljs-string">            id2rel: dictionary of id -&gt; relation name mapping</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(proto_softmax_layer, self).__init__()<br><br>        self.config = config<br>        self.sentence_encoder = sentence_encoder<br>        self.num_class = num_class<br>        self.hidden_size = self.sentence_encoder.output_size<br>        self.fc = nn.Linear(self.hidden_size, self.num_class, bias = <span class="hljs-literal">False</span>)  <span class="hljs-comment"># hidden -&gt; num_class</span><br>        self.drop = nn.Dropout(drop)<br>        self.id2rel = id2rel<br>        self.rel2id = &#123;&#125;<br>        <span class="hljs-comment"># 这里又反向搞了一回</span><br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span>, rel <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(id2rel):<br>            self.rel2id[rel] = <span class="hljs-built_in">id</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">incremental_learning</span>(<span class="hljs-params">self, old_class, add_class</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        增量学习，</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        weight = self.fc.weight.data<br>        self.fc = nn.Linear(<span class="hljs-number">768</span>, old_class + add_class, bias=<span class="hljs-literal">False</span>).cuda()<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            self.fc.weight.data[:old_class] = weight[:old_class]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_memorized_prototypes</span>(<span class="hljs-params">self, protos</span>):<br>        self.prototypes = protos.detach().cuda()<br>   <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_feature</span>(<span class="hljs-params">self, sentences</span>):<br>        rep = self.sentence_encoder(sentences)<br>        <span class="hljs-keyword">return</span> rep.cpu().data.numpy()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_mem_feature</span>(<span class="hljs-params">self, rep</span>):<br>        dis = self.mem_forward(rep)<br>        <span class="hljs-keyword">return</span> dis<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sentences</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            args: depends on the encoder</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            logits, (B, N)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        rep = self.sentence_encoder(sentences) <span class="hljs-comment"># (B, H) 只输出那个E11 E21加权后的</span><br>        rep = self.drop(rep)  <span class="hljs-comment"># 这里又drop一次？之前sentence_encoder感觉在最后drop了一下了</span><br>        logits = self.fc(rep)  <span class="hljs-comment"># 全连接</span><br>        <span class="hljs-keyword">return</span> logits, rep<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mem_forward</span>(<span class="hljs-params">self, rep</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            args: depends on the encoder</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            logits, (B, N)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        dis_mem = self.__distance__(rep, self.prototypes)<br>        <span class="hljs-keyword">return</span> dis_mem<br></code></pre></td></tr></table></figure>

<p><font color="red">这里的add_relation_num实在是不太理解，为什么会是12呢</font> </p>
<p><font color="red">对于fewrel应该是12种，（因为每次8个，8+4），那tacred会不会这里有bug？</font> </p>
<h4 id="1-1-2-开启对于sampler的循环（10个Task）"><a href="#1-1-2-开启对于sampler的循环（10个Task）" class="headerlink" title="1.1.2 开启对于sampler的循环（10个Task）"></a>1.1.2 开启对于sampler的循环（10个Task）</h4><p>这里sampler之前已经处理好了，定义了是4个4个的这样的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> episode, (training_data, valid_data, test_data, current_relations, historic_test_data, seen_relations) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sampler):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    training/valid/test_data：针对当前这些关系的训练/验证/测试集合</span><br><span class="hljs-string">    current_relations：当前的关系</span><br><span class="hljs-string">    historic_test_data：随着episode不断的把test给加进来</span><br><span class="hljs-string">    seen_relations：见过的关系</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    注意，当前的算作见过和test中</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-built_in">print</span>(current_relations)  <span class="hljs-comment"># 本次的关系</span><br>  	<br><br>    train_data_for_initial = []  <span class="hljs-comment"># 每次初始化一个新的</span><br>    <span class="hljs-keyword">for</span> relation <span class="hljs-keyword">in</span> current_relations:<br>        train_data_for_initial += training_data[relation]  <span class="hljs-comment"># 那边的是一个dict，把本次的关系给加进来</span><br>    <span class="hljs-comment"># Step1-2: Initialize: Just Random Initialize the as other model</span><br>    <br>    <span class="hljs-comment"># 这个是作者论文的处理部分，如果是aca的，调用这个get_aca_data方法</span><br>    <span class="hljs-keyword">if</span> config.aca:<br>        <span class="hljs-comment"># 获得数据，并且多出rel_per_task + rel_per_task//2种关系</span><br>        add_aca_data = get_aca_data(config, deepcopy(training_data), current_relations, tokenizer)<br><br>    <span class="hljs-comment"># Step3 -&gt; Step5</span><br>    <span class="hljs-comment"># 预训练了一手？</span><br>    <span class="hljs-keyword">if</span> config.aca:<br>        <span class="hljs-comment"># 数据使用model训练两个epoch，把新增数据混进去</span><br>        logger.info(<span class="hljs-string">f&#x27;data num for step 1: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(train_data_for_initial + add_aca_data)&#125;</span>&#x27;</span>)<br>        model = train_simple_model(config, model, train_data_for_initial + add_aca_data, <span class="hljs-number">2</span>) <br>    <span class="hljs-keyword">else</span>:<br>        logger.info(<span class="hljs-string">f&#x27;data num for step 1: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(train_data_for_initial)&#125;</span>&#x27;</span>)<br>        model = train_simple_model(config, model, train_data_for_initial, <span class="hljs-number">2</span>) <br>        <br>    <span class="hljs-keyword">if</span> config.aca:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        def incremental_learning(self, old_class, add_class):</span><br><span class="hljs-string">            weight = self.fc.weight.data</span><br><span class="hljs-string">            self.fc = nn.Linear(768, old_class + add_class, bias=False).cuda()</span><br><span class="hljs-string">            with torch.no_grad():</span><br><span class="hljs-string">                self.fc.weight.data[:old_class] = weight[:old_class]  # 把之前的复制进来构建这个层增量？</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        logger.info(<span class="hljs-string">&#x27;remove aca node&#x27;</span>)<br>        model.incremental_learning(config.num_of_relation, add_relation_num)  <span class="hljs-comment"># 不理解为什么add_relation_num是config.rel_per_task * 3</span><br><br>    <span class="hljs-comment"># Step6，记录下来一些数据</span><br>    logger.info(<span class="hljs-string">&#x27;Selecting Examples for Memory&#x27;</span>)<br>    <span class="hljs-keyword">for</span> relation <span class="hljs-keyword">in</span> current_relations:<br>        memorized_samples[relation] = select_data(config, encoder, training_data[relation])<br><br>    <span class="hljs-comment"># Step7</span><br>    mem_data = []<br>    <span class="hljs-keyword">for</span> rel <span class="hljs-keyword">in</span> memorized_samples:<br>        mem_data += memorized_samples[rel]<br>    <br>    <br>    <span class="hljs-comment"># Step8: Ak，把记录下来的数据，和老的数据混到一块训？</span><br>    data4step2 = mem_data + train_data_for_initial <br><br>    logger.info(<span class="hljs-string">&#x27;Replay, Activation and Reconsolidation&#x27;</span>)  <span class="hljs-comment"># 重放训练</span><br>    seen_relation_ids = [rel2id[rel] <span class="hljs-keyword">for</span> rel <span class="hljs-keyword">in</span> seen_relations]<br><br>    <span class="hljs-comment"># Step9</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br><br>        <span class="hljs-comment"># Step10 - 12 (use all to compute proto)</span><br>        protos4train = []<br>        <span class="hljs-keyword">for</span> relation <span class="hljs-keyword">in</span> seen_relations:<br>            protos4train.append(get_proto(config, encoder, memorized_samples[relation]))<br>        protos4train = torch.cat(protos4train, dim=<span class="hljs-number">0</span>).detach()<br><br>        <span class="hljs-comment"># Step13 - 15</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Memory Replay and Activation&#x27;</span>)<br>        model = train_simple_model(config, model, data4step2, <span class="hljs-number">1</span>) <span class="hljs-comment"># Memory Replay and Activation</span><br><br>        <span class="hljs-comment"># Step16 - 18 use all Memory Example; balance</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Memory Reconsolidation&quot;</span>)<br>        model = train_model(config, model, mem_data, <span class="hljs-number">1</span>, protos4train, seen_relation_ids) <span class="hljs-comment"># Memory Reconsolidaton</span><br><br>    protos4eval = []<br>    <span class="hljs-keyword">for</span> relation <span class="hljs-keyword">in</span> seen_relations:<br>        r = model.fc.weight[rel2id[relation]].detach()<br>        proto = get_proto(config, encoder, memorized_samples[relation], r)<br>        proto = proto / proto.norm()<br>        protos4eval.append(proto)<br>    protos4eval = torch.cat(protos4eval, dim=<span class="hljs-number">0</span>).detach()<br><br>    model.set_memorized_prototypes(protos4eval)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[Evaluation]&#x27;</span>)<br>    test_data_1 = []<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    current_relations：新增加进来的几个relation</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">for</span> relation <span class="hljs-keyword">in</span> current_relations:<br>        test_data_1 += test_data[relation]<br><br>    test_data_2 = []<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    在所有见过的relation上测</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">for</span> relation <span class="hljs-keyword">in</span> seen_relations:<br>        test_data_2 += historic_test_data[relation]<br>    cur_acc = evaluate_strict_model(config, test_data_1,seen_relations, rel2id, mode=<span class="hljs-string">&quot;cur&quot;</span>, pid2name=pid2name, model=model)<br>    total_acc =  evaluate_strict_model(config, test_data_2 ,seen_relations, rel2id, mode=<span class="hljs-string">&quot;total&quot;</span>, logger=logger, pid2name=pid2name, model=model)<br>    <br>    save_representation_to_file(config, model, sampler, id2rel, <span class="hljs-string">f&#x27;reps/<span class="hljs-subst">&#123;config.exp_name&#125;</span>/<span class="hljs-subst">&#123;i&#125;</span>/<span class="hljs-subst">&#123;episode&#125;</span>.pt&#x27;</span> ,memorized_samples)<br><br>  <br>    logger.info(<span class="hljs-string">f&#x27;Restart Num <span class="hljs-subst">&#123;i+<span class="hljs-number">1</span>&#125;</span>&#x27;</span>)<br>    logger.info(<span class="hljs-string">f&#x27;task--<span class="hljs-subst">&#123;episode + <span class="hljs-number">1</span>&#125;</span>:&#x27;</span>)<br>    test_cur.append(cur_acc)<br>    test_total.append(total_acc)<br>    logger.info(<span class="hljs-string">f&#x27;history test acc:<span class="hljs-subst">&#123;test_total&#125;</span>&#x27;</span>)<br>    logger.info(<span class="hljs-string">f&#x27;current test acc:<span class="hljs-subst">&#123;test_cur&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure>

<h5 id="1-1-2-1-详解get-aca-data函数"><a href="#1-1-2-1-详解get-aca-data函数" class="headerlink" title="1.1.2.1 详解get_aca_data函数"></a>1.1.2.1 详解get_aca_data函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_aca_data</span>(<span class="hljs-params">config, training_data, current_relations, tokenizer</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    config，训练集，当前的关系，tokenzier（加入了special token的tokenizer）</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    rel_id = config.num_of_relation  <span class="hljs-comment"># 40</span><br>    aca_data = []<br>    <span class="hljs-comment"># 把当前的current_relations（只针对这每个task的增量，比如每次新来4个，这样拆成两半）</span><br>    <span class="hljs-keyword">for</span> rel1, rel2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(current_relations[:config.rel_per_task // <span class="hljs-number">2</span>], current_relations[config.rel_per_task // <span class="hljs-number">2</span>:]):<br>        datas1 = training_data[rel1]  <span class="hljs-comment"># 关系i</span><br>        datas2 = training_data[rel2]  <span class="hljs-comment"># 关系j</span><br>        L = <span class="hljs-number">5</span>  <span class="hljs-comment"># 超参数，前后5个词</span><br>        <span class="hljs-keyword">for</span> data1, data2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(datas1, datas2):  <span class="hljs-comment"># 这是两边关系的数据，因为训练集采样320了，那么肯定是不会对不齐的？</span><br>            <span class="hljs-comment"># 对token1进行处理</span><br>            token1 = data1[<span class="hljs-string">&#x27;tokens&#x27;</span>][<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>][:]  <span class="hljs-comment"># 掐头去尾，self.tokenizer.encode可能会把CLS和SEP给带上</span><br>            <span class="hljs-comment"># 找位置，在token1中</span><br>            e11 = token1.index(<span class="hljs-number">30522</span>); e12 = token1.index(<span class="hljs-number">30523</span>)<br>            e21 = token1.index(<span class="hljs-number">30524</span>); e22 = token1.index(<span class="hljs-number">30525</span>)<br>            <span class="hljs-keyword">if</span> e21 &lt;= e11:<br>                <span class="hljs-comment"># 这个处理不是太明白，应该是一个前到后的关系？</span><br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-comment"># 截取下来一段，只要e11的前后5个词或者一个容错处理这样的感觉</span><br>            token1_sub = token1[<span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, e11-L): <span class="hljs-built_in">min</span>(e12+L+<span class="hljs-number">1</span>, e21)]<br>			<br>            <span class="hljs-comment"># token2做相同的处理，最后保留e21前后5个词这样的感觉</span><br>            token2 = data2[<span class="hljs-string">&#x27;tokens&#x27;</span>][<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>][:]<br>            e11 = token2.index(<span class="hljs-number">30522</span>); e12 = token2.index(<span class="hljs-number">30523</span>)<br>            e21 = token2.index(<span class="hljs-number">30524</span>); e22 = token2.index(<span class="hljs-number">30525</span>)<br>            <span class="hljs-keyword">if</span> e21 &lt;= e11:<br>                <span class="hljs-keyword">continue</span><br><br>            token2_sub = token2[<span class="hljs-built_in">max</span>(e12+<span class="hljs-number">1</span>, e21-L): <span class="hljs-built_in">min</span>(e22+L+<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(token2))]<br>			<br>            <span class="hljs-comment"># 再把CLS和SEP给拼回来，然后把这俩强行拼到一起</span><br>            token = [<span class="hljs-number">101</span>] + token1_sub + token2_sub + [<span class="hljs-number">102</span>]<br>            aca_data.append(&#123;<br>                <span class="hljs-string">&#x27;relation&#x27;</span>: rel_id,<br>                <span class="hljs-string">&#x27;tokens&#x27;</span>: token,<br>                <span class="hljs-string">&#x27;string&#x27;</span>: tokenizer.decode(token)<br>            &#125;)<br><br>            <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> [<span class="hljs-number">30522</span>, <span class="hljs-number">30523</span>, <span class="hljs-number">30524</span>, <span class="hljs-number">30525</span>]:<br>                <span class="hljs-keyword">assert</span> index <span class="hljs-keyword">in</span> token <span class="hljs-keyword">and</span> token.count(index) == <span class="hljs-number">1</span><br>                <br>        rel_id += <span class="hljs-number">1</span>  <span class="hljs-comment"># 这里从40开始，每次新加一个类，假设来说的话，每次新来4个类，就会多加出2个类来</span><br><br>    <span class="hljs-keyword">for</span> rel <span class="hljs-keyword">in</span> current_relations:<br>        <span class="hljs-comment"># 这个是无向的关系集合，不知道是做了统计还是怎么的</span><br>        <span class="hljs-keyword">if</span> rel <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;P26&#x27;</span>, <span class="hljs-string">&#x27;P3373&#x27;</span>, <span class="hljs-string">&#x27;per:siblings&#x27;</span>, <span class="hljs-string">&#x27;org:alternate_names&#x27;</span>, <span class="hljs-string">&#x27;per:spous&#x27;</span>, <span class="hljs-string">&#x27;per:alternate_names&#x27;</span>, <span class="hljs-string">&#x27;per:other_family&#x27;</span>]:<br>            <span class="hljs-keyword">continue</span><br>		<br>        <span class="hljs-comment"># 这里使用index然后反过来替换，制作反向的关系，假设4个关系就多出4个没用的</span><br>        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> training_data[rel]:<br>            token = data[<span class="hljs-string">&#x27;tokens&#x27;</span>][:]<br>            e11 = token.index(<span class="hljs-number">30522</span>); e12 = token.index(<span class="hljs-number">30523</span>)<br>            e21 = token.index(<span class="hljs-number">30524</span>); e22 = token.index(<span class="hljs-number">30525</span>)<br>            token[e11] = <span class="hljs-number">30524</span>; token[e12] = <span class="hljs-number">30525</span><br>            token[e21] = <span class="hljs-number">30522</span>; token[e22] = <span class="hljs-number">30523</span><br><br>            aca_data.append(&#123;<br>                    <span class="hljs-string">&#x27;relation&#x27;</span>: rel_id,<br>                    <span class="hljs-string">&#x27;tokens&#x27;</span>: token,<br>                    <span class="hljs-string">&#x27;string&#x27;</span>: tokenizer.decode(token)<br>                &#125;)<br>            <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> [<span class="hljs-number">30522</span>, <span class="hljs-number">30523</span>, <span class="hljs-number">30524</span>, <span class="hljs-number">30525</span>]:<br>                <span class="hljs-keyword">assert</span> index <span class="hljs-keyword">in</span> token <span class="hljs-keyword">and</span> token.count(index) == <span class="hljs-number">1</span><br>        rel_id += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> aca_data  <span class="hljs-comment"># 增强后的数据，这里多出来了不少数据，以及N+N//2种关系，这个算是作者的主要创新部分吗？</span><br></code></pre></td></tr></table></figure>

<h5 id="1-1-2-2-train-simple-model函数"><a href="#1-1-2-2-train-simple-model函数" class="headerlink" title="1.1.2.2 train_simple_model函数"></a>1.1.2.2 train_simple_model函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_simple_model</span>(<span class="hljs-params">config, model, train_set, epochs, step2=<span class="hljs-literal">False</span></span>):<br>    data_loader = get_data_loader(config, train_set, shuffle=<span class="hljs-literal">True</span>)<br>    model.train()<br>    criterion = nn.CrossEntropyLoss()<br>    optimizer = optim.Adam([<br>                            &#123;<span class="hljs-string">&#x27;params&#x27;</span>: model.sentence_encoder.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">0.00001</span>&#125;,<br>                            &#123;<span class="hljs-string">&#x27;params&#x27;</span>: model.fc.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">0.001</span>&#125;<br>                            ])<br>    <br>    <span class="hljs-keyword">for</span> epoch_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        losses = []<br>        <span class="hljs-keyword">for</span> step, (_, labels, sentences, _, _) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(data_loader)):<br>            model.zero_grad()<br>            sentences = torch.stack([x.to(config.device) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> sentences], dim=<span class="hljs-number">0</span>)<br>            logits, _ = model(sentences)<br>            <span class="hljs-keyword">if</span> step2 <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>:<br>                logits = logits[:, :config.num_of_relation]<br>            labels = labels.cuda()<br>            loss = criterion(logits, labels)<br>            loss.backward()<br>            losses.append(loss.item())<br>            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)<br>            optimizer.step()<br>        <span class="hljs-built_in">print</span> (np.array(losses).mean())<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure>

<h4 id="1-1-3-收尾（对齐1-1-1）"><a href="#1-1-3-收尾（对齐1-1-1）" class="headerlink" title="1.1.3 收尾（对齐1.1.1）"></a>1.1.3 收尾（对齐1.1.1）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">test_cur_record = torch.tensor(test_cur_record)<br>test_total_record = torch.tensor(test_total_record)<br><br>test_cur_record = torch.mean(test_cur_record, dim=0).tolist()<br>test_total_record = torch.mean(test_total_record, dim=0).tolist()<br><br>logger.info(f&#x27;Avg current test acc: &#123;test_cur_record&#125;&#x27;)<br>logger.info(f&#x27;Avg total test acc: &#123;test_total_record&#125;&#x27;)<br><br>print(&quot;log file has been saved in: &quot;, log_path)<br></code></pre></td></tr></table></figure>

<h2 id="2-代码中阅读到的问题和重点记录"><a href="#2-代码中阅读到的问题和重点记录" class="headerlink" title="2. 代码中阅读到的问题和重点记录"></a>2. 代码中阅读到的问题和重点记录</h2><ul>
<li>作者使用的是special token的方式，依次加入<code>[E11] [E12] [E21] [E22]</code>，bert的词表大小是30522，所以开始的编号是0-30511的，这几个token也就依次被定位为30522, 30523, 30524, 30525这样的；</li>
<li>对于这些special token的加入，作者是在预处理数据的时候就完成了，数据预处理的时候是先用relation type对应到各个样例，这样relation type在json文件中是第一层，就可以让持续学习比较好处理这种文件；</li>
<li>config文件和shell文件中的下面这几个参数，是针对fewrel数据集来用的，而TACRED数据集是定死的，训练集340，测试集40，没有验证集；</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">parser.add_argument(<span class="hljs-string">&#x27;--num_of_train&#x27;</span>, default=<span class="hljs-number">420</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--num_of_val&#x27;</span>, default=<span class="hljs-number">140</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br>parser.add_argument(<span class="hljs-string">&#x27;--num_of_test&#x27;</span>, default=<span class="hljs-number">140</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br></code></pre></td></tr></table></figure>

<ul>
<li>作者在划分数据集的时候这里代码有一点bug，这里因为是&lt;&#x3D;的符号，所以最后出来的都是41个数据，也不知道是不是他对齐的工作都是这么操作的，这个从作者输出的confusion matrix中也能看出来</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/Wangpeiyi9979/ACA/blob/898238415202d4bd6b7b555660b82732ab03483e/sampler.py#L114">https://github.com/Wangpeiyi9979/ACA/blob/898238415202d4bd6b7b555660b82732ab03483e/sampler.py#L114</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">if i &lt; len(rel_samples) // 5 and count &lt;= 40:<br>    count += 1<br>    test_dataset[self.rel2id[relation]].append(tokenized_sample)<br>else:<br>    count1 += 1<br>    train_dataset[self.rel2id[relation]].append(tokenized_sample)<br>    if count1 &gt;= 320:<br>        break<br></code></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">+----------+----------+----------+---------+----------+<br>|          | per:scho | per:orig | per:age | per:stat |<br>+----------+----------+----------+---------+----------+<br>| per:scho | 35       | 0        | 0       | 0        |<br>+----------+----------+----------+---------+----------+<br>| per:orig | 0        | 40       | 0       | 1        |<br>+----------+----------+----------+---------+----------+<br>| per:age  | 0        | 0        | 41      | 0        |<br>+----------+----------+----------+---------+----------+<br>| per:stat | 1        | 1        | 0       | 39       |<br>+----------+----------+----------+---------+----------+<br></code></pre></td></tr></table></figure>

<ul>
<li>在实现ACA的方法的时候，作者写的还是和论文里差不多的，论文里说用第一种关系重建生成N&#x2F;&#x2F;2种，用第二种反向生成N种，在代码里看起来这是最理想的情况，如果遇到一些特殊情况，或者反向的时候是那种对称的关系，那么就可能不到这个种类数目；</li>
<li>另外在代码这里涉及到一个model.incremental_learning，这个地方感觉不是很理解，他这个add_class的数目是3倍的<code>rel_per_task</code>，那么对于TACRED每个task新进来4个关系的话，这个add_relation_num就变成12，加起来就是40+12&#x3D;52种关系分类，但是每一次最多也就是40 + 4 + 2种关系的分类，这种在训练的过程里，是需要模型来学习这种关系吗？因为感觉模型的fc层，不是从4 8 12 16这样一直加上来的；；；另外这里也没理解这个<code>with torch.no_grad()</code>的作用；</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> config.aca:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    def incremental_learning(self, old_class, add_class):</span><br><span class="hljs-string">        weight = self.fc.weight.data</span><br><span class="hljs-string">        self.fc = nn.Linear(768, old_class + add_class, bias=False).cuda()</span><br><span class="hljs-string">        with torch.no_grad():</span><br><span class="hljs-string">            self.fc.weight.data[:old_class] = weight[:old_class]  # 把之前的复制进来构建这个层增量？</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    logger.info(<span class="hljs-string">&#x27;remove aca node&#x27;</span>)<br>    model.incremental_learning(config.num_of_relation, add_relation_num)  <span class="hljs-comment"># 不理解为什么add_relation_num是config.rel_per_task * 3</span><br></code></pre></td></tr></table></figure>

<ul>
<li><p>反向关系也直接定义成一种新的关系，像群里师兄说的，这个关系会不会没什么意义，而很多反向关系是可以在数据集中找到的，不知道这些数据集中有没有对反向关系的定义，如果我开始学习了B-&gt;A是一种反向新定义的没有含义的关系，但是B-&gt;A这个关系在之后训练出现了，那感觉就很容易分类错误？</p>
</li>
<li><p>那种拼接的方式也确实像师兄说的比较粗暴，看起来只是怎么取了5个词，强行给拼到一块上一样；</p>
</li>
<li><p>cur_acc就是在当前task的几个关系上的F1-Score，total_acc就是在当前+之前见过的所有关系上的F1-score，看起来作者主表格中挂的应该是total_acc</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">test_data_1 = []<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">current_relations：新增加进来的几个relation</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">for</span> relation <span class="hljs-keyword">in</span> current_relations:<br>test_data_1 += test_data[relation]<br><br>test_data_2 = []<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">在所有见过的relation上测</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">for</span> relation <span class="hljs-keyword">in</span> seen_relations:<br>test_data_2 += historic_test_data[relation]<br>cur_acc = evaluate_strict_model(config, test_data_1,seen_relations, rel2id, mode=<span class="hljs-string">&quot;cur&quot;</span>, pid2name=pid2name, model=model)<br>total_acc =  evaluate_strict_model(config, test_data_2 ,seen_relations, rel2id, mode=<span class="hljs-string">&quot;total&quot;</span>, logger=logger, pid2name=pid2name, model=model)<br><br>save_representation_to_file(config, model, sampler, id2rel, <span class="hljs-string">f&#x27;reps/<span class="hljs-subst">&#123;config.exp_name&#125;</span>/<span class="hljs-subst">&#123;i&#125;</span>/<span class="hljs-subst">&#123;episode&#125;</span>.pt&#x27;</span> ,memorized_samples)<br></code></pre></td></tr></table></figure>

<ul>
<li><p>一般来说，学习的顺序和采样的情况都会影响，作者就是很多随机策略，每次换一个seed这样的，5次实验，每次跑10个task，然后比如TACRED就是随机打乱后采样320个训练数据加40个测试数据；</p>
</li>
<li><p>看起来像是一个train_simple_model + train_model的学习范式，在train_simple_model的时候把aca的数据混进去，但好像后续就不需要了（对EMAR还没了解太多）；</p>
</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/research/" class="category-chain-item">research</a>
  
  
    <span>></span>
    
  <a href="/categories/research/code-reading/" class="category-chain-item">code-reading</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/code-reading/">#code-reading</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>emnlp2022-ACA论文代码阅读</div>
      <div>http://example.com/2022/11/23/research/code/emnlp2022-ACA论文代码阅读/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Curious;</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年11月23日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/11/21/develop/redis/redis%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%EF%BC%9A%E2%80%9C5%E7%A7%8D%E5%9F%BA%E7%A1%80%E2%80%9D%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B-%E2%80%9C3%E7%A7%8D%E7%89%B9%E6%AE%8A%E2%80%9D%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/" title="redis基本数据类型：“5种基础”数据类型+“3种特殊”数据类型">
                        <span class="hidden-mobile">redis基本数据类型：“5种基础”数据类型+“3种特殊”数据类型</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
